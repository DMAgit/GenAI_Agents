{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8376f449a4cd31",
   "metadata": {},
   "source": [
    "**TITLE:** MULTI-AGENT INTERVIEWING SYSTEM\n",
    "\n",
    "**DEVELOPERS:**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b669f9e2849047b",
   "metadata": {},
   "source": [
    "# Setup Instructions for Jupyter Notebook\n",
    "\n",
    "This notebook installs essential packages for working with LangChain, OpenAI, and other data handling tools. \n",
    "\n",
    "### Important Notes:\n",
    "- **Google Colab Users**: If you are using Google Colab, ensure to install `google-colab` specific packages. \n",
    "- **GPU Configuration**: If using Google Colab, you can enable GPU for faster performance by going to:\n",
    "  - **Runtime** > **Change runtime type** > **Hardware accelerator** and selecting **GPU**.\n",
    "  \n",
    "---\n",
    "\n",
    "## Step 1: Install General Utilities and Google Colab Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install general utilities and widgets\n",
    "%pip install pandas opendatasets nest_asyncio ipywebrtc ipywidgets IPython "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64043e93ef176632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if using google-colab, else skip it\n",
    "%pip install google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde274d17b4b8f27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Install OpenAI, LangChain, and Related Tools\n",
    "These packages are necessary for using OpenAIâ€™s language models and LangChain's toolkit for search, document processing, and data handling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ffe9a8e044bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI and related LangChain tools\n",
    "%pip install openai langchain_openai\n",
    "\n",
    "# LangChain Community Tools for search and document handling\n",
    "%pip install langchain_community\n",
    "\n",
    "# Typing extensions and Pydantic\n",
    "%pip install typing_extensions pydantic\n",
    "\n",
    "# LangGraph and experimental LangChain tools\n",
    "%pip install langgraph langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For agent tools\n",
    "%pip install pypdf wikipedia duckduckgo-search playwright\n",
    "\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bbc30c8307592",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Database Utilities, SQLAlchemy, and FAISS for Vector Storage\n",
    "\n",
    "- **Database Utilities**: Install SQLAlchemy for database interactions.\n",
    "- **FAISS**: Choose `faiss-cpu` for CPU environments or `faiss-gpu` if you've enabled GPU support on Colab.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5888251935ea6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database utilities and SQLAlchemy\n",
    "%pip install SQLAlchemy\n",
    "\n",
    "# FAISS for vector storage and retrieval\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!where python\n",
    "!pip show playwright\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa27fcb72b8e53e",
   "metadata": {},
   "source": [
    "## General Imports\n",
    "This cell includes the essential imports needed to use LangChain, OpenAI, and other data handling tools in any Jupyter Notebook or Python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d741d4caeeda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for data handling, display, and LangChain functionality\n",
    "import os\n",
    "import opendatasets as od\n",
    "import nest_asyncio\n",
    "\n",
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChain and related tools\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# LangChain Agents and supporting libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, trim_messages\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Literal, Sequence, List\n",
    "import functools\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6e32631d8416c",
   "metadata": {},
   "source": [
    "## Google Colab Specific Imports\n",
    "This cell should be run only if you're using Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33c03e419f01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab specific imports\n",
    "from google.colab import output\n",
    "from google.colab import userdata\n",
    "from google.colab import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70f69fdf69c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41518f1fbffb425c",
   "metadata": {},
   "source": [
    "# Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c49167463ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    # Retrieve API key from Google Colab userdata (if stored there)\n",
    "    open_ai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    # Not running on Google Colab; prompt for API key input or retrieve from environment variables\n",
    "    open_ai_api_key = os.getenv('OPENAI_API_KEY') or input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Set the API key as an environment variable for universal access within the notebook\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_api_key\n",
    "\n",
    "# Confirm setup\n",
    "if open_ai_api_key:\n",
    "    print(f\"API key successfully set: {open_ai_api_key}\")\n",
    "else:\n",
    "    print(\"API key not set. Please check your setup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3bc41d2c30b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: \n",
    "# Need to have an alternative that grabs a HuggingFace API key and interfaces with free models there (Llama-3-8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a360a1929e585",
   "metadata": {},
   "source": [
    "# Create Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241141af1e856401",
   "metadata": {},
   "source": [
    "## 1. Speech-to-text\n",
    "\n",
    "This tool allows the user to record speech and converts it to a text using OpenAI Whisper model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e18d7b5333b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7ad06af747527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a614597653bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_audio_recorder():\n",
    "    camera = CameraStream(constraints={'audio': True, 'video': False})\n",
    "    recorder = AudioRecorder(stream=camera)\n",
    "    display(recorder)\n",
    "    return recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b682f5e34981ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recording(recorder):\n",
    "    audio_data = recorder.audio.value\n",
    "    if audio_data:\n",
    "        with open(\"recording.webm\", \"wb\") as f:\n",
    "            f.write(audio_data)\n",
    "        return \"recording.webm\"\n",
    "    else:\n",
    "        print(\"No audio data was captured. Please try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8005ca2b61e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(input_filename, output_filename=\"my_recording.wav\"):\n",
    "    if input_filename and os.path.exists(input_filename):\n",
    "        os.system(f\"ffmpeg -i {input_filename} -ac 1 -f wav {output_filename} -y -hide_banner -loglevel panic\")\n",
    "        if os.path.exists(output_filename):\n",
    "            return output_filename\n",
    "        else:\n",
    "            print(\"Conversion failed.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Input file does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d27fc546946d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(filename):\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file\n",
    "        )\n",
    "    print(\"\")\n",
    "    print(\"Transcription:\", transcription.text)\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0a312a15ebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_transcribe_candidate_answer():\n",
    "    \"\"\"Record and transcribe a candidate's answer on interviewers' questions.\"\"\"\n",
    "    # Set up the recorder\n",
    "    recorder = setup_audio_recorder()\n",
    "\n",
    "    # Create a save button\n",
    "    print(\"\")\n",
    "    save_button = widgets.Button(description=\"Save Recording\")\n",
    "\n",
    "    # This dictionary will store the transcribed text\n",
    "    transcription_result = {}\n",
    "\n",
    "    # Define the callback function for the save button\n",
    "    def on_save_clicked(button):\n",
    "        # Save the recording\n",
    "        webm_file = save_recording(recorder)\n",
    "        if webm_file:\n",
    "            # Convert to wav format\n",
    "            wav_file = convert_to_wav(webm_file)\n",
    "            if wav_file:\n",
    "                # Transcribe the audio and store the result\n",
    "                transcription_result['text'] = transcribe_audio(wav_file)\n",
    "\n",
    "    save_button.on_click(on_save_clicked)\n",
    "    display(save_button)\n",
    "\n",
    "    # Return the transcription result dictionary\n",
    "    return transcription_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca99386ecd3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Try to do live transcription, rather than recording a file. \n",
    "# Take a look at https://gist.github.com/Vaibhavs10/a48d141534cc8d877937d421bb828d8e\n",
    "# and https://github.com/VRSEN/langchain-agents-tutorial/blob/main/main.py\n",
    "\n",
    "# FOSS alternative pipeline, that doesn't rely on OpenAI models\n",
    "# Using HF free API instead \n",
    "# Something like https://github.com/nyrahealth/CrisperWhisper?tab=readme-ov-file#31-usage-with--transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d2e5e97f917bb",
   "metadata": {},
   "source": [
    "## 2. Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f961638c01d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_text_input():\n",
    "    text_input = widgets.Textarea(\n",
    "        placeholder=\"Type your answer here...\",\n",
    "        description=\"Answer:\",\n",
    "        layout=widgets.Layout(width='500px', height='100px')\n",
    "    )\n",
    "    display(text_input)\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bc16915c6f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_text_input(text_widget):\n",
    "    user_text = text_widget.value\n",
    "    if user_text.strip():\n",
    "        print(\"\\nInput:\\n\", user_text)\n",
    "        return user_text\n",
    "    else:\n",
    "        print(\"No input was provided. Please type your answer and try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091dca5fa15aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_submit_text():\n",
    "    \"\"\"Record a candidate's text answer on interviewers' questions which require written output like code.\"\"\"\n",
    "    # Set up the text input widget\n",
    "    text_widget = setup_text_input()\n",
    "\n",
    "    # Create a submit button\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Save Answer\")\n",
    "\n",
    "    # This variable will store the submitted text\n",
    "    submission_result = {}\n",
    "\n",
    "    # Define the callback function for the submit button\n",
    "    def on_submit_clicked(button):\n",
    "        # Capture the user's text input and store it in the dictionary\n",
    "        submission_result['text'] = submit_text_input(text_widget)\n",
    "\n",
    "    submit_button.on_click(on_submit_clicked)\n",
    "    display(submit_button)\n",
    "\n",
    "    # Wait for user input to be submitted\n",
    "    return submission_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f0fd76213961e",
   "metadata": {},
   "source": [
    "## 3. CV Reader\n",
    "\n",
    "CV Reader for PDF and DOCX files.\n",
    "\n",
    "Instead of CV you can upload your LinkedIn profile extract, which can be exported in a PDF format.\n",
    "\n",
    "This tools can be easily changed to any file reading service, e.g., Azure DI, LlamaParse, custom parsing with PyPdf, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63b534771d50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab version\n",
    "\n",
    "def upload_and_filter_file():\n",
    "    # Upload a single file\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    # Check if only one file was uploaded\n",
    "    if len(uploaded) != 1:\n",
    "        print(\"Please upload exactly one file.\")\n",
    "        return None\n",
    "\n",
    "    # Get the uploaded file name and data\n",
    "    file_name, file_data = next(iter(uploaded.items()))\n",
    "\n",
    "    # Check if the file is .pdf or .docx\n",
    "    if not file_name.endswith(('.pdf', '.docx')):\n",
    "        print(\"Invalid file type. Please upload only .pdf or .docx files.\")\n",
    "        return None\n",
    "\n",
    "    # Save the file directly to the /content/ directory\n",
    "    file_path = f'/content/{file_name}'\n",
    "\n",
    "    return file_path\n",
    "\n",
    "cv_file_path = upload_and_filter_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a536813081d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local jupyter notebook\n",
    "\n",
    "cv_file_path = r'C:\\Users\\DMA\\Downloads\\CV - 2024-1.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a65c074428d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9266679f8fa046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cv_retriever(file_path, k):\n",
    "    pages = []\n",
    "\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type.\")\n",
    "\n",
    "    for page in loader.load():\n",
    "        pages.append(page)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(pages)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63f50b2351bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_retriever = create_cv_retriever(cv_file_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b754be1121566a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tool = create_retriever_tool(\n",
    "    cv_retriever,\n",
    "    \"search_candidate_info\",\n",
    "    \"Searches and returns candidate's profile with experience, education, and skills.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b580af1e74d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: \n",
    "# Free alternative for embeddings that doesn't use OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18422b0153a965fd",
   "metadata": {},
   "source": [
    "## 4. Hiring Company Info Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411db1436669cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(query):\n",
    "    \"\"\"Fetches content from Wikipedia based on a query.\"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    wikipedia_content = wikipedia.run(query)\n",
    "    return wikipedia_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c94cc9e174acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_websites_links(query):\n",
    "    \"\"\"Fetches a list of website links based on a search query using DuckDuckGo.\"\"\"\n",
    "    search = DuckDuckGoSearchResults(output_format=\"list\")\n",
    "    search_results = search.invoke(query)\n",
    "    return [result[\"link\"] for result in search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138048ef56616c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_websites_content(websites):\n",
    "    \"\"\"Loads the HTML content of a list of websites.\"\"\"\n",
    "    content_list = []\n",
    "    for website in websites:\n",
    "        loader = AsyncChromiumLoader([website])\n",
    "        html_content = loader.load()\n",
    "        content_list.append(html_content)\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61832d87cfa91b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_html_content(html_content_list, tags = [\"span\", \"p\", \"b\", \"h3\", \"h4\"]):\n",
    "    \"\"\"Transforms HTML content to extract specific tags using BeautifulSoup.\"\"\"\n",
    "    transformed_content = []\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "    for html in html_content_list:\n",
    "        docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=tags)\n",
    "        for doc in docs_transformed:\n",
    "            transformed_content.append(doc.page_content)\n",
    "    return transformed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e031bd1b42305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_content(query):\n",
    "    \"\"\"Main function to gather content from Wikipedia and websites based on a query.\"\"\"\n",
    "    content = []\n",
    "\n",
    "    wikipedia_content = get_wikipedia_content(query)\n",
    "    content.append(wikipedia_content)\n",
    "\n",
    "    website_links = get_websites_links(f\"What is {query}?\")\n",
    "\n",
    "    html_content_list = load_websites_content(website_links)\n",
    "\n",
    "    transformed_content = transform_html_content(html_content_list)\n",
    "\n",
    "    content.extend(transformed_content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32851a57d96f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Deloitte Company\"\n",
    "websites_content = get_web_content(query)\n",
    "websites_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392099cca9bc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_company_info_retriever(websites_content, k):\n",
    "    docs = []\n",
    "\n",
    "    for website_content in websites_content:\n",
    "        doc = Document(page_content=website_content)\n",
    "        docs.append(doc)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()  # need a FOSS alternative\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2feee0d40acb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info_retriever = create_company_info_retriever(websites_content, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d378d450302799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: update this tool so it gets correct data, this is copied from the cv\n",
    "\n",
    "company_info_tool = create_retriever_tool(\n",
    "    company_info_retriever,\n",
    "    \"search_company_info\",\n",
    "    \"Searches and returns company's profile with company's details to be considered by HR Specialist.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d716952ac5020",
   "metadata": {},
   "source": [
    "## 5. Querying a Dataset\n",
    "\n",
    "This is an optional tool for enhancing the process of hard skills review.\n",
    "\n",
    "The dataset can be changed depending on the needs of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada19680d80eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"https://www.kaggle.com/datasets/syedmharis/software-engineering-interview-questions-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c587555228f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaggle_ds(dataset_url):\n",
    "    od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59829e5ba10feda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "\n",
    "# Set the file path to the downloaded data and the encoding of the file\n",
    "file_path = r\"C:\\Users\\DMA\\Downloads\\Software Questions.csv\"\n",
    "encoding = \"ISO-8859-1\"  # default English encoding\n",
    "\n",
    "loader = CSVLoader(file_path=file_path, encoding=encoding)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba42e05d275965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text splitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f67d516dc51406",
   "metadata": {},
   "source": [
    "### 5.1a Using OpenAI Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fec19fe82b517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_questions_dataset_retriever(texts, k):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c013344d9c5a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_questions_dataset_retriever(texts=texts, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d590160f25aa77e",
   "metadata": {},
   "source": [
    "### 5.1b Using HuggingFace Embeddings \n",
    "\n",
    "To represent each chunk as a high-dimensional vector, weâ€™ll use Hugging Face's pre-trained model sentence-transformers/all-MiniLM-L6-v2. This model is efficient and well-suited for generating text embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84977845029fc567",
   "metadata": {},
   "source": [
    "Weâ€™ll define a simple helper class to handle embedding generation using the Hugging Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8dca7934bf6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbeddings:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        # Load the model and tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def embed_texts(self, texts):\n",
    "        # Generate embeddings for each text\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ce16f9cfb72df",
   "metadata": {},
   "source": [
    "Now, letâ€™s generate embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897a69ec2ff2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Generate embeddings for each chunk of text\n",
    "embeddings = embeddings_model.embed_texts([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eff59bdd058320",
   "metadata": {},
   "source": [
    "After this step, `embeddings` will contain a vector representation of each document chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663811a7f4974e2",
   "metadata": {},
   "source": [
    "To make our embeddings searchable, weâ€™ll use FAISS to create an index. This allows us to find the most similar embeddings to any query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62481f0456eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Initialize the FAISS index\n",
    "embedding_dim = embeddings.shape[1]  # Dimension of embeddings\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6a9124bc25b62",
   "metadata": {},
   "source": [
    "Finally, weâ€™ll define a `retriever` function that, given a query, will embed it and retrieve the most similar document chunks from the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d84f1e759c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query, texts, embeddings_model, faiss_index, k=5):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embeddings_model.embed_texts([query])[0]\n",
    "    \n",
    "    # Search FAISS index for the top-k similar chunks\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    # Retrieve the corresponding text chunks\n",
    "    results = [texts[i].page_content for i in indices[0]]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498063ba154aff57",
   "metadata": {},
   "source": [
    "For testing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d35189e601810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your query\n",
    "query = \"What is the topic of interest?\"\n",
    "\n",
    "# Call the retriever with the required arguments\n",
    "results = retriever(query, texts, embeddings_model, faiss_index, k=5)\n",
    "\n",
    "# Print the top results\n",
    "print(\"Top similar chunks:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9fc77f1caaec8",
   "metadata": {},
   "source": [
    "### 5.2 Define the tool for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae193c12d94107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: update this tool so its usable by agents\n",
    "\n",
    "@tool\n",
    "questions_database_tool = create_retriever_tool(\n",
    "    create_questions_dataset_retriever,\n",
    "    \"search_subject_matter_questions\",\n",
    "    \"Searches and returns subject matter questions for checking hard skills.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367d4aa565d2239",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Initialize Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32abecc57086fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Need to test this with OAI key\n",
    "# Test each of the tools are working\n",
    "\n",
    "# Create LangGraph agents, give them roles, assign interactions and tools to each\n",
    "\n",
    "# Implement user-agent interaction\n",
    "# LangGraph - https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/human_in_the_loop/wait-user-input.ipynb\n",
    "\n",
    "# Add a FOSS alternative for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968cd86c5123092",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")  # need a FOSS alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfe73049656d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_input_form_with_return():\n",
    "    # Capture inputs\n",
    "    print(\"Invoice input\")\n",
    "    print(\"\")\n",
    "    voice_input = record_and_transcribe_candidate_answer()\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Text input\")\n",
    "    print(\"\")\n",
    "    written_input = record_and_submit_text()\n",
    "\n",
    "    # Define what happens on submit\n",
    "    def on_submit(button):\n",
    "        clear_output()\n",
    "        print(\"Submitted successfully. Moving to the next step...\")\n",
    "\n",
    "    # Create the submit button and link to the on_submit action\n",
    "    print(\"\")\n",
    "    print(\"================================================\")\n",
    "    print(\"Please, click submit button to send your answers\")\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(on_submit)\n",
    "\n",
    "    display(submit_button)\n",
    "\n",
    "    if submit_button:\n",
    "      return voice_input, written_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87f94dee3d8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice, text_input = display_input_form_with_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ce6ff531f8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = f\"Answer: {voice.get('text', '') if voice else ''}\\n\\n{text_input.get('text', '') if text_input else ''}\"\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1956a344b297ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5c6dc75a09db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242afbd93da45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7780cecfc20472",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8c4f051218be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"type\": \"user\", \"content\": answer}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1784f90bf83fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"type\": \"user\", \"content\": answer}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fb910",
   "metadata": {},
   "source": [
    "# Agents (DMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedfdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0365ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")  # need a FOSS alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a844d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going \"bind\" all tools to the model\n",
    "# We have the ACTUAL tools from above, but we also need a mock tool to ask a human\n",
    "# Since `bind_tools` takes in tools but also just tool definitions,\n",
    "# We can define a tool definition for `ask_human`\n",
    "class AskHuman(BaseModel):\n",
    "    \"\"\"Ask the human a question\"\"\"\n",
    "\n",
    "    question: str\n",
    "  \n",
    "llm = llm.bind_tools(tools + [AskHuman])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda97721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # If tool call is asking Human, we return that node\n",
    "    # You could also add logic here to let some system know that there's something that requires Human input\n",
    "    # For example, send a slack message, etc\n",
    "    elif last_message.tool_calls[0][\"name\"] == \"AskHuman\":\n",
    "        return \"ask_human\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad90dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that calls the llm\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10819a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a fake node to ask the human\n",
    "def ask_human(state):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e59fe",
   "metadata": {},
   "source": [
    "Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda63a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the three nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "workflow.add_node(\"ask_human\", ask_human)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed74746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # We may ask the human\n",
    "        \"ask_human\": \"ask_human\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc918237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# After we get back the human response, we go back to the agent\n",
    "workflow.add_edge(\"ask_human\", \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a47d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f70d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "# We add a breakpoint BEFORE the `ask_human` node so it never executes\n",
    "app = workflow.compile(checkpointer=memory, interrupt_before=[\"ask_human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c324e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(\n",
    "    content=\"Ask the user where they are\"\n",
    ")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_id = app.get_state(config).values[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "tool_message = [ToolMessage(tool_call_id=tool_call_id, content=\"san francisco\")]\n",
    "\n",
    "# We now update the state\n",
    "# Notice that we are also specifying `as_node=\"ask_human\"`\n",
    "# This will apply this update as this node,\n",
    "# which will make it so that afterwards it continues as normal\n",
    "app.update_state(config, {\"messages\": tool_message}, as_node=\"ask_human\")\n",
    "\n",
    "# We can check the state\n",
    "# We can see that the state currently has the `agent` node next\n",
    "# This is based on how we define our graph,\n",
    "# where after the `ask_human` node goes (which we just triggered)\n",
    "# there is an edge to the `agent` node\n",
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in app.stream(None, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the current state of the application\n",
    "state = app.get_state(config).values\n",
    "\n",
    "# Access the list of messages from the state\n",
    "messages = state[\"messages\"]\n",
    "\n",
    "# Iterate through each message and print its content\n",
    "for message in messages:\n",
    "    print(f\"{message.type.capitalize()} Message: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fa4c8cd964f92",
   "metadata": {},
   "source": [
    "-----\n",
    "# Stretch goal: TTS\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08d9683f86af87",
   "metadata": {},
   "source": [
    "Define model and TTS pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210545fca2c5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the TTS model\n",
    "tts_pipeline = pipeline(\"text-to-speech\", model=\"espnet/kan-bayashi_ljspeech_vits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c26c3dd6ea189",
   "metadata": {},
   "source": [
    "Generate and Play Text with TTS in Real-Time\n",
    "\n",
    "Create a loop where the language model generates text in small chunks. Each chunk will be converted to speech and played immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81c891045e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "def generate_and_play_text(prompt, max_chunks=5, chunk_size=50):\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    # Generate text in chunks\n",
    "    for _ in range(max_chunks):\n",
    "        # Generate a chunk of text\n",
    "        output = text_generator(prompt + generated_text, max_new_tokens=chunk_size, do_sample=True)\n",
    "        new_text = output[0][\"generated_text\"][len(prompt + generated_text):]\n",
    "        \n",
    "        # Append the new text to the generated text\n",
    "        generated_text += new_text\n",
    "        print(new_text)  # Print the generated text chunk\n",
    "\n",
    "        # Generate TTS for the current chunk\n",
    "        audio = tts_pipeline(new_text)\n",
    "\n",
    "        # Autoplay the audio chunk in the notebook\n",
    "        ipd.display(ipd.Audio(audio[\"wav\"], autoplay=True))\n",
    "        \n",
    "        # Add a short delay to simulate real-time generation if needed\n",
    "        # time.sleep(1)  # Uncomment if you want to control the timing\n",
    "\n",
    "# Example usage\n",
    "generate_and_play_text(\"Once upon a time,\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_agents",
   "language": "python",
   "name": "genai_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
