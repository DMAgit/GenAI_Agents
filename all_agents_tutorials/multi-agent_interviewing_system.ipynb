{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**TITLE:** MULTI-AGENT INTERVIEWING SYSTEM\n",
    "\n",
    "**DEVELOPERS:**\n",
    "\n",
    "------"
   ],
   "id": "1f8376f449a4cd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup Instructions for Jupyter Notebook\n",
    "\n",
    "This notebook installs essential packages for working with LangChain, OpenAI, and other data handling tools. \n",
    "\n",
    "### Important Notes:\n",
    "- **Google Colab Users**: If you are using Google Colab, ensure to install `google-colab` specific packages. \n",
    "- **GPU Configuration**: If using Google Colab, you can enable GPU for faster performance by going to:\n",
    "  - **Runtime** > **Change runtime type** > **Hardware accelerator** and selecting **GPU**.\n",
    "  \n",
    "---\n",
    "\n",
    "## Step 1: Install General Utilities and Google Colab Packages\n"
   ],
   "id": "4b669f9e2849047b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Install general utilities and widgets\n",
    "!pip install pandas opendatasets nest_asyncio ipywebrtc ipywidgets IPython"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Only run this cell if using google-colab, else skip it\n",
    "!pip install google-colab"
   ],
   "id": "64043e93ef176632",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Step 2: Install OpenAI, LangChain, and Related Tools\n",
    "These packages are necessary for using OpenAI’s language models and LangChain's toolkit for search, document processing, and data handling.\n",
    "\n",
    "---\n"
   ],
   "id": "cde274d17b4b8f27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# OpenAI and related LangChain tools\n",
    "!pip install openai langchain_openai\n",
    "\n",
    "# LangChain Community Tools for search and document handling\n",
    "!pip install langchain_community\n",
    "\n",
    "# Typing extensions and Pydantic\n",
    "!pip install typing_extensions pydantic\n",
    "\n",
    "# LangGraph and experimental LangChain tools\n",
    "!pip install langgraph langchain_experimental"
   ],
   "id": "c52ffe9a8e044bac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Step 3: Database Utilities, SQLAlchemy, and FAISS for Vector Storage\n",
    "\n",
    "- **Database Utilities**: Install SQLAlchemy for database interactions.\n",
    "- **FAISS**: Choose `faiss-cpu` for CPU environments or `faiss-gpu` if you've enabled GPU support on Colab.\n",
    "\n",
    "---\n"
   ],
   "id": "963bbc30c8307592"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Database utilities and SQLAlchemy\n",
    "!pip install SQLAlchemy\n",
    "\n",
    "# Temporary file management\n",
    "!pip install tempfile\n",
    "\n",
    "# FAISS for vector storage and retrieval\n",
    "!pip install faiss-cpu  # or use !pip install faiss-gpu if using GPU"
   ],
   "id": "e5888251935ea6e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## General Imports\n",
    "This cell includes the essential imports needed to use LangChain, OpenAI, and other data handling tools in any Jupyter Notebook or Python environment.\n"
   ],
   "id": "2aa27fcb72b8e53e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# General imports for data handling, display, and LangChain functionality\n",
    "import os\n",
    "import opendatasets as od\n",
    "import nest_asyncio\n",
    "\n",
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChain and related tools\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# LangChain Agents and supporting libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, trim_messages\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Literal, Sequence, List\n",
    "import functools\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ],
   "id": "e2d741d4caeeda23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Google Colab Specific Imports\n",
    "This cell should be run only if you're using Google Colab.\n"
   ],
   "id": "6ac6e32631d8416c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Google Colab specific imports\n",
    "from google.colab import output\n",
    "from google.colab import userdata\n",
    "from google.colab import files\n"
   ],
   "id": "ef33c03e419f01aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nest_asyncio.apply()",
   "id": "ff70f69fdf69c15e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get API Keys",
   "id": "41518f1fbffb425c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    # Retrieve API key from Google Colab userdata (if stored there)\n",
    "    open_ai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    # Not running on Google Colab; prompt for API key input or retrieve from environment variables\n",
    "    open_ai_api_key = os.getenv('OPENAI_API_KEY') or input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Set the API key as an environment variable for universal access within the notebook\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_api_key\n",
    "\n",
    "# Confirm setup\n",
    "if open_ai_api_key:\n",
    "    print(\"API key successfully set.\")\n",
    "else:\n",
    "    print(\"API key not set. Please check your setup.\")\n"
   ],
   "id": "e34c49167463ef77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Todo: \n",
    "# Need to have an alternative that grabs a HuggingFace API key and interfaces with free models there (Llama-3-8B)"
   ],
   "id": "91f3bc41d2c30b0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Tools",
   "id": "2d8a360a1929e585"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Speech-to-text\n",
    "\n",
    "This tool allows the user to record speech and converts it to a text using OpenAI Whisper model.\n",
    "\n"
   ],
   "id": "241141af1e856401"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "client = OpenAI()",
   "id": "b3e18d7b5333b8ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for colab\n",
    "output.enable_custom_widget_manager()"
   ],
   "id": "78d7ad06af747527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def setup_audio_recorder():\n",
    "    camera = CameraStream(constraints={'audio': True, 'video': False})\n",
    "    recorder = AudioRecorder(stream=camera)\n",
    "    display(recorder)\n",
    "    return recorder"
   ],
   "id": "98a614597653bc8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_recording(recorder):\n",
    "    audio_data = recorder.audio.value\n",
    "    if audio_data:\n",
    "        with open(\"recording.webm\", \"wb\") as f:\n",
    "            f.write(audio_data)\n",
    "        return \"recording.webm\"\n",
    "    else:\n",
    "        print(\"No audio data was captured. Please try again.\")\n",
    "        return None"
   ],
   "id": "5b682f5e34981ddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert_to_wav(input_filename, output_filename=\"my_recording.wav\"):\n",
    "    if input_filename and os.path.exists(input_filename):\n",
    "        os.system(f\"ffmpeg -i {input_filename} -ac 1 -f wav {output_filename} -y -hide_banner -loglevel panic\")\n",
    "        if os.path.exists(output_filename):\n",
    "            return output_filename\n",
    "        else:\n",
    "            print(\"Conversion failed.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Input file does not exist.\")\n",
    "        return None"
   ],
   "id": "8ba8005ca2b61e95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def transcribe_audio(filename):\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file\n",
    "        )\n",
    "    print(\"\")\n",
    "    print(\"Transcription:\", transcription.text)\n",
    "    return transcription.text"
   ],
   "id": "c10d27fc546946d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def record_and_transcribe_candidate_answer():\n",
    "    \"\"\"Record and transcribe a candidate's answer on interviewers' questions.\"\"\"\n",
    "    # Set up the recorder\n",
    "    recorder = setup_audio_recorder()\n",
    "\n",
    "    # Create a save button\n",
    "    print(\"\")\n",
    "    save_button = widgets.Button(description=\"Save Recording\")\n",
    "\n",
    "    # This dictionary will store the transcribed text\n",
    "    transcription_result = {}\n",
    "\n",
    "    # Define the callback function for the save button\n",
    "    def on_save_clicked(button):\n",
    "        # Save the recording\n",
    "        webm_file = save_recording(recorder)\n",
    "        if webm_file:\n",
    "            # Convert to wav format\n",
    "            wav_file = convert_to_wav(webm_file)\n",
    "            if wav_file:\n",
    "                # Transcribe the audio and store the result\n",
    "                transcription_result['text'] = transcribe_audio(wav_file)\n",
    "\n",
    "    save_button.on_click(on_save_clicked)\n",
    "    display(save_button)\n",
    "\n",
    "    # Return the transcription result dictionary\n",
    "    return transcription_result"
   ],
   "id": "17f0a312a15ebfdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Todo:\n",
    "# Try to do live transcription, rather than recording a file. \n",
    "# Take a look at https://gist.github.com/Vaibhavs10/a48d141534cc8d877937d421bb828d8e\n",
    "# and https://github.com/VRSEN/langchain-agents-tutorial/blob/main/main.py\n",
    "\n",
    "# FOSS alternative pipeline, that doesn't rely on OpenAI models\n",
    "# Using HF free API instead \n",
    "# Something like https://github.com/nyrahealth/CrisperWhisper?tab=readme-ov-file#31-usage-with--transformers"
   ],
   "id": "70ca99386ecd3228",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Text Input",
   "id": "b17d2e5e97f917bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def setup_text_input():\n",
    "    text_input = widgets.Textarea(\n",
    "        placeholder=\"Type your answer here...\",\n",
    "        description=\"Answer:\",\n",
    "        layout=widgets.Layout(width='500px', height='100px')\n",
    "    )\n",
    "    display(text_input)\n",
    "    return text_input"
   ],
   "id": "d94f961638c01d8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def submit_text_input(text_widget):\n",
    "    user_text = text_widget.value\n",
    "    if user_text.strip():\n",
    "        print(\"\\nInput:\\n\", user_text)\n",
    "        return user_text\n",
    "    else:\n",
    "        print(\"No input was provided. Please type your answer and try again.\")\n",
    "        return None"
   ],
   "id": "726bc16915c6f039",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def record_and_submit_text():\n",
    "    \"\"\"Record a candidate's text answer on interviewers' questions which require written output like code.\"\"\"\n",
    "    # Set up the text input widget\n",
    "    text_widget = setup_text_input()\n",
    "\n",
    "    # Create a submit button\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Save Answer\")\n",
    "\n",
    "    # This variable will store the submitted text\n",
    "    submission_result = {}\n",
    "\n",
    "    # Define the callback function for the submit button\n",
    "    def on_submit_clicked(button):\n",
    "        # Capture the user's text input and store it in the dictionary\n",
    "        submission_result['text'] = submit_text_input(text_widget)\n",
    "\n",
    "    submit_button.on_click(on_submit_clicked)\n",
    "    display(submit_button)\n",
    "\n",
    "    # Wait for user input to be submitted\n",
    "    return submission_result"
   ],
   "id": "f091dca5fa15aee5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. CV Reader\n",
    "\n",
    "CV Reader for PDF and DOCX files.\n",
    "\n",
    "Instead of CV you can upload your LinkedIn profile extract, which can be exported in a PDF format.\n",
    "\n",
    "This tools can be easily changed to any file reading service, e.g., Azure DI, LlamaParse, custom parsing with PyPdf, etc."
   ],
   "id": "e37f0fd76213961e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# colab version\n",
    "\n",
    "def upload_and_filter_file():\n",
    "    # Upload a single file\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    # Check if only one file was uploaded\n",
    "    if len(uploaded) != 1:\n",
    "        print(\"Please upload exactly one file.\")\n",
    "        return None\n",
    "\n",
    "    # Get the uploaded file name and data\n",
    "    file_name, file_data = next(iter(uploaded.items()))\n",
    "\n",
    "    # Check if the file is .pdf or .docx\n",
    "    if not file_name.endswith(('.pdf', '.docx')):\n",
    "        print(\"Invalid file type. Please upload only .pdf or .docx files.\")\n",
    "        return None\n",
    "\n",
    "    # Save the file directly to the /content/ directory\n",
    "    file_path = f'/content/{file_name}'\n",
    "\n",
    "    return file_path\n",
    "\n",
    "cv_file_path = upload_and_filter_file()"
   ],
   "id": "ec63b534771d50f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# local jupyter notebook\n",
    "\n",
    "cv_file_path = r'C:\\Users\\DMA\\Downloads\\CV - 2024-1.pdf'"
   ],
   "id": "2a536813081d7a26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cv_file_path",
   "id": "b6a65c074428d13e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_cv_retriever(file_path, k):\n",
    "    pages = []\n",
    "\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type.\")\n",
    "\n",
    "    for page in loader.load():\n",
    "        pages.append(page)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(pages)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ],
   "id": "9a9266679f8fa046",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cv_retriever = create_cv_retriever(cv_file_path, 5)",
   "id": "bd63f50b2351bd85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_tool = create_retriever_tool(\n",
    "    cv_retriever,\n",
    "    \"search_candidate_info\",\n",
    "    \"Searches and returns candidate's profile with experience, education, and skills.\",\n",
    ")"
   ],
   "id": "1b754be1121566a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo: \n",
    "# Free alternative for embeddings that doesn't use OpenAI"
   ],
   "id": "78b580af1e74d58a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Hiring Company Info Scraper",
   "id": "18422b0153a965fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_wikipedia_content(query):\n",
    "    \"\"\"Fetches content from Wikipedia based on a query.\"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    wikipedia_content = wikipedia.run(query)\n",
    "    return wikipedia_content"
   ],
   "id": "411db1436669cbf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_websites_links(query):\n",
    "    \"\"\"Fetches a list of website links based on a search query using DuckDuckGo.\"\"\"\n",
    "    search = DuckDuckGoSearchResults(output_format=\"list\")\n",
    "    search_results = search.invoke(query)\n",
    "    return [result[\"link\"] for result in search_results]"
   ],
   "id": "d0c94cc9e174acba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_websites_content(websites):\n",
    "    \"\"\"Loads the HTML content of a list of websites.\"\"\"\n",
    "    content_list = []\n",
    "    for website in websites:\n",
    "        loader = AsyncChromiumLoader([website])\n",
    "        html_content = loader.load()\n",
    "        content_list.append(html_content)\n",
    "    return content_list"
   ],
   "id": "6138048ef56616c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def transform_html_content(html_content_list, tags = [\"span\", \"p\", \"b\", \"h3\", \"h4\"]):\n",
    "    \"\"\"Transforms HTML content to extract specific tags using BeautifulSoup.\"\"\"\n",
    "    transformed_content = []\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "    for html in html_content_list:\n",
    "        docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=tags)\n",
    "        for doc in docs_transformed:\n",
    "            transformed_content.append(doc.page_content)\n",
    "    return transformed_content"
   ],
   "id": "61832d87cfa91b8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_web_content(query):\n",
    "    \"\"\"Main function to gather content from Wikipedia and websites based on a query.\"\"\"\n",
    "    content = []\n",
    "\n",
    "    wikipedia_content = get_wikipedia_content(query)\n",
    "    content.append(wikipedia_content)\n",
    "\n",
    "    website_links = get_websites_links(f\"What is {query}?\")\n",
    "\n",
    "    html_content_list = load_websites_content(website_links)\n",
    "\n",
    "    transformed_content = transform_html_content(html_content_list)\n",
    "\n",
    "    content.extend(transformed_content)\n",
    "    return content"
   ],
   "id": "404e031bd1b42305",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"Deloitte Company\"\n",
    "websites_content = get_web_content(query)\n",
    "websites_content"
   ],
   "id": "6e32851a57d96f8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_company_info_retriever(websites_content, k):\n",
    "    docs = []\n",
    "\n",
    "    for website_content in websites_content:\n",
    "        doc = Document(page_content=website_content)\n",
    "        docs.append(doc)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()  # need a FOSS alternative\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ],
   "id": "1392099cca9bc87f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "company_info_retriever = create_company_info_retriever(websites_content, 5)",
   "id": "2e2feee0d40acb9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo: update this tool so it gets correct data, this is copied from the cv\n",
    "\n",
    "company_info_tool = create_retriever_tool(\n",
    "    cv_retriever,\n",
    "    \"search_company_info\",\n",
    "    \"Searches and returns company's profile with company's details to be considered by HR Specialist.\",\n",
    ")"
   ],
   "id": "60d378d450302799",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Querying a Dataset\n",
    "\n",
    "This is an optional tool for enhancing the process of hard skills review.\n",
    "\n",
    "The dataset can be changed depending on the needs of users."
   ],
   "id": "cb6d716952ac5020"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds = \"https://www.kaggle.com/datasets/syedmharis/software-engineering-interview-questions-dataset\"",
   "id": "fada19680d80eb21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_kaggle_ds(dataset_url):\n",
    "    od.download(dataset_url)"
   ],
   "id": "64c587555228f91f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "\n",
    "# Set the file path to the downloaded data and the encoding of the file\n",
    "file_path = r\"C:\\Users\\DMA\\Downloads\\Software Questions.csv\"\n",
    "encoding = \"utf-16\"  # default English encoding\n",
    "\n",
    "loader = CSVLoader(file_path=file_path, encoding=encoding)\n",
    "docs = loader.load()\n"
   ],
   "id": "59829e5ba10feda2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define text splitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ],
   "id": "5ba42e05d275965d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1a Using OpenAI Embeddings ",
   "id": "51f67d516dc51406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_questions_dataset_retriever(texts, k):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ],
   "id": "93fec19fe82b517b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "create_questions_dataset_retriever(texts=texts, k=5)",
   "id": "7c013344d9c5a60c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1b Using HuggingFace Embeddings \n",
    "\n",
    "To represent each chunk as a high-dimensional vector, we’ll use Hugging Face's pre-trained model sentence-transformers/all-MiniLM-L6-v2. This model is efficient and well-suited for generating text embeddings.\n"
   ],
   "id": "9d590160f25aa77e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We’ll define a simple helper class to handle embedding generation using the Hugging Face model.",
   "id": "84977845029fc567"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbeddings:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        # Load the model and tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def embed_texts(self, texts):\n",
    "        # Generate embeddings for each text\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        return np.array(embeddings)"
   ],
   "id": "bf8dca7934bf6da6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let’s generate embeddings for each of the text chunks.",
   "id": "186ce16f9cfb72df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the embedding model\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Generate embeddings for each chunk of text\n",
    "embeddings = embeddings_model.embed_texts([text.page_content for text in texts])"
   ],
   "id": "a897a69ec2ff2f94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After this step, `embeddings` will contain a vector representation of each document chunk.",
   "id": "e5eff59bdd058320"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To make our embeddings searchable, we’ll use FAISS to create an index. This allows us to find the most similar embeddings to any query.",
   "id": "c663811a7f4974e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import faiss\n",
    "\n",
    "# Initialize the FAISS index\n",
    "embedding_dim = embeddings.shape[1]  # Dimension of embeddings\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "faiss_index.add(embeddings)"
   ],
   "id": "ab62481f0456eeae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we’ll define a `retriever` function that, given a query, will embed it and retrieve the most similar document chunks from the FAISS index.",
   "id": "b3a6a9124bc25b62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def retriever(query, texts, embeddings_model, faiss_index, k=5):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embeddings_model.embed_texts([query])[0]\n",
    "    \n",
    "    # Search FAISS index for the top-k similar chunks\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    # Retrieve the corresponding text chunks\n",
    "    results = [texts[i].page_content for i in indices[0]]\n",
    "    return results\n"
   ],
   "id": "795d84f1e759c5cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For testing it:",
   "id": "498063ba154aff57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define your query\n",
    "query = \"What is the topic of interest?\"\n",
    "\n",
    "# Call the retriever with the required arguments\n",
    "results = retriever(query, texts, embeddings_model, faiss_index, k=5)\n",
    "\n",
    "# Print the top results\n",
    "print(\"Top similar chunks:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result}\")\n"
   ],
   "id": "81d35189e601810e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Define the tool for agents",
   "id": "2dc9fc77f1caaec8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo: update this tool so its usable by agents\n",
    "\n",
    "questions_database_tool = create_retriever_tool(\n",
    "    cv_retriever,\n",
    "    \"search_subject_matter_questions\",\n",
    "    \"Searches and returns subject matter questions for checking hard skills.\",\n",
    ")"
   ],
   "id": "e5ae193c12d94107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "-----\n",
    "\n",
    "# Initialize Agents"
   ],
   "id": "8367d4aa565d2239"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Todo:\n",
    "# Need to test this with OAI key\n",
    "# Test each of the tools are working\n",
    "\n",
    "# Create LangGraph agents, give them roles, assign interactions and tools to each\n",
    "\n",
    "# Implement user-agent interaction\n",
    "# LangGraph - https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/human_in_the_loop/wait-user-input.ipynb\n",
    "\n",
    "# Add a FOSS alternative for models"
   ],
   "id": "a32abecc57086fb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "llm = ChatOpenAI(model_name=\"gpt-4o\")  # need a FOSS alternative",
   "id": "2968cd86c5123092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_input_form_with_return():\n",
    "    # Capture inputs\n",
    "    print(\"Invoice input\")\n",
    "    print(\"\")\n",
    "    voice_input = record_and_transcribe_candidate_answer()\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Text input\")\n",
    "    print(\"\")\n",
    "    written_input = record_and_submit_text()\n",
    "\n",
    "    # Define what happens on submit\n",
    "    def on_submit(button):\n",
    "        clear_output()\n",
    "        print(\"Submitted successfully. Moving to the next step...\")\n",
    "\n",
    "    # Create the submit button and link to the on_submit action\n",
    "    print(\"\")\n",
    "    print(\"================================================\")\n",
    "    print(\"Please, click submit button to send your answers\")\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(on_submit)\n",
    "\n",
    "    display(submit_button)\n",
    "\n",
    "    if submit_button:\n",
    "      return voice_input, written_input"
   ],
   "id": "26bfe73049656d06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "voice, text_input = display_input_form_with_return()",
   "id": "1a87f94dee3d8945",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "answer = f\"Answer: {voice['text']}\\n\\n{text_input['text']}\"\n",
    "answer"
   ],
   "id": "855ce6ff531f8e1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ],
   "id": "f1956a344b297ebb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "memory = MemorySaver()",
   "id": "fdc5c6dc75a09db1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ],
   "id": "c242afbd93da45fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "config = {\"configurable\": {\"thread_id\": \"1\"}}",
   "id": "9f7780cecfc20472",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_message = {\"type\": \"user\", \"content\": answer}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ],
   "id": "38b8c4f051218be3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_message = {\"type\": \"user\", \"content\": answer}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ],
   "id": "7c1784f90bf83fad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "-----\n",
    "# Stretch goal: TTS\n",
    "\n",
    "Example:"
   ],
   "id": "720fa4c8cd964f92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define model and TTS pipelines",
   "id": "6d08d9683f86af87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the TTS model\n",
    "tts_pipeline = pipeline(\"text-to-speech\", model=\"espnet/kan-bayashi_ljspeech_vits\")\n"
   ],
   "id": "7210545fca2c5e7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Generate and Play Text with TTS in Real-Time\n",
    "\n",
    "Create a loop where the language model generates text in small chunks. Each chunk will be converted to speech and played immediately."
   ],
   "id": "e26c26c3dd6ea189"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "def generate_and_play_text(prompt, max_chunks=5, chunk_size=50):\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    # Generate text in chunks\n",
    "    for _ in range(max_chunks):\n",
    "        # Generate a chunk of text\n",
    "        output = text_generator(prompt + generated_text, max_new_tokens=chunk_size, do_sample=True)\n",
    "        new_text = output[0][\"generated_text\"][len(prompt + generated_text):]\n",
    "        \n",
    "        # Append the new text to the generated text\n",
    "        generated_text += new_text\n",
    "        print(new_text)  # Print the generated text chunk\n",
    "\n",
    "        # Generate TTS for the current chunk\n",
    "        audio = tts_pipeline(new_text)\n",
    "\n",
    "        # Autoplay the audio chunk in the notebook\n",
    "        ipd.display(ipd.Audio(audio[\"wav\"], autoplay=True))\n",
    "        \n",
    "        # Add a short delay to simulate real-time generation if needed\n",
    "        # time.sleep(1)  # Uncomment if you want to control the timing\n",
    "\n",
    "# Example usage\n",
    "generate_and_play_text(\"Once upon a time,\")\n"
   ],
   "id": "ee81c891045e545",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
