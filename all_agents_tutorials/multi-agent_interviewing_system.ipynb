{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8376f449a4cd31",
   "metadata": {},
   "source": [
    "**TITLE:** MULTI-AGENT INTERVIEWING SYSTEM\n",
    "\n",
    "**DEVELOPERS:**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b669f9e2849047b",
   "metadata": {},
   "source": [
    "# Setup Instructions for Jupyter Notebook\n",
    "\n",
    "This notebook installs essential packages for working with LangChain, OpenAI, and other data handling tools. \n",
    "\n",
    "### Important Notes:\n",
    "- **Google Colab Users**: If you are using Google Colab, ensure to install `google-colab` specific packages. \n",
    "- **GPU Configuration**: If using Google Colab, you can enable GPU for faster performance by going to:\n",
    "  - **Runtime** > **Change runtime type** > **Hardware accelerator** and selecting **GPU**.\n",
    "  \n",
    "---\n",
    "\n",
    "## Step 1: Install General Utilities and Google Colab Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install general utilities and widgets\n",
    "%pip install pandas opendatasets nest_asyncio ipywebrtc ipywidgets IPython \n",
    "\n",
    "%pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64043e93ef176632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if using google-colab, else skip it\n",
    "%pip install google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde274d17b4b8f27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Install OpenAI, LangChain, and Related Tools\n",
    "These packages are necessary for using OpenAIâ€™s language models and LangChain's toolkit for search, document processing, and data handling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ffe9a8e044bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI and related LangChain tools\n",
    "%pip install openai langchain_openai\n",
    "\n",
    "# LangChain Community Tools for search and document handling\n",
    "%pip install langchain_community\n",
    "\n",
    "# Typing extensions and Pydantic\n",
    "%pip install typing_extensions pydantic\n",
    "\n",
    "# LangGraph and experimental LangChain tools\n",
    "%pip install langgraph langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For agent tools\n",
    "%pip install pypdf wikipedia duckduckgo-search playwright\n",
    "\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bbc30c8307592",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Database Utilities, SQLAlchemy, and FAISS for Vector Storage\n",
    "\n",
    "- **Database Utilities**: Install SQLAlchemy for database interactions.\n",
    "- **FAISS**: Choose `faiss-cpu` for CPU environments or `faiss-gpu` if you've enabled GPU support on Colab.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5888251935ea6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database utilities and SQLAlchemy\n",
    "%pip install SQLAlchemy\n",
    "\n",
    "# FAISS for vector storage and retrieval\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!where python\n",
    "!pip show playwright\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa27fcb72b8e53e",
   "metadata": {},
   "source": [
    "## General Imports\n",
    "This cell includes the essential imports needed to use LangChain, OpenAI, and other data handling tools in any Jupyter Notebook or Python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d741d4caeeda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for data handling, display, and LangChain functionality\n",
    "import os\n",
    "import opendatasets as od\n",
    "import nest_asyncio\n",
    "\n",
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChain and related tools\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# LangChain Agents and supporting libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, trim_messages\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Literal, Sequence, List\n",
    "import functools\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6e32631d8416c",
   "metadata": {},
   "source": [
    "## Google Colab Specific Imports\n",
    "This cell should be run only if you're using Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33c03e419f01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab specific imports\n",
    "try:\n",
    "    from google.colab import output\n",
    "    from google.colab import userdata\n",
    "    from google.colab import files\n",
    "    # for colab\n",
    "    output.enable_custom_widget_manager()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70f69fdf69c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41518f1fbffb425c",
   "metadata": {},
   "source": [
    "# Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e34c49167463ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key successfully set: 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\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    # Retrieve API key from Google Colab userdata (if stored there)\n",
    "    open_ai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    # Not running on Google Colab; prompt for API key input or retrieve from environment variables\n",
    "    open_ai_api_key = os.getenv('OPENAI_API_KEY') or input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Set the API key as an environment variable for universal access within the notebook\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_api_key\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "\n",
    "# Confirm setup\n",
    "if open_ai_api_key:\n",
    "    print(f\"API key successfully set: {open_ai_api_key}\")\n",
    "else:\n",
    "    print(\"API key not set. Please check your setup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3bc41d2c30b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: \n",
    "# Need to have an alternative that grabs a HuggingFace API key and interfaces with free models there (Llama-3-8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a360a1929e585",
   "metadata": {},
   "source": [
    "# Create Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241141af1e856401",
   "metadata": {},
   "source": [
    "## 1. Speech-to-text\n",
    "\n",
    "This tool allows the user to record speech and converts it to a text using OpenAI Whisper model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e18d7b5333b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a614597653bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_audio_recorder():\n",
    "    camera = CameraStream(constraints={'audio': True, 'video': False})\n",
    "    recorder = AudioRecorder(stream=camera)\n",
    "    display(recorder)\n",
    "    return recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b682f5e34981ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recording(recorder):\n",
    "    audio_data = recorder.audio.value\n",
    "    if audio_data:\n",
    "        with open(\"recording.webm\", \"wb\") as f:\n",
    "            f.write(audio_data)\n",
    "        return \"recording.webm\"\n",
    "    else:\n",
    "        print(\"No audio data was captured. Please try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8005ca2b61e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(input_filename, output_filename=\"my_recording.wav\"):\n",
    "    if input_filename and os.path.exists(input_filename):\n",
    "        os.system(f\"ffmpeg -i {input_filename} -ac 1 -f wav {output_filename} -y -hide_banner -loglevel panic\")\n",
    "        if os.path.exists(output_filename):\n",
    "            return output_filename\n",
    "        else:\n",
    "            print(\"Conversion failed.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Input file does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d27fc546946d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(filename):\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file\n",
    "        )\n",
    "    print(\"\")\n",
    "    print(\"Transcription:\", transcription.text)\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0a312a15ebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_transcribe_candidate_answer():\n",
    "    \"\"\"Record and transcribe a candidate's answer on interviewers' questions.\"\"\"\n",
    "    # Set up the recorder\n",
    "    recorder = setup_audio_recorder()\n",
    "\n",
    "    # Create a save button\n",
    "    print(\"\")\n",
    "    save_button = widgets.Button(description=\"Save Recording\")\n",
    "\n",
    "    # This dictionary will store the transcribed text\n",
    "    transcription_result = {}\n",
    "\n",
    "    # Define the callback function for the save button\n",
    "    def on_save_clicked(button):\n",
    "        # Save the recording\n",
    "        webm_file = save_recording(recorder)\n",
    "        if webm_file:\n",
    "            # Convert to wav format\n",
    "            wav_file = convert_to_wav(webm_file)\n",
    "            if wav_file:\n",
    "                # Transcribe the audio and store the result\n",
    "                transcription_result['text'] = transcribe_audio(wav_file)\n",
    "\n",
    "    save_button.on_click(on_save_clicked)\n",
    "    display(save_button)\n",
    "\n",
    "    # Return the transcription result dictionary\n",
    "    return transcription_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca99386ecd3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Try to do live transcription, rather than recording a file. \n",
    "# Take a look at https://gist.github.com/Vaibhavs10/a48d141534cc8d877937d421bb828d8e\n",
    "# and https://github.com/VRSEN/langchain-agents-tutorial/blob/main/main.py\n",
    "\n",
    "# FOSS alternative pipeline, that doesn't rely on OpenAI models\n",
    "# Using HF free API instead \n",
    "# Something like https://github.com/nyrahealth/CrisperWhisper?tab=readme-ov-file#31-usage-with--transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d2e5e97f917bb",
   "metadata": {},
   "source": [
    "## 2. Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f961638c01d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_text_input():\n",
    "    text_input = widgets.Textarea(\n",
    "        placeholder=\"Type your answer here...\",\n",
    "        description=\"Answer:\",\n",
    "        layout=widgets.Layout(width='500px', height='100px')\n",
    "    )\n",
    "    display(text_input)\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bc16915c6f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_text_input(text_widget):\n",
    "    user_text = text_widget.value\n",
    "    if user_text.strip():\n",
    "        print(\"\\nInput:\\n\", user_text)\n",
    "        return user_text\n",
    "    else:\n",
    "        print(\"No input was provided. Please type your answer and try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091dca5fa15aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_submit_text():\n",
    "    \"\"\"Record a candidate's text answer on interviewers' questions which require written output like code.\"\"\"\n",
    "    # Set up the text input widget\n",
    "    text_widget = setup_text_input()\n",
    "\n",
    "    # Create a submit button\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Save Answer\")\n",
    "\n",
    "    # This variable will store the submitted text\n",
    "    submission_result = {}\n",
    "\n",
    "    # Define the callback function for the submit button\n",
    "    def on_submit_clicked(button):\n",
    "        # Capture the user's text input and store it in the dictionary\n",
    "        submission_result['text'] = submit_text_input(text_widget)\n",
    "\n",
    "    submit_button.on_click(on_submit_clicked)\n",
    "    display(submit_button)\n",
    "\n",
    "    # Wait for user input to be submitted\n",
    "    return submission_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f0fd76213961e",
   "metadata": {},
   "source": [
    "## 3. CV Reader\n",
    "\n",
    "CV Reader for PDF and DOCX files.\n",
    "\n",
    "Instead of CV you can upload your LinkedIn profile extract, which can be exported in a PDF format.\n",
    "\n",
    "This tools can be easily changed to any file reading service, e.g., Azure DI, LlamaParse, custom parsing with PyPdf, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63b534771d50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab version\n",
    "\n",
    "cv_file_path = None \n",
    "job_description_file_path = None\n",
    "\n",
    "def upload_and_filter_file(title = \"CV\", job_description=False):\n",
    "    # Upload a single file\n",
    "    try:\n",
    "        uploaded = files.upload()\n",
    "            # Check if only one file was uploaded\n",
    "        if len(uploaded) != 1:\n",
    "            print(\"Please upload exactly one file.\")\n",
    "            return None\n",
    "\n",
    "        # Get the uploaded file name and data\n",
    "        file_name, file_data = next(iter(uploaded.items()))\n",
    "\n",
    "        # Check if the file is .pdf or .docx\n",
    "        if not file_name.endswith(('.pdf', '.docx')):\n",
    "            print(\"Invalid file type. Please upload only .pdf or .docx files.\")\n",
    "            return None\n",
    "\n",
    "        # Save the file directly to the /content/ directory\n",
    "        file_path = f'/content/{file_name}'\n",
    "        if job_description:\n",
    "            global job_description_file_path\n",
    "            job_description_file_path = f'/tmp/{filename}'\n",
    "        else:\n",
    "            global cv_file_path\n",
    "            cv_file_path = f'/tmp/{filename}'\n",
    "        return file_path\n",
    "    except NameError:\n",
    "\n",
    "        upload_widget = widgets.FileUpload(\n",
    "            accept='',  # Accept all file types\n",
    "            multiple=False  # Single file upload\n",
    "        )\n",
    "        print(f\"Upload the file with {title}\")\n",
    "        # Display the widget\n",
    "        display(upload_widget)\n",
    "        def handle_upload(change):\n",
    "            for f in upload_widget.value:\n",
    "                filename = f['name']\n",
    "                fileinfo = f['content']\n",
    "            with open(f'/tmp/{filename}', 'wb') as f:\n",
    "                f.write(fileinfo)\n",
    "            if job_description:\n",
    "                global job_description_file_path\n",
    "                job_description_file_path = f'/tmp/{filename}'\n",
    "                print(job_description_file_path)\n",
    "            else:\n",
    "                global cv_file_path\n",
    "                cv_file_path = f'/tmp/{filename}'\n",
    "                print(cv_file_path)\n",
    "\n",
    "        # Bind the handler to the upload event\n",
    "        upload_widget.observe(handle_upload, names='value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06904cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_filter_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a536813081d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local jupyter notebook\n",
    "\n",
    "upload_and_filter_file(title = \"Job Description\", job_description=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9266679f8fa046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cv_retriever(file_path, k):\n",
    "    pages = []\n",
    "\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type.\")\n",
    "\n",
    "    for page in loader.load():\n",
    "        pages.append(page)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(pages)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63f50b2351bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_retriever = create_cv_retriever(cv_file_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b754be1121566a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tool = create_retriever_tool(\n",
    "    cv_retriever,\n",
    "    \"search_candidate_info\",\n",
    "    \"Searches and returns candidate's profile with experience, education, and skills.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ccv_text(file_path):\n",
    "    text = ''\n",
    "\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type.\")\n",
    "\n",
    "    for page in loader.load():\n",
    "        text += page.page_content\n",
    "\n",
    "    return text\n",
    "\n",
    "extract_ccv_text(cv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b580af1e74d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extract_ccv_text(job_description_file_path)\n",
    "# todo: \n",
    "# Free alternative for embeddings that doesn't use OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18422b0153a965fd",
   "metadata": {},
   "source": [
    "## 4. Hiring Company Info Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411db1436669cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(query):\n",
    "    \"\"\"Fetches content from Wikipedia based on a query.\"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    wikipedia_content = wikipedia.run(query)\n",
    "    return wikipedia_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c94cc9e174acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_websites_links(query):\n",
    "    \"\"\"Fetches a list of website links based on a search query using DuckDuckGo.\"\"\"\n",
    "    search = DuckDuckGoSearchResults(output_format=\"list\")\n",
    "    search_results = search.invoke(query)\n",
    "    return [result[\"link\"] for result in search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138048ef56616c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_websites_content(websites):\n",
    "    \"\"\"Loads the HTML content of a list of websites.\"\"\"\n",
    "    content_list = []\n",
    "    for website in websites:\n",
    "        loader = AsyncChromiumLoader([website])\n",
    "        html_content = loader.load()\n",
    "        content_list.append(html_content)\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61832d87cfa91b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_html_content(html_content_list, tags = [\"span\", \"p\", \"b\", \"h3\", \"h4\"]):\n",
    "    \"\"\"Transforms HTML content to extract specific tags using BeautifulSoup.\"\"\"\n",
    "    transformed_content = []\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "    for html in html_content_list:\n",
    "        docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=tags)\n",
    "        for doc in docs_transformed:\n",
    "            transformed_content.append(doc.page_content)\n",
    "    return transformed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e031bd1b42305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_content(query):\n",
    "    \"\"\"Main function to gather content from Wikipedia and websites based on a query.\"\"\"\n",
    "    content = []\n",
    "\n",
    "    wikipedia_content = get_wikipedia_content(query)\n",
    "    content.append(wikipedia_content)\n",
    "\n",
    "    website_links = get_websites_links(f\"What is {query}?\")\n",
    "\n",
    "    html_content_list = load_websites_content(website_links)\n",
    "\n",
    "    transformed_content = transform_html_content(html_content_list)\n",
    "\n",
    "    content.extend(transformed_content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32851a57d96f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Deloitte Company\"\n",
    "websites_content = get_web_content(query)\n",
    "websites_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392099cca9bc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_company_info_retriever(websites_content, k):\n",
    "    docs = []\n",
    "\n",
    "    for website_content in websites_content:\n",
    "        doc = Document(page_content=website_content)\n",
    "        docs.append(doc)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()  # need a FOSS alternative\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2feee0d40acb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info_retriever = create_company_info_retriever(websites_content, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d378d450302799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: update this tool so it gets correct data, this is copied from the cv\n",
    "\n",
    "company_info_tool = create_retriever_tool(\n",
    "    company_info_retriever,\n",
    "    \"search_company_info\",\n",
    "    \"Searches and returns company's profile with company's details to be considered by HR Specialist.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d716952ac5020",
   "metadata": {},
   "source": [
    "## 5. Querying a Dataset\n",
    "\n",
    "This is an optional tool for enhancing the process of hard skills review.\n",
    "\n",
    "The dataset can be changed depending on the needs of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada19680d80eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"https://www.kaggle.com/datasets/syedmharis/software-engineering-interview-questions-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c587555228f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaggle_ds(dataset_url):\n",
    "    od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59829e5ba10feda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "\n",
    "# Set the file path to the downloaded data and the encoding of the file\n",
    "file_path = r\"C:\\Users\\DMA\\Downloads\\Software Questions.csv\"\n",
    "encoding = \"ISO-8859-1\"  # default English encoding\n",
    "\n",
    "loader = CSVLoader(file_path=file_path, encoding=encoding)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba42e05d275965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text splitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f67d516dc51406",
   "metadata": {},
   "source": [
    "### 5.1a Using OpenAI Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fec19fe82b517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_questions_dataset_retriever(texts, k):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c013344d9c5a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_questions_dataset_retriever(texts=texts, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d590160f25aa77e",
   "metadata": {},
   "source": [
    "### 5.1b Using HuggingFace Embeddings \n",
    "\n",
    "To represent each chunk as a high-dimensional vector, weâ€™ll use Hugging Face's pre-trained model sentence-transformers/all-MiniLM-L6-v2. This model is efficient and well-suited for generating text embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84977845029fc567",
   "metadata": {},
   "source": [
    "Weâ€™ll define a simple helper class to handle embedding generation using the Hugging Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8dca7934bf6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbeddings:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        # Load the model and tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def embed_texts(self, texts):\n",
    "        # Generate embeddings for each text\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ce16f9cfb72df",
   "metadata": {},
   "source": [
    "Now, letâ€™s generate embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897a69ec2ff2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Generate embeddings for each chunk of text\n",
    "embeddings = embeddings_model.embed_texts([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eff59bdd058320",
   "metadata": {},
   "source": [
    "After this step, `embeddings` will contain a vector representation of each document chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663811a7f4974e2",
   "metadata": {},
   "source": [
    "To make our embeddings searchable, weâ€™ll use FAISS to create an index. This allows us to find the most similar embeddings to any query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62481f0456eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Initialize the FAISS index\n",
    "embedding_dim = embeddings.shape[1]  # Dimension of embeddings\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6a9124bc25b62",
   "metadata": {},
   "source": [
    "Finally, weâ€™ll define a `retriever` function that, given a query, will embed it and retrieve the most similar document chunks from the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d84f1e759c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query, texts, embeddings_model, faiss_index, k=5):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embeddings_model.embed_texts([query])[0]\n",
    "    \n",
    "    # Search FAISS index for the top-k similar chunks\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    # Retrieve the corresponding text chunks\n",
    "    results = [texts[i].page_content for i in indices[0]]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498063ba154aff57",
   "metadata": {},
   "source": [
    "For testing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d35189e601810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your query\n",
    "query = \"What is the topic of interest?\"\n",
    "\n",
    "# Call the retriever with the required arguments\n",
    "results = retriever(query, texts, embeddings_model, faiss_index, k=5)\n",
    "\n",
    "# Print the top results\n",
    "print(\"Top similar chunks:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9fc77f1caaec8",
   "metadata": {},
   "source": [
    "### 5.2 Define the tool for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae193c12d94107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: update this tool so its usable by agents\n",
    "\n",
    "# @tool\n",
    "questions_database_tool = create_retriever_tool(\n",
    "    create_questions_dataset_retriever,\n",
    "    \"search_subject_matter_questions\",\n",
    "    \"Searches and returns subject matter questions for checking hard skills.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367d4aa565d2239",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Initialize Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32abecc57086fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Need to test this with OAI key\n",
    "# Test each of the tools are working\n",
    "\n",
    "# Create LangGraph agents, give them roles, assign interactions and tools to each\n",
    "\n",
    "# Implement user-agent interaction\n",
    "# LangGraph - https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/human_in_the_loop/wait-user-input.ipynb\n",
    "\n",
    "# Add a FOSS alternative for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968cd86c5123092",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")  # need a FOSS alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfe73049656d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_input_form_with_return():\n",
    "    # Capture inputs\n",
    "    print(\"Invoice input\")\n",
    "    print(\"\")\n",
    "    voice_input = record_and_transcribe_candidate_answer()\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Text input\")\n",
    "    print(\"\")\n",
    "    written_input = record_and_submit_text()\n",
    "\n",
    "    # Define what happens on submit\n",
    "    def on_submit(button):\n",
    "        clear_output()\n",
    "        print(\"Submitted successfully. Moving to the next step...\")\n",
    "\n",
    "    # Create the submit button and link to the on_submit action\n",
    "    print(\"\")\n",
    "    print(\"================================================\")\n",
    "    print(\"Please, click submit button to send your answers\")\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(on_submit)\n",
    "\n",
    "    display(submit_button)\n",
    "\n",
    "    if submit_button:\n",
    "      return voice_input, written_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87f94dee3d8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice, text_input = display_input_form_with_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ce6ff531f8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = f\"Answer: {voice.get('text', '') if voice else ''}\\n\\n{text_input.get('text', '') if text_input else ''}\"\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1956a344b297ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5c6dc75a09db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242afbd93da45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7780cecfc20472",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8c4f051218be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"type\": \"user\", \"content\": answer}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ecc3bc",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293add6",
   "metadata": {},
   "source": [
    "## Workflow Structure\n",
    "\n",
    "- **Orchestration Stage:**\n",
    "  - **Information Gathering:**\n",
    "    - Obtain user documents (CV, Job description) using the `FileUpload` tool.\n",
    "    - Retrieve company information via the `WebScraper` tool.\n",
    "    - Create a \"Scene\" to outline the interview structure.\n",
    "\n",
    "- **Interview Stages:**\n",
    "  - **HR Stage:** Introduce the user, ask general and behavioral questions, and evaluate responses.\n",
    "  - **Manager Stage:** Introduce the team, ask role-specific questions, and evaluate responses.\n",
    "  - **Technical/Field Expert Stage:** Pose field-specific questions, assess technical knowledge, and evaluate responses.\n",
    "\n",
    "- **Feedback Stage:** Compile evaluations from all stages and generate final feedback for the user.\n",
    "\n",
    "---\n",
    "\n",
    "## Graphs and Subgraphs\n",
    "\n",
    "### Subgraphs\n",
    "Each stage is implemented as a **subgraph**, encapsulating its sub-stages (nodes) and logic. Each task is a node.\n",
    "\n",
    "### Parent Graph\n",
    "A **parent graph** orchestrates the overall workflow by chaining the subgraphs. It also manages data flow between stages through transformers or shared state objects.\n",
    "\n",
    "Example of a parent graph:\n",
    "```python\n",
    "parent_graph = StateGraph(BaseModel)\n",
    "\n",
    "# Add Orchestration subgraph\n",
    "parent_graph.add_subgraph('orchestration', orchestration_graph, OrchestrationState)\n",
    "\n",
    "# Add HR stage subgraph\n",
    "parent_graph.add_subgraph('hr', hr_graph, HRState)\n",
    "\n",
    "# Define the flow between subgraphs\n",
    "def transfer_to_hr(orchestration_state: OrchestrationState) -> HRState:\n",
    "    return HRState(\n",
    "        cv=orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "        job_description=orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "        company_info=orchestration_state.company_info,\n",
    "        scene=orchestration_state.scene\n",
    "    )\n",
    "\n",
    "parent_graph.add_edge('orchestration', 'hr', transformer=transfer_to_hr)\n",
    "parent_graph.add_edge('hr', END)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## `AgentState` and Data Flow\n",
    "\n",
    "**Utilizing Multiple `AgentState` Classes:**\n",
    "\n",
    "Each stage can have its own `AgentState` class to encapsulate relevant data, enhancing modularity and clarity. For example:\n",
    "\n",
    "- **Orchestration State:**\n",
    "  ```python\n",
    "  from pydantic import BaseModel\n",
    "\n",
    "  class OrchestrationState(BaseModel):\n",
    "      user_documents: dict = {}\n",
    "      company_info: str = \"\"\n",
    "      scene: str = \"\"\n",
    "  ```\n",
    "\n",
    "- **Interview State:**\n",
    "  ```python\n",
    "  from pydantic import BaseModel\n",
    "\n",
    "  class InterviewState(BaseModel):\n",
    "      scene: str = \"\"\n",
    "      user_documents: dict = {}\n",
    "      company_info: str = \"\"\n",
    "      questions: list = []\n",
    "      user_responses: dict = {}\n",
    "      evaluation: str = \"\"\n",
    "  ```\n",
    "\n",
    "### How Data Flows Between Stages\n",
    "- Use **transformers** to extract data from one state and initialize the next.\n",
    "- Example: Passing data from `OrchestrationState` to `InterviewState`.\n",
    "\n",
    "```python\n",
    "# define orchestration_graph\n",
    "# final output result_orchestration_state \n",
    "\n",
    "# Extract data from OrchestrationState\n",
    "hr_initial_state = InterviewState(\n",
    "    cv=result_orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "    job_description=result_orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "    company_info=result_orchestration_state.company_info,\n",
    "    scene=result_orchestration_state.scene\n",
    ")\n",
    "\n",
    "# define hr_graph\n",
    "# final output result_hr_state \n",
    "\n",
    "# Execute HR graph with pre-initialized starting state\n",
    "result_hr_state = hr_app.invoke(hr_initial_state)\n",
    "\n",
    "# ----- #\n",
    "\n",
    "# Combining Orchestration and HR 'apps'\n",
    "# Define a parent graph\n",
    "parent_graph = StateGraph(BaseModel)\n",
    "\n",
    "# Add Orchestration as a subgraph\n",
    "parent_graph.add_subgraph('orchestration', orchestration_graph, OrchestrationState)\n",
    "\n",
    "# Add HR stage as a subgraph\n",
    "parent_graph.add_subgraph('hr', hr_graph, InterviewState)\n",
    "\n",
    "# Define data transfer and flow\n",
    "def transfer_to_hr(orchestration_state: OrchestrationState) -> HRState:\n",
    "    return HRState(\n",
    "        cv=orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "        job_description=orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "        company_info=orchestration_state.company_info,\n",
    "        scene=orchestration_state.scene\n",
    "    )\n",
    "\n",
    "parent_graph.add_edge('orchestration', 'hr', transformer=transfer_to_hr)\n",
    "parent_graph.add_edge('hr', END)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260be015",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb2438",
   "metadata": {},
   "source": [
    "Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OAI only for now, need to add HF\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911709f",
   "metadata": {},
   "source": [
    "## 2.1. Define Orchestration State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestrationState(BaseModel):\n",
    "    user_documents: dict = Field(default_factory=dict)\n",
    "    company_info: str = \"\"\n",
    "    scene: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_content = get_web_content(\"Deloitte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cb520",
   "metadata": {},
   "source": [
    "Define the tools for the Orchestration stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03248af4",
   "metadata": {},
   "source": [
    "Tools (placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "880f1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def FileUpload(description: str) -> dict:\n",
    "    \"\"\"\n",
    "    Simulate a file upload process.\n",
    "\n",
    "    Args:\n",
    "        description (str): A brief description of the file upload context.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing simulated content for 'CV' and 'Job Description'.\n",
    "    \"\"\"\n",
    "\n",
    "    ccv_text = extract_ccv_text(cv_file_path)\n",
    "    job_description = extract_ccv_text(job_description_file_path)\n",
    "    \n",
    "    return {\"CV\": ccv_text, \"Job Description\": job_description}\n",
    "\n",
    "@tool\n",
    "def WebScraper(company_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulate web scraping to extract company information.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of a company\n",
    "\n",
    "    Returns:\n",
    "        str: Simulated company information extracted from the website.\n",
    "    \"\"\"\n",
    "    web_content = get_web_content(company_name)\n",
    "    content = \"\\n\".join(web_content)\n",
    "\n",
    "    return content\n",
    "\n",
    "@tool\n",
    "def UserInput(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the user for input.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The message displayed to the user.\n",
    "\n",
    "    Returns:\n",
    "        str: The user's input as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO use with coab notebook not always works locally\n",
    "    # print(prompt)\n",
    "    # voice, text_input = display_input_form_with_return()\n",
    "    # answer = f\"Answer: {voice.get('text', '') if voice else ''}\\n\\n{text_input.get('text', '') if text_input else ''}\"\n",
    "\n",
    "\n",
    "    return input(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71fb40",
   "metadata": {},
   "source": [
    "Define Nodes for the three sub-stages of the Orchestration stage:\n",
    "- Document upload\n",
    "- Scrape information about company\n",
    "- Create a \"Scene\" to outline the interview structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the Scene\n",
    "# placeholder\n",
    "\n",
    "GENERATE_SCENE_SYSTEM_PROMPT = \"\"\"\n",
    "    Based on the following:\n",
    "    - CV: {cv}\n",
    "    - Job Description: {job_description}\n",
    "    - Company Info: {company_info}\n",
    "\n",
    "    Generate a \"scene\" for conducting an interview.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Upload Node\n",
    "def upload_documents(state: OrchestrationState):\n",
    "    state.user_documents = FileUpload(\"Upload user documents (CV, job description).\")\n",
    "    return state\n",
    "\n",
    "# Web Scraper Node\n",
    "def scrape_company_info(state: OrchestrationState):\n",
    "    state.company_info = WebScraper(\"https://company.website\")\n",
    "    return state\n",
    "\n",
    "# Scene Generation Node\n",
    "# todo: update system prompt\n",
    "def generate_scene(state: OrchestrationState):\n",
    "    cv = state.user_documents.get(\"CV\", \"\")\n",
    "    job_description = state.user_documents.get(\"Job Description\", \"\")\n",
    "    company_info = state.company_info\n",
    "\n",
    "    system_prompt = GENERATE_SCENE_SYSTEM_PROMPT\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.scene = response.content.strip()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7d63a",
   "metadata": {},
   "source": [
    "Create the graph for the orchestration stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph for orchestration\n",
    "orchestration_graph = StateGraph(OrchestrationState)\n",
    "\n",
    "# Add the nodes for each of the \"sub-stages\"\n",
    "orchestration_graph.add_node('upload_documents', upload_documents)\n",
    "orchestration_graph.add_node('scrape_company_info', scrape_company_info)\n",
    "orchestration_graph.add_node('generate_scene', generate_scene)\n",
    "\n",
    "# Define the flow of tasks\n",
    "orchestration_graph.add_edge(START, 'upload_documents')\n",
    "orchestration_graph.add_edge('upload_documents', 'scrape_company_info')\n",
    "orchestration_graph.add_edge('scrape_company_info', 'generate_scene')\n",
    "orchestration_graph.add_edge('generate_scene', END)\n",
    "\n",
    "# Compile the orchestration graph\n",
    "orchestration_app = orchestration_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf338c1",
   "metadata": {},
   "source": [
    "Visualize orchestration graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(orchestration_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7997e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_orchestration_state = OrchestrationState()\n",
    "result_orchestration_state = orchestration_app.invoke(initial_orchestration_state)\n",
    "result_orchestration_state = OrchestrationState(**result_orchestration_state)\n",
    "print(type(result_orchestration_state))\n",
    "print(result_orchestration_state.scene)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a92335",
   "metadata": {},
   "source": [
    "## 2.2. Interview Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(BaseModel):\n",
    "    scene: str = \"\"\n",
    "    user_documents: dict = {}\n",
    "    company_info: str = \"\"\n",
    "    questions: list = []\n",
    "    user_responses: dict = {}\n",
    "    evaluation: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08bc01",
   "metadata": {},
   "source": [
    "### 2.2.1. Define tools for interview stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tool  # ALREADY DEFINED\n",
    "# def UserInput(prompt: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Simulates user input for demonstration purposes.\n",
    "\n",
    "#     Args:\n",
    "#         prompt (str): The message displayed to the user prompting them for input.\n",
    "\n",
    "#     Returns:\n",
    "#         str: The user's input as a string.\n",
    "    \n",
    "#     Note:\n",
    "#         This function uses Python's `input()` function to capture input. In a real-world scenario, \n",
    "#         it would be used to obtain actual user input interactively.\n",
    "#     \"\"\"\n",
    "#     return input(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d8713",
   "metadata": {},
   "source": [
    "There will be three sub-stages within the Interview stage: \n",
    "- The HR interview\n",
    "- The Manager interview\n",
    "- The technical interview\n",
    "\n",
    "These are quite similar, so they share the base State (InterviewState) and the tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6eff65",
   "metadata": {},
   "source": [
    "### 2.2.2. HR Interview sub-stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386e12d",
   "metadata": {},
   "source": [
    "For the HR Interview sub-stage, there are going to be the following tasks, which we will define as nodes for our graph: \n",
    "1. Introduction: Introduces the candidate and sets the stage for the interview.\n",
    "2. Generate Questions Node: Uses the `Scene`, `User Documents`, and `Company Information` to generate a list of interview questions.\n",
    "3. Ask Questions Node: Iterates through the generated questions, asks the user, and records responses.\n",
    "4. Write Evaluation Node: Summarizes the candidateâ€™s responses and writes an evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d1d48",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3182273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "def introduction(state: InterviewState):\n",
    "    print(f\"Welcome to the interview! Here's an overview: {state.scene}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating questions\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an HR assistant conducting an interview.\n",
    "    Use the following information to generate between 1 and 3 tailored questions:\n",
    "\n",
    "    Scene: {state.scene}\n",
    "    CV: {state.cv}\n",
    "    Job Description: {state.job_description}\n",
    "    Company Information: {state.company_info}\n",
    "\"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.questions = response.content.strip().split('\\n')[:3]  # list slicing to cut off at max 3 questions\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions, using the UserInput tool\n",
    "\n",
    "def ask_questions(state: InterviewState):\n",
    "    while state.questions:\n",
    "        question = state.questions.pop(0)\n",
    "        response = UserInput(question)\n",
    "        state.user_responses[question] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52181c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final evaluation\n",
    "\n",
    "def write_evaluation(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    Based on the following user responses, write a brief evaluation of the candidate:\n",
    "\n",
    "    Responses: {state.user_responses}\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.evaluation = response.content.strip()\n",
    "    print(\"Evaluation:\", state.evaluation)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c15b14",
   "metadata": {},
   "source": [
    "#### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b44edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HR sub-graph\n",
    "hr_graph = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes\n",
    "hr_graph.add_node('introduction', introduction)\n",
    "hr_graph.add_node('generate_questions', generate_questions)\n",
    "hr_graph.add_node('ask_questions', ask_questions)\n",
    "hr_graph.add_node('write_evaluation', write_evaluation)\n",
    "\n",
    "# Define edges\n",
    "hr_graph.add_edge(START, 'introduction')\n",
    "hr_graph.add_edge('introduction', 'generate_questions')\n",
    "hr_graph.add_edge('generate_questions', 'ask_questions')\n",
    "hr_graph.add_edge('ask_questions', 'write_evaluation')\n",
    "hr_graph.add_edge('write_evaluation', END)\n",
    "\n",
    "# Compile the HR graph\n",
    "hr_app = hr_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(hr_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5340813",
   "metadata": {},
   "source": [
    "### 2.2.3. Manager Interview Sub-Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99fe6a",
   "metadata": {},
   "source": [
    "The Manager interview sub-stage is going to be very similar to the HR interview in its structure, but it will differ in the System Prompts we give to the Manager agent, in order to gear it more towards organizational and technical questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e037628",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "def introduction(state: InterviewState):\n",
    "    print(f\"Welcome to the interview! Here's an overview: {state.scene}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb467fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating questions\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an HR assistant conducting an interview.\n",
    "    Use the following information to generate between 1 and 3 tailored questions:\n",
    "\n",
    "    Scene: {state.scene}\n",
    "    CV: {state.cv}\n",
    "    Job Description: {state.job_description}\n",
    "    Company Information: {state.company_info}\n",
    "\"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.questions = response.content.strip().split('\\n')[:3]  # list slicing to cut off at max 3 questions\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98596479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions, using the UserInput tool\n",
    "\n",
    "def ask_questions(state: InterviewState):\n",
    "    while state.questions:\n",
    "        question = state.questions.pop(0)\n",
    "        response = UserInput(question)\n",
    "        state.user_responses[question] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72809b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final evaluation\n",
    "\n",
    "def write_evaluation(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    Based on the following user responses, write a brief evaluation of the candidate:\n",
    "\n",
    "    Responses: {state.user_responses}\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.evaluation = response.content.strip()\n",
    "    print(\"Evaluation:\", state.evaluation)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6f72c",
   "metadata": {},
   "source": [
    "#### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manager sub-graph\n",
    "manager_graph = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes\n",
    "manager_graph.add_node('introduction', introduction)\n",
    "manager_graph.add_node('generate_questions', generate_questions)\n",
    "manager_graph.add_node('ask_questions', ask_questions)\n",
    "manager_graph.add_node('write_evaluation', write_evaluation)\n",
    "\n",
    "# Define edges\n",
    "manager_graph.add_edge(START, 'introduction')\n",
    "manager_graph.add_edge('introduction', 'generate_questions')\n",
    "manager_graph.add_edge('generate_questions', 'ask_questions')\n",
    "manager_graph.add_edge('ask_questions', 'write_evaluation')\n",
    "manager_graph.add_edge('write_evaluation', END)\n",
    "\n",
    "# Compile the Manager graph\n",
    "manager_app = manager_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbca73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(manager_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7d290",
   "metadata": {},
   "source": [
    "### 2.2.4. Field-Specific Interview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa7bf9",
   "metadata": {},
   "source": [
    "The field-specific, or technical interiew, is geared towards asking questions about specifics in the field. For our example, the Field Expert agent can ask programming questions related to Python, Data Science libraries mentioned in the Job Description, etc. \n",
    "\n",
    "Due to this, this interview will differ slightly in its structure, in that there will be no introductory phase, and the questions can be pulled from a database of questions, if one has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d0bc5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ExpertInterviewState(BaseModel):\n",
    "    scene: str = \"\"\n",
    "    user_documents: dict = {}\n",
    "    company_info: str = \"\"\n",
    "    generated_query: str = \"\"\n",
    "    retrieved_context: str = \"\"\n",
    "    questions: list = []\n",
    "    user_responses: dict = {}\n",
    "    evaluation: str = \"\"\n",
    "    db_is_available: bool = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d8605",
   "metadata": {},
   "source": [
    "#### Tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "dba59b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def query_questions_database(query: str) -> str: # TODO on pause do not use for now\n",
    "    \"\"\"\n",
    "    Query the FAISS database for the most relevant questions or context.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved documents as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve relevant documents\n",
    "        results = retriever.get_relevant_documents(query)\n",
    "\n",
    "        # Format results as a string for the tool's output\n",
    "        formatted_results = \"\\n\".join([doc.page_content for doc in results])\n",
    "        return formatted_results\n",
    "    except Exception as e:\n",
    "        return f\"Error querying the database: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02390626",
   "metadata": {},
   "source": [
    "#### Nodes\n",
    "\n",
    "Placeholders, prompts need to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "896d4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(state: InterviewState):\n",
    "    state.generated_query = f\"\"\"\n",
    "    Generate questions for the following:\n",
    "    - Scene: {state.scene}\n",
    "    - CV: {state.user_documents.get('CV', '')}\n",
    "    - Job Description: {state.user_documents.get('Job Description', '')}\n",
    "    - Company Information: {state.company_info}\n",
    "\n",
    "    Focus on retrieving relevant topics for the interview.\n",
    "    \"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def retrieve_context(state: InterviewState):\n",
    "    if \"DB_AVAILABLE\" in globals():  # Check if the database is available\n",
    "        # Use the generated query to search the database\n",
    "        results = query_questions_database(state.generated_query)\n",
    "        state.retrieved_context = \"\\n\".join(results.split('\\n'))  # Combine matches into context\n",
    "    else:\n",
    "        state.retrieved_context = \"\"  # No context retrieved if DB is unavailable\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    # Generate questions using retrieved context (if available) and other information\n",
    "    system_prompt = f\"\"\"\n",
    "    Using the retrieved context below, generate three new interview questions:\n",
    "    \n",
    "    Retrieved Context:\n",
    "    {state.retrieved_context}\n",
    "    \n",
    "    Additional Information:\n",
    "    - Scene: {state.scene}\n",
    "    - CV: {state.user_documents.get('CV', '')}\n",
    "    - Job Description: {state.user_documents.get('Job Description', '')}\n",
    "    - Company Information: {state.company_info}\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.questions = response.content.strip().split('\\n')\n",
    "    return state\n",
    "\n",
    "def ask_questions(state: InterviewState):\n",
    "    while state.questions:\n",
    "        question = state.questions.pop(0)\n",
    "        response = UserInput(question)  # Capture user input via tool\n",
    "        state.user_responses[question] = response\n",
    "    return state\n",
    "\n",
    "def evaluate_answers(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    Evaluate the following user responses based on the context:\n",
    "    - Scene: {state.scene}\n",
    "    - CV: {state.user_documents.get('CV', '')}\n",
    "    - Job Description: {state.user_documents.get('Job Description', '')}\n",
    "    - Company Information: {state.company_info}\n",
    "    - User Responses: {state.user_responses}\n",
    "    \n",
    "    Provide a detailed evaluation.\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.evaluation = response.content.strip()\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46142c81",
   "metadata": {},
   "source": [
    "#### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1627b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Interview sub-graph\n",
    "expert_interview_graph = StateGraph(ExpertInterviewState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "expert_interview_graph.add_node('generate_query', generate_query)\n",
    "expert_interview_graph.add_node('retrieve_context', retrieve_context)\n",
    "expert_interview_graph.add_node('generate_questions', generate_questions)\n",
    "expert_interview_graph.add_node('ask_questions', ask_questions)\n",
    "expert_interview_graph.add_node('evaluate_answers', evaluate_answers)\n",
    "\n",
    "# Define the condition function for the start node\n",
    "def start_condition(state: ExpertInterviewState):\n",
    "    if state.db_is_available:\n",
    "        return 'generate_query'\n",
    "    else:\n",
    "        return 'generate_questions'\n",
    "\n",
    "# Add edges\n",
    "expert_interview_graph.add_conditional_edges(START, start_condition, {'generate_query': 'generate_query', 'generate_questions': 'generate_questions'})\n",
    "expert_interview_graph.add_edge('generate_query', 'retrieve_context')\n",
    "expert_interview_graph.add_edge('retrieve_context', 'generate_questions')\n",
    "expert_interview_graph.add_edge('generate_questions', 'ask_questions')\n",
    "expert_interview_graph.add_edge('ask_questions', 'evaluate_answers')\n",
    "expert_interview_graph.add_edge('evaluate_answers', END)\n",
    "\n",
    "# Compile the graph\n",
    "expert_interview_app = expert_interview_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "6dbbd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(expert_interview_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12916755",
   "metadata": {},
   "source": [
    "### 2.2.5. Connecting the interview subgraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd79de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_graph = StateGraph(BaseModel)\n",
    "\n",
    "# Add HR stage as a subgraph\n",
    "interview_graph.add_node('hr', hr_app)\n",
    "\n",
    "# Add Manager stage as a subgraph\n",
    "interview_graph.add_node('manager', manager_app)\n",
    "\n",
    "# Add Field expert stage as a subgraph\n",
    "interview_graph.add_node('expert', expert_interview_app)\n",
    "\n",
    "\n",
    "# add edges\n",
    "interview_graph.add_edge(START, \"hr\")\n",
    "interview_graph.add_edge(\"hr\", \"manager\")\n",
    "interview_graph.add_edge(\"manager\", \"expert\")\n",
    "interview_graph.add_edge(\"expert\", END)\n",
    "parent_graph = interview_graph.compile()\n",
    "\n",
    "# Compile the graph\n",
    "interview_app = interview_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "mermaid_diagram = interview_app.get_graph(xray=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba27d1",
   "metadata": {},
   "source": [
    "## 2.3. Feedback subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af8b85",
   "metadata": {},
   "source": [
    "This will be the simplest stage, where we pass along the context from the Orchestration stage, the feedback of each of the interviewers, and generate a comprehensive feedback document as the final summary. The final output will be saved as a PDF, using a markdown to PDF tool we will define. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fef748",
   "metadata": {},
   "source": [
    "#### PDF writing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install markdown reportlab beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1443c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdown import markdown\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.pagesizes import letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ea629",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def markdown_to_pdf(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a Markdown string to a PDF file.\n",
    "\n",
    "    Args:\n",
    "    - markdown_text (str): The Markdown-formatted string.\n",
    "\n",
    "    Returns:\n",
    "    - str: Confirmation message upon successful creation of the PDF.\n",
    "    \"\"\"\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown(markdown_text)\n",
    "    \n",
    "    # Create a PDF document\n",
    "    doc = SimpleDocTemplate(\"feedback.pdf\", pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = []\n",
    "\n",
    "    # Split HTML into paragraphs\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    for element in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\"]):\n",
    "        style = styles[\"BodyText\"]\n",
    "        if element.name.startswith(\"h\"):\n",
    "            style = styles[\"Heading{}\".format(min(int(element.name[1]), 4))]\n",
    "        story.append(Paragraph(element.text, style))\n",
    "\n",
    "    # Build the PDF\n",
    "    doc.build(story)\n",
    "    return \"PDF generated: 'feedback.pdf'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9727a",
   "metadata": {},
   "source": [
    "#### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackState(BaseModel):\n",
    "    hr_feedback: str = \"\"\n",
    "    manager_feedback: str = \"\"\n",
    "    field_specialist_feedback: str = \"\"\n",
    "    consolidated_feedback: str = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df9cf0",
   "metadata": {},
   "source": [
    "Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_feedback(state: FeedbackState, hr_state: InterviewState, manager_state: InterviewState, expert_state: ExpertInterviewState):\n",
    "    state.hr_feedback = hr_state.evaluation\n",
    "    state.manager_feedback = manager_state.evaluation\n",
    "    state.field_specialist_feedback = expert_state.evaluation\n",
    "    return state\n",
    "\n",
    "def write_feedback(state: FeedbackState):\n",
    "    # Construct the system prompt for the LLM\n",
    "    system_prompt = \"\"\"\n",
    "    You are an experienced interviewer assistant. Based on the feedback provided by the HR team, the Manager, and the Field Specialist, generate a professional and concise final feedback for the candidate.\n",
    "    Ensure the feedback is holistic, constructive, and highlights key points.\n",
    "\n",
    "    Here is the feedback from each stage:\n",
    "    \"\"\"\n",
    "\n",
    "    # Append individual feedback to the system prompt\n",
    "    system_prompt += f\"\"\"\n",
    "    HR Feedback:\n",
    "    {state.hr_feedback}\n",
    "\n",
    "    Manager Feedback:\n",
    "    {state.manager_feedback}\n",
    "\n",
    "    Field Specialist Feedback:\n",
    "    {state.field_specialist_feedback}\n",
    "\n",
    "    Provide a summary that combines and synthesizes all the points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the LLM to generate the final feedback\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.consolidated_feedback = response.content.strip()\n",
    "    return state\n",
    "\n",
    "def generate_pdf(state: FeedbackState):\n",
    "    confirmation_message = markdown_to_pdf(state.consolidated_feedback)\n",
    "    print(confirmation_message)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c3e74",
   "metadata": {},
   "source": [
    "Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1002053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Define the feedback graph\n",
    "feedback_graph = StateGraph(FeedbackState)\n",
    "\n",
    "# Add nodes\n",
    "feedback_graph.add_node('gather_feedback', gather_feedback)\n",
    "feedback_graph.add_node('write_feedback', write_feedback)\n",
    "feedback_graph.add_node('generate_pdf', generate_pdf)\n",
    "\n",
    "# Define edges\n",
    "feedback_graph.add_edge(START, 'gather_feedback')\n",
    "feedback_graph.add_edge('gather_feedback', 'write_feedback')\n",
    "feedback_graph.add_edge('write_feedback', 'generate_pdf')\n",
    "feedback_graph.add_edge('generate_pdf', END)\n",
    "\n",
    "# Compile the feedback graph\n",
    "feedback_app = feedback_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(feedback_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968575c0",
   "metadata": {},
   "source": [
    "# 3. Connecting the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "da8479d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_graph = StateGraph(BaseModel)\n",
    "\n",
    "# Add Orchestration stage as a subgraph\n",
    "parent_graph.add_node('orchestration', orchestration_app)\n",
    "\n",
    "# Add Interview stage as a subgraph\n",
    "parent_graph.add_node('interview', interview_app)\n",
    "\n",
    "# Add Feedback stage as a subgraph\n",
    "parent_graph.add_node('feedback', feedback_app)\n",
    "\n",
    "\n",
    "# add edges\n",
    "parent_graph.add_edge(START, \"orchestration\")\n",
    "parent_graph.add_edge(\"orchestration\", \"interview\")\n",
    "parent_graph.add_edge(\"interview\", \"feedback\")\n",
    "parent_graph.add_edge(\"feedback\", END)\n",
    "\n",
    "# Compile the graph\n",
    "parent_app = parent_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "971f39de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(parent_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05ecbc",
   "metadata": {},
   "source": [
    "## 3.4. Transferring data between subgraph `State`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea607056",
   "metadata": {},
   "source": [
    "To ensure seamless data transfer from the Orchestration graph to the Interview sub-graphs, we need to:\n",
    "1. Extract relevant data from the OrchestrationState (e.g., `User Documents`, `Company Info`, `Scene`).\n",
    "2. Transform it into an appropriate format for the HRState.\n",
    "3. Use a transformer function in the parent graph to bridge the data between the two sub-graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_interview(orchestration_state: OrchestrationState) -> InterviewState:\n",
    "    return InterviewState(\n",
    "        cv=orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "        job_description=orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "        company_info=orchestration_state.company_info,\n",
    "        scene=orchestration_state.scene\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fa4c8cd964f92",
   "metadata": {},
   "source": [
    "-----\n",
    "# Stretch goal: TTS\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08d9683f86af87",
   "metadata": {},
   "source": [
    "Define model and TTS pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210545fca2c5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the TTS model\n",
    "tts_pipeline = pipeline(\"text-to-speech\", model=\"espnet/kan-bayashi_ljspeech_vits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c26c3dd6ea189",
   "metadata": {},
   "source": [
    "Generate and Play Text with TTS in Real-Time\n",
    "\n",
    "Create a loop where the language model generates text in small chunks. Each chunk will be converted to speech and played immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81c891045e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "def generate_and_play_text(prompt, max_chunks=5, chunk_size=50):\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    # Generate text in chunks\n",
    "    for _ in range(max_chunks):\n",
    "        # Generate a chunk of text\n",
    "        output = text_generator(prompt + generated_text, max_new_tokens=chunk_size, do_sample=True)\n",
    "        new_text = output[0][\"generated_text\"][len(prompt + generated_text):]\n",
    "        \n",
    "        # Append the new text to the generated text\n",
    "        generated_text += new_text\n",
    "        print(new_text)  # Print the generated text chunk\n",
    "\n",
    "        # Generate TTS for the current chunk\n",
    "        audio = tts_pipeline(new_text)\n",
    "\n",
    "        # Autoplay the audio chunk in the notebook\n",
    "        ipd.display(ipd.Audio(audio[\"wav\"], autoplay=True))\n",
    "        \n",
    "        # Add a short delay to simulate real-time generation if needed\n",
    "        # time.sleep(1)  # Uncomment if you want to control the timing\n",
    "\n",
    "# Example usage\n",
    "generate_and_play_text(\"Once upon a time,\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_agents",
   "language": "python",
   "name": "genai_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
