{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8376f449a4cd31",
   "metadata": {},
   "source": [
    "**TITLE:** MULTI-AGENT INTERVIEWING SYSTEM\n",
    "\n",
    "**DEVELOPERS:**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b669f9e2849047b",
   "metadata": {},
   "source": [
    "# Setup Instructions for Jupyter Notebook\n",
    "\n",
    "This notebook installs essential packages for working with LangChain, OpenAI, and other data handling tools. \n",
    "\n",
    "### Important Notes:\n",
    "- **Google Colab Users**: If you are using Google Colab, ensure to install `google-colab` specific packages. \n",
    "- **GPU Configuration**: If using Google Colab, you can enable GPU for faster performance by going to:\n",
    "  - **Runtime** > **Change runtime type** > **Hardware accelerator** and selecting **GPU**.\n",
    "  \n",
    "---\n",
    "\n",
    "## Step 1: Install General Utilities and Google Colab Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install general utilities and widgets\n",
    "%pip install pandas opendatasets nest_asyncio ipywebrtc ipywidgets IPython "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64043e93ef176632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if using google-colab, else skip it\n",
    "%pip install google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde274d17b4b8f27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Install OpenAI, LangChain, and Related Tools\n",
    "These packages are necessary for using OpenAI’s language models and LangChain's toolkit for search, document processing, and data handling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ffe9a8e044bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI and related LangChain tools\n",
    "%pip install openai langchain_openai\n",
    "\n",
    "# LangChain Community Tools for search and document handling\n",
    "%pip install langchain_community\n",
    "\n",
    "# Typing extensions and Pydantic\n",
    "%pip install typing_extensions pydantic\n",
    "\n",
    "# LangGraph and experimental LangChain tools\n",
    "%pip install langgraph langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For agent tools\n",
    "%pip install pypdf wikipedia duckduckgo-search playwright\n",
    "\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bbc30c8307592",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Database Utilities, SQLAlchemy, and FAISS for Vector Storage\n",
    "\n",
    "- **Database Utilities**: Install SQLAlchemy for database interactions.\n",
    "- **FAISS**: Choose `faiss-cpu` for CPU environments or `faiss-gpu` if you've enabled GPU support on Colab.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5888251935ea6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database utilities and SQLAlchemy\n",
    "%pip install SQLAlchemy\n",
    "\n",
    "# FAISS for vector storage and retrieval\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!where python\n",
    "!pip show playwright\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa27fcb72b8e53e",
   "metadata": {},
   "source": [
    "## General Imports\n",
    "This cell includes the essential imports needed to use LangChain, OpenAI, and other data handling tools in any Jupyter Notebook or Python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d741d4caeeda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for data handling, display, and LangChain functionality\n",
    "import os\n",
    "import opendatasets as od\n",
    "import nest_asyncio\n",
    "\n",
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChain and related tools\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# LangChain Agents and supporting libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, trim_messages\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Literal, Sequence, List\n",
    "import functools\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6e32631d8416c",
   "metadata": {},
   "source": [
    "## Google Colab Specific Imports\n",
    "This cell should be run only if you're using Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33c03e419f01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab specific imports\n",
    "from google.colab import output\n",
    "from google.colab import userdata\n",
    "from google.colab import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70f69fdf69c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41518f1fbffb425c",
   "metadata": {},
   "source": [
    "# Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c49167463ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    # Retrieve API key from Google Colab userdata (if stored there)\n",
    "    open_ai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    # Not running on Google Colab; prompt for API key input or retrieve from environment variables\n",
    "    open_ai_api_key = os.getenv('OPENAI_API_KEY') or input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Set the API key as an environment variable for universal access within the notebook\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_api_key\n",
    "\n",
    "# Confirm setup\n",
    "if open_ai_api_key:\n",
    "    print(f\"API key successfully set: {open_ai_api_key}\")\n",
    "else:\n",
    "    print(\"API key not set. Please check your setup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3bc41d2c30b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: \n",
    "# Need to have an alternative that grabs a HuggingFace API key and interfaces with free models there (Llama-3-8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a360a1929e585",
   "metadata": {},
   "source": [
    "# Create Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241141af1e856401",
   "metadata": {},
   "source": [
    "## 1. Speech-to-text\n",
    "\n",
    "This tool allows the user to record speech and converts it to a text using OpenAI Whisper model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e18d7b5333b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7ad06af747527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a614597653bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_audio_recorder():\n",
    "    camera = CameraStream(constraints={'audio': True, 'video': False})\n",
    "    recorder = AudioRecorder(stream=camera)\n",
    "    display(recorder)\n",
    "    return recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b682f5e34981ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recording(recorder):\n",
    "    audio_data = recorder.audio.value\n",
    "    if audio_data:\n",
    "        with open(\"recording.webm\", \"wb\") as f:\n",
    "            f.write(audio_data)\n",
    "        return \"recording.webm\"\n",
    "    else:\n",
    "        print(\"No audio data was captured. Please try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8005ca2b61e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(input_filename, output_filename=\"my_recording.wav\"):\n",
    "    if input_filename and os.path.exists(input_filename):\n",
    "        os.system(f\"ffmpeg -i {input_filename} -ac 1 -f wav {output_filename} -y -hide_banner -loglevel panic\")\n",
    "        if os.path.exists(output_filename):\n",
    "            return output_filename\n",
    "        else:\n",
    "            print(\"Conversion failed.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Input file does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d27fc546946d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(filename):\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file\n",
    "        )\n",
    "    print(\"\")\n",
    "    print(\"Transcription:\", transcription.text)\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0a312a15ebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_transcribe_candidate_answer():\n",
    "    \"\"\"Record and transcribe a candidate's answer on interviewers' questions.\"\"\"\n",
    "    # Set up the recorder\n",
    "    recorder = setup_audio_recorder()\n",
    "\n",
    "    # Create a save button\n",
    "    print(\"\")\n",
    "    save_button = widgets.Button(description=\"Save Recording\")\n",
    "\n",
    "    # This dictionary will store the transcribed text\n",
    "    transcription_result = {}\n",
    "\n",
    "    # Define the callback function for the save button\n",
    "    def on_save_clicked(button):\n",
    "        # Save the recording\n",
    "        webm_file = save_recording(recorder)\n",
    "        if webm_file:\n",
    "            # Convert to wav format\n",
    "            wav_file = convert_to_wav(webm_file)\n",
    "            if wav_file:\n",
    "                # Transcribe the audio and store the result\n",
    "                transcription_result['text'] = transcribe_audio(wav_file)\n",
    "\n",
    "    save_button.on_click(on_save_clicked)\n",
    "    display(save_button)\n",
    "\n",
    "    # Return the transcription result dictionary\n",
    "    return transcription_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca99386ecd3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Try to do live transcription, rather than recording a file. \n",
    "# Take a look at https://gist.github.com/Vaibhavs10/a48d141534cc8d877937d421bb828d8e\n",
    "# and https://github.com/VRSEN/langchain-agents-tutorial/blob/main/main.py\n",
    "\n",
    "# FOSS alternative pipeline, that doesn't rely on OpenAI models\n",
    "# Using HF free API instead \n",
    "# Something like https://github.com/nyrahealth/CrisperWhisper?tab=readme-ov-file#31-usage-with--transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d2e5e97f917bb",
   "metadata": {},
   "source": [
    "## 2. Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f961638c01d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_text_input():\n",
    "    text_input = widgets.Textarea(\n",
    "        placeholder=\"Type your answer here...\",\n",
    "        description=\"Answer:\",\n",
    "        layout=widgets.Layout(width='500px', height='100px')\n",
    "    )\n",
    "    display(text_input)\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bc16915c6f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_text_input(text_widget):\n",
    "    user_text = text_widget.value\n",
    "    if user_text.strip():\n",
    "        print(\"\\nInput:\\n\", user_text)\n",
    "        return user_text\n",
    "    else:\n",
    "        print(\"No input was provided. Please type your answer and try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091dca5fa15aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_submit_text():\n",
    "    \"\"\"Record a candidate's text answer on interviewers' questions which require written output like code.\"\"\"\n",
    "    # Set up the text input widget\n",
    "    text_widget = setup_text_input()\n",
    "\n",
    "    # Create a submit button\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Save Answer\")\n",
    "\n",
    "    # This variable will store the submitted text\n",
    "    submission_result = {}\n",
    "\n",
    "    # Define the callback function for the submit button\n",
    "    def on_submit_clicked(button):\n",
    "        # Capture the user's text input and store it in the dictionary\n",
    "        submission_result['text'] = submit_text_input(text_widget)\n",
    "\n",
    "    submit_button.on_click(on_submit_clicked)\n",
    "    display(submit_button)\n",
    "\n",
    "    # Wait for user input to be submitted\n",
    "    return submission_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f0fd76213961e",
   "metadata": {},
   "source": [
    "## 3. CV Reader\n",
    "\n",
    "CV Reader for PDF and DOCX files.\n",
    "\n",
    "Instead of CV you can upload your LinkedIn profile extract, which can be exported in a PDF format.\n",
    "\n",
    "This tools can be easily changed to any file reading service, e.g., Azure DI, LlamaParse, custom parsing with PyPdf, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63b534771d50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab version\n",
    "\n",
    "def upload_and_filter_file():\n",
    "    # Upload a single file\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    # Check if only one file was uploaded\n",
    "    if len(uploaded) != 1:\n",
    "        print(\"Please upload exactly one file.\")\n",
    "        return None\n",
    "\n",
    "    # Get the uploaded file name and data\n",
    "    file_name, file_data = next(iter(uploaded.items()))\n",
    "\n",
    "    # Check if the file is .pdf or .docx\n",
    "    if not file_name.endswith(('.pdf', '.docx')):\n",
    "        print(\"Invalid file type. Please upload only .pdf or .docx files.\")\n",
    "        return None\n",
    "\n",
    "    # Save the file directly to the /content/ directory\n",
    "    file_path = f'/content/{file_name}'\n",
    "\n",
    "    return file_path\n",
    "\n",
    "cv_file_path = upload_and_filter_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a536813081d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local jupyter notebook\n",
    "\n",
    "cv_file_path = r'C:\\Users\\DMA\\Downloads\\CV - 2024-1.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a65c074428d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9266679f8fa046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cv_retriever(file_path, k):\n",
    "    pages = []\n",
    "\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type.\")\n",
    "\n",
    "    for page in loader.load():\n",
    "        pages.append(page)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(pages)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63f50b2351bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_retriever = create_cv_retriever(cv_file_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b754be1121566a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tool = create_retriever_tool(\n",
    "    cv_retriever,\n",
    "    \"search_candidate_info\",\n",
    "    \"Searches and returns candidate's profile with experience, education, and skills.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b580af1e74d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: \n",
    "# Free alternative for embeddings that doesn't use OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18422b0153a965fd",
   "metadata": {},
   "source": [
    "## 4. Hiring Company Info Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411db1436669cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(query):\n",
    "    \"\"\"Fetches content from Wikipedia based on a query.\"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    wikipedia_content = wikipedia.run(query)\n",
    "    return wikipedia_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c94cc9e174acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_websites_links(query):\n",
    "    \"\"\"Fetches a list of website links based on a search query using DuckDuckGo.\"\"\"\n",
    "    search = DuckDuckGoSearchResults(output_format=\"list\")\n",
    "    search_results = search.invoke(query)\n",
    "    return [result[\"link\"] for result in search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138048ef56616c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_websites_content(websites):\n",
    "    \"\"\"Loads the HTML content of a list of websites.\"\"\"\n",
    "    content_list = []\n",
    "    for website in websites:\n",
    "        loader = AsyncChromiumLoader([website])\n",
    "        html_content = loader.load()\n",
    "        content_list.append(html_content)\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61832d87cfa91b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_html_content(html_content_list, tags = [\"span\", \"p\", \"b\", \"h3\", \"h4\"]):\n",
    "    \"\"\"Transforms HTML content to extract specific tags using BeautifulSoup.\"\"\"\n",
    "    transformed_content = []\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "    for html in html_content_list:\n",
    "        docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=tags)\n",
    "        for doc in docs_transformed:\n",
    "            transformed_content.append(doc.page_content)\n",
    "    return transformed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e031bd1b42305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_content(query):\n",
    "    \"\"\"Main function to gather content from Wikipedia and websites based on a query.\"\"\"\n",
    "    content = []\n",
    "\n",
    "    wikipedia_content = get_wikipedia_content(query)\n",
    "    content.append(wikipedia_content)\n",
    "\n",
    "    website_links = get_websites_links(f\"What is {query}?\")\n",
    "\n",
    "    html_content_list = load_websites_content(website_links)\n",
    "\n",
    "    transformed_content = transform_html_content(html_content_list)\n",
    "\n",
    "    content.extend(transformed_content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32851a57d96f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Deloitte Company\"\n",
    "websites_content = get_web_content(query)\n",
    "websites_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392099cca9bc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_company_info_retriever(websites_content, k):\n",
    "    docs = []\n",
    "\n",
    "    for website_content in websites_content:\n",
    "        doc = Document(page_content=website_content)\n",
    "        docs.append(doc)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()  # need a FOSS alternative\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2feee0d40acb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info_retriever = create_company_info_retriever(websites_content, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d378d450302799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: update this tool so it gets correct data, this is copied from the cv\n",
    "\n",
    "company_info_tool = create_retriever_tool(\n",
    "    company_info_retriever,\n",
    "    \"search_company_info\",\n",
    "    \"Searches and returns company's profile with company's details to be considered by HR Specialist.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d716952ac5020",
   "metadata": {},
   "source": [
    "## 5. Querying a Dataset\n",
    "\n",
    "This is an optional tool for enhancing the process of hard skills review.\n",
    "\n",
    "The dataset can be changed depending on the needs of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada19680d80eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"https://www.kaggle.com/datasets/syedmharis/software-engineering-interview-questions-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c587555228f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaggle_ds(dataset_url):\n",
    "    od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59829e5ba10feda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "\n",
    "# Set the file path to the downloaded data and the encoding of the file\n",
    "file_path = r\"C:\\Users\\DMA\\Downloads\\Software Questions.csv\"\n",
    "encoding = \"ISO-8859-1\"  # default English encoding\n",
    "\n",
    "loader = CSVLoader(file_path=file_path, encoding=encoding)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba42e05d275965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text splitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f67d516dc51406",
   "metadata": {},
   "source": [
    "### 5.1a Using OpenAI Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fec19fe82b517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_questions_dataset_retriever(texts, k):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c013344d9c5a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_questions_dataset_retriever(texts=texts, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d590160f25aa77e",
   "metadata": {},
   "source": [
    "### 5.1b Using HuggingFace Embeddings \n",
    "\n",
    "To represent each chunk as a high-dimensional vector, we’ll use Hugging Face's pre-trained model sentence-transformers/all-MiniLM-L6-v2. This model is efficient and well-suited for generating text embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84977845029fc567",
   "metadata": {},
   "source": [
    "We’ll define a simple helper class to handle embedding generation using the Hugging Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8dca7934bf6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbeddings:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        # Load the model and tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def embed_texts(self, texts):\n",
    "        # Generate embeddings for each text\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ce16f9cfb72df",
   "metadata": {},
   "source": [
    "Now, let’s generate embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897a69ec2ff2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Generate embeddings for each chunk of text\n",
    "embeddings = embeddings_model.embed_texts([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eff59bdd058320",
   "metadata": {},
   "source": [
    "After this step, `embeddings` will contain a vector representation of each document chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663811a7f4974e2",
   "metadata": {},
   "source": [
    "To make our embeddings searchable, we’ll use FAISS to create an index. This allows us to find the most similar embeddings to any query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62481f0456eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Initialize the FAISS index\n",
    "embedding_dim = embeddings.shape[1]  # Dimension of embeddings\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6a9124bc25b62",
   "metadata": {},
   "source": [
    "Finally, we’ll define a `retriever` function that, given a query, will embed it and retrieve the most similar document chunks from the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d84f1e759c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query, texts, embeddings_model, faiss_index, k=5):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embeddings_model.embed_texts([query])[0]\n",
    "    \n",
    "    # Search FAISS index for the top-k similar chunks\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    # Retrieve the corresponding text chunks\n",
    "    results = [texts[i].page_content for i in indices[0]]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498063ba154aff57",
   "metadata": {},
   "source": [
    "For testing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d35189e601810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your query\n",
    "query = \"What is the topic of interest?\"\n",
    "\n",
    "# Call the retriever with the required arguments\n",
    "results = retriever(query, texts, embeddings_model, faiss_index, k=5)\n",
    "\n",
    "# Print the top results\n",
    "print(\"Top similar chunks:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9fc77f1caaec8",
   "metadata": {},
   "source": [
    "### 5.2 Define the tool for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae193c12d94107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: update this tool so its usable by agents\n",
    "\n",
    "@tool\n",
    "questions_database_tool = create_retriever_tool(\n",
    "    create_questions_dataset_retriever,\n",
    "    \"search_subject_matter_questions\",\n",
    "    \"Searches and returns subject matter questions for checking hard skills.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367d4aa565d2239",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Initialize Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32abecc57086fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Need to test this with OAI key\n",
    "# Test each of the tools are working\n",
    "\n",
    "# Create LangGraph agents, give them roles, assign interactions and tools to each\n",
    "\n",
    "# Implement user-agent interaction\n",
    "# LangGraph - https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/human_in_the_loop/wait-user-input.ipynb\n",
    "\n",
    "# Add a FOSS alternative for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968cd86c5123092",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")  # need a FOSS alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfe73049656d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_input_form_with_return():\n",
    "    # Capture inputs\n",
    "    print(\"Invoice input\")\n",
    "    print(\"\")\n",
    "    voice_input = record_and_transcribe_candidate_answer()\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Text input\")\n",
    "    print(\"\")\n",
    "    written_input = record_and_submit_text()\n",
    "\n",
    "    # Define what happens on submit\n",
    "    def on_submit(button):\n",
    "        clear_output()\n",
    "        print(\"Submitted successfully. Moving to the next step...\")\n",
    "\n",
    "    # Create the submit button and link to the on_submit action\n",
    "    print(\"\")\n",
    "    print(\"================================================\")\n",
    "    print(\"Please, click submit button to send your answers\")\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(on_submit)\n",
    "\n",
    "    display(submit_button)\n",
    "\n",
    "    if submit_button:\n",
    "      return voice_input, written_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87f94dee3d8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice, text_input = display_input_form_with_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ce6ff531f8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = f\"Answer: {voice.get('text', '') if voice else ''}\\n\\n{text_input.get('text', '') if text_input else ''}\"\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1956a344b297ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5c6dc75a09db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242afbd93da45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7780cecfc20472",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8c4f051218be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"type\": \"user\", \"content\": answer}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1784f90bf83fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"type\": \"user\", \"content\": answer}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fb910",
   "metadata": {},
   "source": [
    "# 3. Agents (DMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedfdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0365ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")  # need a FOSS alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a844d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going \"bind\" all tools to the model\n",
    "# We have the ACTUAL tools from above, but we also need a mock tool to ask a human\n",
    "# Since `bind_tools` takes in tools but also just tool definitions,\n",
    "# We can define a tool definition for `ask_human`\n",
    "class AskHuman(BaseModel):\n",
    "    \"\"\"Ask the human a question\"\"\"\n",
    "\n",
    "    question: str\n",
    "  \n",
    "llm = llm.bind_tools(tools + [AskHuman])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda97721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # If tool call is asking Human, we return that node\n",
    "    # You could also add logic here to let some system know that there's something that requires Human input\n",
    "    # For example, send a slack message, etc\n",
    "    elif last_message.tool_calls[0][\"name\"] == \"AskHuman\":\n",
    "        return \"ask_human\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad90dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that calls the llm\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10819a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a fake node to ask the human\n",
    "def ask_human(state):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e59fe",
   "metadata": {},
   "source": [
    "Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda63a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the three nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "workflow.add_node(\"ask_human\", ask_human)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed74746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # We may ask the human\n",
    "        \"ask_human\": \"ask_human\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc918237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# After we get back the human response, we go back to the agent\n",
    "workflow.add_edge(\"ask_human\", \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a47d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f70d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "# We add a breakpoint BEFORE the `ask_human` node so it never executes\n",
    "app = workflow.compile(checkpointer=memory, interrupt_before=[\"ask_human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c324e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(\n",
    "    content=\"Ask the user where they are\"\n",
    ")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_id = app.get_state(config).values[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "tool_message = [ToolMessage(tool_call_id=tool_call_id, content=\"san francisco\")]\n",
    "\n",
    "# We now update the state\n",
    "# Notice that we are also specifying `as_node=\"ask_human\"`\n",
    "# This will apply this update as this node,\n",
    "# which will make it so that afterwards it continues as normal\n",
    "app.update_state(config, {\"messages\": tool_message}, as_node=\"ask_human\")\n",
    "\n",
    "# We can check the state\n",
    "# We can see that the state currently has the `agent` node next\n",
    "# This is based on how we define our graph,\n",
    "# where after the `ask_human` node goes (which we just triggered)\n",
    "# there is an edge to the `agent` node\n",
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in app.stream(None, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the current state of the application\n",
    "state = app.get_state(config).values\n",
    "\n",
    "# Access the list of messages from the state\n",
    "messages = state[\"messages\"]\n",
    "\n",
    "# Iterate through each message and print its content\n",
    "for message in messages:\n",
    "    print(f\"{message.type.capitalize()} Message: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30a1b0",
   "metadata": {},
   "source": [
    "## 3.1. Define Agents & Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3468408",
   "metadata": {},
   "source": [
    "### 3.1.1. Define Base Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, role: str, skills: List[str], llm):\n",
    "        \"\"\"\n",
    "        Base class for an agent.\n",
    "\n",
    "        Args:\n",
    "            name (str): The name of the agent.\n",
    "            role (str): The role of the agent (e.g., HR, Manager, etc.).\n",
    "            skills (List[str]): The list of skills the agent possesses.\n",
    "            llm: A language model instance for generating responses.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.skills = skills\n",
    "        self.llm = llm\n",
    "\n",
    "    def process_task(self, task: str, context: Optional[List[Dict[str, str]]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Process a task assigned to the agent.\n",
    "\n",
    "        Args:\n",
    "            task (str): The task or question to process.\n",
    "            context (Optional[List[Dict[str, str]]]): Additional context for the task.\n",
    "\n",
    "        Returns:\n",
    "            str: The agent's response.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"You are {self.name}, a {self.role}. Your skills include: {', '.join(self.skills)}. Respond to the task based on your role and skills.\")\n",
    "        ]\n",
    "        \n",
    "        # Include provided context\n",
    "        if context:\n",
    "            for msg in context:\n",
    "                if msg['role'] == 'human':\n",
    "                    messages.append(HumanMessage(content=msg['content']))\n",
    "                elif msg['role'] == 'ai':\n",
    "                    messages.append(AIMessage(content=msg['content']))\n",
    "        \n",
    "        # Add the task as the final input\n",
    "        messages.append(HumanMessage(content=task))\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a2414",
   "metadata": {},
   "source": [
    "### 3.1.1. Define Orchestrator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorAgent(Agent):\n",
    "    def __init__(self, name: str, role: str, skills: List[str], llm):\n",
    "        super().__init__(name, role, skills, llm)\n",
    "\n",
    "    def generate_scenario(self, job_description: str, company_info: str, candidate_cv: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a scenario for the interview based on input data.\n",
    "\n",
    "        Args:\n",
    "            job_description (str): The job description.\n",
    "            company_info (str): Information about the company.\n",
    "            candidate_cv (str): The candidate's CV.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A structured interview scenario.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Analyze the job description, company information, and candidate CV to generate an interview scenario. \"\n",
    "            \"Include priorities, suggested questions, and areas to focus on.\"\n",
    "        )\n",
    "        context = [\n",
    "            {\"role\": \"human\", \"content\": f\"Job Description: {job_description}\"},\n",
    "            {\"role\": \"human\", \"content\": f\"Company Info: {company_info}\"},\n",
    "            {\"role\": \"human\", \"content\": f\"Candidate CV: {candidate_cv}\"}\n",
    "        ]\n",
    "        response = self.process_task(task, context)\n",
    "        return response  # This should return a structured JSON or text output\n",
    "\n",
    "    def assign_tasks(self, graph, scenario: Dict):\n",
    "        \"\"\"\n",
    "        Assign tasks to other agents based on the generated scenario.\n",
    "\n",
    "        Args:\n",
    "            graph: LangGraph graph instance.\n",
    "            scenario (Dict): The structured interview scenario.\n",
    "        \"\"\"\n",
    "        # Extract priorities from the scenario\n",
    "        for agent_task in scenario.get(\"agents\", []):\n",
    "            agent_name = agent_task[\"role\"]\n",
    "            task = agent_task[\"tasks\"]\n",
    "            \n",
    "            # Dynamically find the corresponding agent node\n",
    "            agent_node = graph.get_node(agent_name)\n",
    "            if agent_node:\n",
    "                graph.add_edge(self, agent_node, task=task)\n",
    "\n",
    "    def process_feedback(self, feedback: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Process feedback from agents to synthesize a final report.\n",
    "\n",
    "        Args:\n",
    "            feedback (Dict): Feedback data from agents.\n",
    "\n",
    "        Returns:\n",
    "            str: A synthesized report on the candidate.\n",
    "        \"\"\"\n",
    "        task = \"Synthesize the following feedback into a unified candidate assessment report.\"\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Feedback: {feedback}\"}]\n",
    "        return self.process_task(task, context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d408a5",
   "metadata": {},
   "source": [
    "### 3.1.2. Define HR Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanResourcesAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"\n",
    "        Initialize the Human Resources Agent with specific skills.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=\"Jessica Taylor\",\n",
    "            role=\"Human Resources Specialist\",\n",
    "            skills=[\n",
    "                \"Sourcing Candidates\",\n",
    "                \"Screening Resumes\",\n",
    "                \"Interviewing Techniques\",\n",
    "                \"Candidate Assessment\",\n",
    "                \"Offer Negotiation\",\n",
    "                \"Reference Checking\",\n",
    "                \"Talent Pool Development\",\n",
    "                \"Job Description Writing\",\n",
    "                \"Onboarding Coordination\",\n",
    "                \"Effective Communication\",\n",
    "                \"Active Listening\",\n",
    "                \"Empathy and Rapport Building\",\n",
    "                \"Persuasion and Influence\",\n",
    "            ],\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "    def generate_behavioral_questions(self, job_description: str, company_values: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate a list of behavioral questions based on the job description and company values.\n",
    "\n",
    "        Args:\n",
    "            job_description (str): The job description.\n",
    "            company_values (str): The company's core values.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of behavioral interview questions.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Generate behavioral interview questions tailored to the following job description \"\n",
    "            \"and company values. Focus on assessing adaptability, teamwork, and problem-solving skills.\"\n",
    "        )\n",
    "        context = [\n",
    "            {\"role\": \"human\", \"content\": f\"Job Description: {job_description}\"},\n",
    "            {\"role\": \"human\", \"content\": f\"Company Values: {company_values}\"}\n",
    "        ]\n",
    "        response = self.process_task(task, context)\n",
    "        return response.split(\"\\n\")  # Assuming questions are returned as a newline-separated string\n",
    "\n",
    "    def ensure_compliance(self, questions: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Check the list of questions for compliance with non-discrimination policies.\n",
    "\n",
    "        Args:\n",
    "            questions (List[str]): The list of interview questions.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A filtered list of compliant questions.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Review the following list of interview questions for compliance with \"\n",
    "            \"non-discrimination policies. Remove or revise any questions that could be biased.\"\n",
    "        )\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Questions: {', '.join(questions)}\"}]\n",
    "        response = self.process_task(task, context)\n",
    "        return response.split(\"\\n\")\n",
    "\n",
    "    def generate_interview_agenda(self, role_requirements: str, company_info: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a structured interview agenda based on the role requirements and company information.\n",
    "\n",
    "        Args:\n",
    "            role_requirements (str): The role requirements.\n",
    "            company_info (str): Information about the company.\n",
    "\n",
    "        Returns:\n",
    "            str: A detailed interview agenda.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Create a structured interview agenda for the role described below. Include time allocations \"\n",
    "            \"for introductions, behavioral questions, technical questions, and a Q&A session.\"\n",
    "        )\n",
    "        context = [\n",
    "            {\"role\": \"human\", \"content\": f\"Role Requirements: {role_requirements}\"},\n",
    "            {\"role\": \"human\", \"content\": f\"Company Information: {company_info}\"}\n",
    "        ]\n",
    "        response = self.process_task(task, context)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e9519",
   "metadata": {},
   "source": [
    "### 3.1.3. Define Manager Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManagerAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"\n",
    "        Initialize the Hiring Manager Agent with specific skills.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=\"Michael Brown\",\n",
    "            role=\"Hiring Manager\",\n",
    "            skills=[\n",
    "                # Core Hiring and Candidate Assessment Skills\n",
    "                \"Interviewing Techniques\",\n",
    "                \"Candidate Evaluation\",\n",
    "                \"Behavioral Assessment\",\n",
    "                \"Decision-Making\",\n",
    "                \"Reference Checking\",\n",
    "                # Soft Skills\n",
    "                \"Effective Communication\",\n",
    "                \"Active Listening\",\n",
    "                \"Empathy and Rapport Building\",\n",
    "                \"Time Management\",\n",
    "                # Technical Skills\n",
    "                \"Proficiency in Applicant Tracking Systems (ATS)\",\n",
    "                \"Data-Driven Hiring Decisions\"\n",
    "            ],\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "    def assess_cultural_fit(self, candidate_answers: str, company_values: str) -> str:\n",
    "        \"\"\"\n",
    "        Assess the candidate's cultural fit based on their answers and the company's values.\n",
    "\n",
    "        Args:\n",
    "            candidate_answers (str): Candidate's responses to cultural fit questions.\n",
    "            company_values (str): The company's core values.\n",
    "\n",
    "        Returns:\n",
    "            str: An evaluation of the candidate's cultural fit.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Based on the following candidate responses, evaluate how well they align with \"\n",
    "            \"the company's core values.\"\n",
    "        )\n",
    "        context = [\n",
    "            {\"role\": \"human\", \"content\": f\"Candidate Answers: {candidate_answers}\"},\n",
    "            {\"role\": \"human\", \"content\": f\"Company Values: {company_values}\"}\n",
    "        ]\n",
    "        response = self.process_task(task, context)\n",
    "        return response\n",
    "\n",
    "    def evaluate_leadership_potential(self, scenario_response: str) -> str:\n",
    "        \"\"\"\n",
    "        Evaluate the candidate's leadership potential based on their response to a scenario.\n",
    "\n",
    "        Args:\n",
    "            scenario_response (str): The candidate's response to a leadership scenario.\n",
    "\n",
    "        Returns:\n",
    "            str: An assessment of the candidate's leadership potential.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Evaluate the candidate's leadership potential based on their response to the following scenario. \"\n",
    "            \"Focus on their decision-making, problem-solving, and ability to inspire others.\"\n",
    "        )\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Scenario Response: {scenario_response}\"}]\n",
    "        response = self.process_task(task, context)\n",
    "        return response\n",
    "\n",
    "    def make_data_driven_decision(self, candidate_metrics: Dict[str, float]) -> str:\n",
    "        \"\"\"\n",
    "        Make a hiring recommendation based on candidate metrics.\n",
    "\n",
    "        Args:\n",
    "            candidate_metrics (Dict[str, float]): A dictionary of candidate metrics (e.g., skills, experience, cultural fit).\n",
    "\n",
    "        Returns:\n",
    "            str: A hiring recommendation.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Based on the following candidate metrics, make a data-driven recommendation on whether to move forward with this candidate.\"\n",
    "        )\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Candidate Metrics: {candidate_metrics}\"}]\n",
    "        response = self.process_task(task, context)\n",
    "        return response\n",
    "\n",
    "    def summarize_interview_feedback(self, feedback: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Summarize feedback from multiple interviewers into a cohesive report.\n",
    "\n",
    "        Args:\n",
    "            feedback (List[Dict[str, str]]): A list of feedback dictionaries from other interviewers.\n",
    "\n",
    "        Returns:\n",
    "            str: A cohesive summary of the feedback.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Summarize the feedback from the following interviewers into a cohesive report. \"\n",
    "            \"Highlight the candidate's strengths, weaknesses, and overall fit for the role.\"\n",
    "        )\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Feedback: {feedback}\"}]\n",
    "        response = self.process_task(task, context)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c496116",
   "metadata": {},
   "source": [
    "### 3.1.4. Define Field Specialist Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9622e6bf",
   "metadata": {},
   "source": [
    "Would be cool to have the specific role and skills be generated depending on the field wrt the interview, maybe from another agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FieldSpecialistAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"\n",
    "        Initialize the Field Specialist Agent with specific skills.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=\"Emily Johnson\",\n",
    "            role=\"Field Specialist\",\n",
    "            skills=[\n",
    "                # Core Technical and Functional Skills\n",
    "                \"Technical Expertise in Field Operations\",\n",
    "                \"Problem-Solving in Real-Time Scenarios\",\n",
    "                \"Data Collection and Reporting\",\n",
    "                \"Equipment Handling and Maintenance\",\n",
    "                \"Compliance with Safety Standards\",\n",
    "                # Collaboration and Teamwork\n",
    "                \"Team Coordination\",\n",
    "                \"Effective Communication\",\n",
    "                \"Adaptability\",\n",
    "                \"Conflict Resolution\",\n",
    "                # Technical Skills\n",
    "                \"Proficiency in Field-Specific Software\",\n",
    "                \"Report Writing and Documentation\"\n",
    "            ],\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "    def generate_technical_questions(self, role_specific_requirements: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate technical questions based on role-specific requirements.\n",
    "\n",
    "        Args:\n",
    "            role_specific_requirements (str): The technical skills and responsibilities of the role.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of technical interview questions.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Create a list of technical questions tailored to the following role-specific requirements. \"\n",
    "            \"Focus on assessing the candidate's expertise in practical, field-specific scenarios.\"\n",
    "        )\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Role Requirements: {role_specific_requirements}\"}]\n",
    "        response = self.process_task(task, context)\n",
    "        return response.split(\"\\n\")  # Assuming questions are returned as a newline-separated string\n",
    "\n",
    "    def simulate_real_world_scenario(self, scenario_description: str) -> str:\n",
    "        \"\"\"\n",
    "        Simulate a real-world scenario and provide the candidate with a task to solve.\n",
    "\n",
    "        Args:\n",
    "            scenario_description (str): A description of the real-world scenario.\n",
    "\n",
    "        Returns:\n",
    "            str: A description of the simulated task for the candidate.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Design a real-world scenario based on the description below. Provide a detailed task for the candidate to solve, \"\n",
    "            \"focusing on their problem-solving skills and adaptability.\"\n",
    "        )\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Scenario Description: {scenario_description}\"}]\n",
    "        response = self.process_task(task, context)\n",
    "        return response\n",
    "\n",
    "    def evaluate_adaptability(self, candidate_response: str) -> str:\n",
    "        \"\"\"\n",
    "        Evaluate the candidate's adaptability based on their response to a scenario.\n",
    "\n",
    "        Args:\n",
    "            candidate_response (str): The candidate's response to a field-specific scenario.\n",
    "\n",
    "        Returns:\n",
    "            str: An evaluation of the candidate's adaptability.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Evaluate the candidate's adaptability based on their response to the following field-specific scenario. \"\n",
    "            \"Focus on their ability to adjust to unexpected changes and find effective solutions.\"\n",
    "        )\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Candidate Response: {candidate_response}\"}]\n",
    "        response = self.process_task(task, context)\n",
    "        return response\n",
    "\n",
    "    def review_certifications(self, certifications: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Review and validate the candidate's certifications for the role.\n",
    "\n",
    "        Args:\n",
    "            certifications (List[str]): A list of the candidate's certifications.\n",
    "\n",
    "        Returns:\n",
    "            str: An assessment of the relevance and validity of the certifications.\n",
    "        \"\"\"\n",
    "        task = (\n",
    "            \"Review the following certifications to determine their relevance and validity for the role. \"\n",
    "            \"Provide feedback on their applicability to the field.\"\n",
    "        )\n",
    "        context = [{\"role\": \"human\", \"content\": f\"Certifications: {', '.join(certifications)}\"}]\n",
    "        response = self.process_task(task, context)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9b59e",
   "metadata": {},
   "source": [
    "## Example workflow:\n",
    "\n",
    "### Scenario Overview\n",
    "\n",
    "#### Job Description:\n",
    "\"Senior Field Operations Engineer responsible for managing industrial equipment, leading on-site teams, and troubleshooting hydraulic and mechanical systems under tight deadlines.\"\n",
    "\n",
    "#### Company Values:\n",
    "\"Innovation, Teamwork, Adaptability.\"\n",
    "\n",
    "#### Candidate's Background:\n",
    "* Experience: 8 years in field operations.\n",
    "* Skills: Troubleshooting mechanical systems, team leadership, safety compliance.\n",
    "* Certifications: Certified Field Technician, OSHA Safety Certification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2680aa30",
   "metadata": {},
   "source": [
    "### Initiate agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16157af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator = OrchestratorAgent(\n",
    "    name=\"Orchestrator\",\n",
    "    role=\"Coordinator\",\n",
    "    skills=[\"Scenario Planning\", \"Task Delegation\", \"Feedback Synthesis\"],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "hr_agent = HumanResourcesAgent(llm)\n",
    "manager_agent = ManagerAgent(llm)\n",
    "field_specialist_agent = FieldSpecialistAgent(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3377fd",
   "metadata": {},
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e034a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow graph\n",
    "workflow = StateGraph(MessagesState)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes (Tasks)\n",
    "def collect_candidate_details(state):\n",
    "    job_description = state.get(\"job_description\", \"Senior Software Engineer for backend development.\")\n",
    "    company_info = state.get(\"company_info\", \"TechCorp values innovation, collaboration, and customer focus.\")\n",
    "    candidate_cv = state.get(\"candidate_cv\", \"Experienced backend engineer with Python and cloud expertise.\")\n",
    "    return {\"job_description\": job_description, \"company_info\": company_info, \"candidate_cv\": candidate_cv}\n",
    "\n",
    "def orchestrator_generate_scenario(state):\n",
    "    scenario = orchestrator.generate_scenario(\n",
    "        state[\"job_description\"], state[\"company_info\"], state[\"candidate_cv\"]\n",
    "    )\n",
    "    return {\"scenario\": scenario}\n",
    "\n",
    "def hr_generate_questions(state):\n",
    "    behavioral_questions = hr_agent.generate_behavioral_questions(\n",
    "        state[\"job_description\"], state[\"company_info\"]\n",
    "    )\n",
    "    return {\"behavioral_questions\": behavioral_questions}\n",
    "\n",
    "def manager_evaluate_candidate(state):\n",
    "    cultural_fit = manager_agent.assess_cultural_fit(\n",
    "        candidate_answers=\"I believe in open communication and teamwork.\",\n",
    "        company_values=state[\"company_info\"],\n",
    "    )\n",
    "    leadership_potential = manager_agent.evaluate_leadership_potential(\n",
    "        scenario_response=\"I prioritize clear delegation and support team members during challenges.\"\n",
    "    )\n",
    "    return {\"cultural_fit\": cultural_fit, \"leadership_potential\": leadership_potential}\n",
    "\n",
    "def field_specialist_tasks(state):\n",
    "    technical_questions = field_specialist_agent.generate_technical_questions(\n",
    "        role_specific_requirements=state[\"job_description\"]\n",
    "    )\n",
    "    adaptability_evaluation = field_specialist_agent.evaluate_adaptability(\n",
    "        candidate_response=\"I inspected system pressure levels and found obstructions in valves.\"\n",
    "    )\n",
    "    return {\n",
    "        \"technical_questions\": technical_questions,\n",
    "        \"adaptability_evaluation\": adaptability_evaluation,\n",
    "    }\n",
    "\n",
    "def orchestrator_synthesize_feedback(state):\n",
    "    feedback = {\n",
    "        \"HR Feedback\": state.get(\"behavioral_questions\"),\n",
    "        \"Manager Feedback\": {\n",
    "            \"Cultural Fit\": state.get(\"cultural_fit\"),\n",
    "            \"Leadership Potential\": state.get(\"leadership_potential\"),\n",
    "        },\n",
    "        \"Field Specialist Feedback\": {\n",
    "            \"Technical Questions\": state.get(\"technical_questions\"),\n",
    "            \"Adaptability Evaluation\": state.get(\"adaptability_evaluation\"),\n",
    "        },\n",
    "    }\n",
    "    final_report = orchestrator.process_feedback(feedback)\n",
    "    return {\"final_report\": final_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49116e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "workflow.add_node(\"collect_candidate_details\", collect_candidate_details)\n",
    "workflow.add_node(\"orchestrator_generate_scenario\", orchestrator_generate_scenario)\n",
    "workflow.add_node(\"hr_generate_questions\", hr_generate_questions)\n",
    "workflow.add_node(\"manager_evaluate_candidate\", manager_evaluate_candidate)\n",
    "workflow.add_node(\"field_specialist_tasks\", field_specialist_tasks)\n",
    "workflow.add_node(\"orchestrator_synthesize_feedback\", orchestrator_synthesize_feedback)\n",
    "\n",
    "# Define the workflow edges\n",
    "workflow.set_entry_point(\"collect_candidate_details\")\n",
    "workflow.add_edge(\"collect_candidate_details\", \"orchestrator_generate_scenario\")\n",
    "workflow.add_edge(\"orchestrator_generate_scenario\", \"hr_generate_questions\")\n",
    "workflow.add_edge(\"orchestrator_generate_scenario\", \"manager_evaluate_candidate\")\n",
    "workflow.add_edge(\"orchestrator_generate_scenario\", \"field_specialist_tasks\")\n",
    "workflow.add_edge(\"hr_generate_questions\", \"orchestrator_synthesize_feedback\")\n",
    "workflow.add_edge(\"manager_evaluate_candidate\", \"orchestrator_synthesize_feedback\")\n",
    "workflow.add_edge(\"field_specialist_tasks\", \"orchestrator_synthesize_feedback\")\n",
    "workflow.add_edge(\"orchestrator_synthesize_feedback\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6350435",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example run\n",
    "state = app.run({\n",
    "    \"job_description\": \"Senior Software Engineer for backend development.\",\n",
    "    \"company_info\": \"TechCorp values innovation, collaboration, and customer focus.\",\n",
    "    \"candidate_cv\": \"Experienced backend engineer with Python and cloud expertise.\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effcbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state[\"final_report\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ebb8e",
   "metadata": {},
   "source": [
    "# Agents 2 (DMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c35e7",
   "metadata": {},
   "source": [
    "Define Names of the Agents\n",
    "\n",
    "These are just strings but as we’ll have to type each of these strings multiple times it will be very annoying if we change or mistype one, hence storing these in a single place up top is the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_AGENT_NAME = \"hr_agent\"\n",
    "MANAGER_NAME = \"manager_agent\"\n",
    "SPECIALIST_NAME = \"specialist_agent\"\n",
    "FEEDBACK_NAME = \"feedback_agent\"\n",
    "ORCHESTRATOR_NAME = \"orchestrator\"\n",
    "MEMBERS = [HR_AGENT_NAME, MANAGER_NAME, SPECIALIST_NAME, FEEDBACK_NAME]\n",
    "OPTIONS = [\"FINISH\"] + MEMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aca100",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")  # need a FOSS alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03beca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(llm: BaseChatModel, tools: list, system_prompt: str):\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt_template)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools)  # type: ignore\n",
    "    return agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1af963",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": OPTIONS},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dcf26",
   "metadata": {},
   "source": [
    "Define Orchestrator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: \n",
    "# update placeholder for our project\n",
    "\n",
    "ORCHESTRATOR_SYSTEM_PROMPT = \"\"\"\n",
    "You are a supervisor tasked with managing a conversation between the following workers: {members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. The end goal is to provide a good travel itinerary for the user, with things to see and do, practical tips on how to deal with language difficulties, and a nice visualization that goes with the travel plan (in the form of an image path, the visualizer will save the image for you and you only need the path).\n",
    "\n",
    "Make sure you call on each team member ({members}) at least once. Do not call the visualizer again if you've already received an image file path. Do not call any team member a second time unless they didn't provide enough details or a valid response and you need them to redo their work. When finished, respond with FINISH, but before you do, make sure you have a travel itinerary, language tips for the location, and an image file-path. If you don't have all of these, call the appropriate team member to get the missing information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f109a0",
   "metadata": {},
   "source": [
    "This time we have three messages. The first is the ORCHESTRATOR_SYSTEM_PROMPT we defined above. The second is a MessagesPlaceholder for the messages variable (conversation context so far) and the third is a short system message that reminds the team supervisor what it’s task is and what options it has available to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe835dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ORCHESTRATOR_SYSTEM_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=\", \".join(OPTIONS), members=\", \".join(MEMBERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2353e66",
   "metadata": {},
   "source": [
    "So the orchastrator is basically going to act like a router between our agents, deciding who is up next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_chain = (\n",
    "    orchestrator_prompt_template\n",
    "    | llm.bind_tools(tools=[router_function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cfd44c",
   "metadata": {},
   "source": [
    "Define rest of the agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a0074",
   "metadata": {},
   "source": [
    "System prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0dafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: \n",
    "# update placeholder for our project\n",
    "\n",
    "HR_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can suggest and review travel itinerary plans, providing critical feedback on how the trip can be enriched for enjoyment of the local culture. If the plan already includes local experiences, you can mention that the plan is satisfactory, with rationale.\n",
    "\n",
    "Assume a general interest in popular tourist destinations and local culture, do not ask the user any follow-up questions.\n",
    "\n",
    "You have access to a web search function for additional or up-to-date research if needed. You are not required to use this if you already have sufficient information to answer the question.\n",
    "\"\"\"\n",
    "\n",
    "MANAGER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale.\n",
    "\n",
    "You have access to a web search function for additional or up-to-date research if needed. You are not required to use this if you already have sufficient information to answer the question.\n",
    "\"\"\"\n",
    "\n",
    "SPECIALIST_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can generate images based on a detailed description. You are part of a travel agent team and your job is to look at the location and travel itinerary and then generate an appropriate image to go with the travel plan. You have access to a function that will generate the image as long as you provide a good description including the location and visual characteristics of the image you want to generate. This function will download the image and return the path of the image file to you.\n",
    "\n",
    "Make sure you provide the image, and then communicate back as your response only the path to the image file you generated. You do not need to give any other textual feedback, just the path to the image file.\n",
    "\"\"\"\n",
    "\n",
    "FEEDBACK_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can generate images based on a detailed description. You are part of a travel agent team and your job is to look at the location and travel itinerary and then generate an appropriate image to go with the travel plan. You have access to a function that will generate the image as long as you provide a good description including the location and visual characteristics of the image you want to generate. This function will download the image and return the path of the image file to you.\n",
    "\n",
    "Make sure you provide the image, and then communicate back as your response only the path to the image file you generated. You do not need to give any other textual feedback, just the path to the image file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c498ad",
   "metadata": {},
   "source": [
    "Define agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: \n",
    "# Add list of tools each agent can use\n",
    "\n",
    "hr_agent = create_agent(llm=llm, tools=[], system_prompt=HR_SYSTEM_PROMPT)\n",
    "hr_agent_node = functools.partial(\n",
    "    agent_node, agent=hr_agent, name=HR_AGENT_NAME\n",
    ")\n",
    "\n",
    "manager_agent = create_agent(llm=llm, tools=[], system_prompt=MANAGER_SYSTEM_PROMPT)\n",
    "manager_agent_node = functools.partial(\n",
    "    agent_node, agent=manager_agent, name=MANAGER_NAME\n",
    ")\n",
    "\n",
    "specialist_agent = create_agent(llm=llm, tools=[], system_prompt=SPECIALIST_SYSTEM_PROMPT)\n",
    "specialist_agent_node = functools.partial(\n",
    "    agent_node, agent=specialist_agent, name=SPECIALIST_NAME\n",
    ")\n",
    "\n",
    "feedback_agent = create_agent(llm=llm, tools=[], system_prompt=FEEDBACK_SYSTEM_PROMPT)\n",
    "feedback_agent_node = functools.partial(\n",
    "    agent_node, agent=feedback_agent, name=FEEDBACK_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59cfaaf",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(HR_AGENT_NAME, hr_agent_node)\n",
    "workflow.add_node(MANAGER_NAME, manager_agent_node)\n",
    "workflow.add_node(SPECIALIST_NAME, specialist_agent_node)\n",
    "workflow.add_node(FEEDBACK_NAME, feedback_agent_node)\n",
    "workflow.add_node(ORCHESTRATOR_NAME, orchestrator_chain)\n",
    "\n",
    "for member in MEMBERS:\n",
    "    workflow.add_edge(member, ORCHESTRATOR_NAME)\n",
    "\n",
    "workflow.add_edge(FEEDBACK_NAME, END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_map = {name: name for name in MEMBERS}\n",
    "conditional_map[\"FINISH\"] = FEEDBACK_NAME\n",
    "workflow.add_conditional_edges(\n",
    "    ORCHESTRATOR_NAME, lambda x: x[\"next\"], conditional_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(ORCHESTRATOR_NAME)\n",
    "\n",
    "travel_agent_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a65137",
   "metadata": {},
   "source": [
    "Visualize graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610eaa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(travel_agent_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ecc3bc",
   "metadata": {},
   "source": [
    "# Agents 3 (DMA) - I'm getting there OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293add6",
   "metadata": {},
   "source": [
    "## Workflow Structure\n",
    "\n",
    "- **Orchestration Stage:**\n",
    "  - **Information Gathering:**\n",
    "    - Obtain user documents (CV, Job description) using the `FileUpload` tool.\n",
    "    - Retrieve company information via the `WebScraper` tool.\n",
    "    - Create a \"Scene\" to outline the interview structure.\n",
    "\n",
    "- **Interview Stages:**\n",
    "  - **HR Stage:** Introduce the user, ask general and behavioral questions, and evaluate responses.\n",
    "  - **Manager Stage:** Introduce the team, ask role-specific questions, and evaluate responses.\n",
    "  - **Technical/Field Expert Stage:** Pose field-specific questions, assess technical knowledge, and evaluate responses.\n",
    "\n",
    "- **Feedback Stage:** Compile evaluations from all stages and generate final feedback for the user.\n",
    "\n",
    "---\n",
    "\n",
    "## Graphs and Subgraphs\n",
    "\n",
    "### Subgraphs\n",
    "Each stage is implemented as a **subgraph**, encapsulating its sub-stages (nodes) and logic. Each task is a node.\n",
    "\n",
    "### Parent Graph\n",
    "A **parent graph** orchestrates the overall workflow by chaining the subgraphs. It also manages data flow between stages through transformers or shared state objects.\n",
    "\n",
    "Example of a parent graph:\n",
    "```python\n",
    "parent_graph = StateGraph(BaseModel)\n",
    "\n",
    "# Add Orchestration subgraph\n",
    "parent_graph.add_subgraph('orchestration', orchestration_graph, OrchestrationState)\n",
    "\n",
    "# Add HR stage subgraph\n",
    "parent_graph.add_subgraph('hr', hr_graph, HRState)\n",
    "\n",
    "# Define the flow between subgraphs\n",
    "def transfer_to_hr(orchestration_state: OrchestrationState) -> HRState:\n",
    "    return HRState(\n",
    "        cv=orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "        job_description=orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "        company_info=orchestration_state.company_info,\n",
    "        scene=orchestration_state.scene\n",
    "    )\n",
    "\n",
    "parent_graph.add_edge('orchestration', 'hr', transformer=transfer_to_hr)\n",
    "parent_graph.add_edge('hr', END)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## `AgentState` and Data Flow\n",
    "\n",
    "**Utilizing Multiple `AgentState` Classes:**\n",
    "\n",
    "Each stage can have its own `AgentState` class to encapsulate relevant data, enhancing modularity and clarity. For example:\n",
    "\n",
    "- **Orchestration State:**\n",
    "  ```python\n",
    "  from pydantic import BaseModel\n",
    "\n",
    "  class OrchestrationState(BaseModel):\n",
    "      user_documents: dict = {}\n",
    "      company_info: str = \"\"\n",
    "      scene: str = \"\"\n",
    "  ```\n",
    "\n",
    "- **Interview State:**\n",
    "  ```python\n",
    "  from pydantic import BaseModel\n",
    "\n",
    "  class InterviewState(BaseModel):\n",
    "      scene: str = \"\"\n",
    "      user_documents: dict = {}\n",
    "      company_info: str = \"\"\n",
    "      questions: list = []\n",
    "      user_responses: dict = {}\n",
    "      evaluation: str = \"\"\n",
    "  ```\n",
    "\n",
    "### How Data Flows Between Stages\n",
    "- Use **transformers** to extract data from one state and initialize the next.\n",
    "- Example: Passing data from `OrchestrationState` to `InterviewState`.\n",
    "\n",
    "```python\n",
    "# define orchestration_graph\n",
    "# final output result_orchestration_state \n",
    "\n",
    "# Extract data from OrchestrationState\n",
    "hr_initial_state = InterviewState(\n",
    "    cv=result_orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "    job_description=result_orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "    company_info=result_orchestration_state.company_info,\n",
    "    scene=result_orchestration_state.scene\n",
    ")\n",
    "\n",
    "# define hr_graph\n",
    "# final output result_hr_state \n",
    "\n",
    "# Execute HR graph with pre-initialized starting state\n",
    "result_hr_state = hr_app.invoke(hr_initial_state)\n",
    "\n",
    "# ----- #\n",
    "\n",
    "# Combining Orchestration and HR 'apps'\n",
    "# Define a parent graph\n",
    "parent_graph = StateGraph(BaseModel)\n",
    "\n",
    "# Add Orchestration as a subgraph\n",
    "parent_graph.add_subgraph('orchestration', orchestration_graph, OrchestrationState)\n",
    "\n",
    "# Add HR stage as a subgraph\n",
    "parent_graph.add_subgraph('hr', hr_graph, InterviewState)\n",
    "\n",
    "# Define data transfer and flow\n",
    "def transfer_to_hr(orchestration_state: OrchestrationState) -> HRState:\n",
    "    return HRState(\n",
    "        cv=orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "        job_description=orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "        company_info=orchestration_state.company_info,\n",
    "        scene=orchestration_state.scene\n",
    "    )\n",
    "\n",
    "parent_graph.add_edge('orchestration', 'hr', transformer=transfer_to_hr)\n",
    "parent_graph.add_edge('hr', END)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260be015",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb2438",
   "metadata": {},
   "source": [
    "Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OAI only for now, need to add HF\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911709f",
   "metadata": {},
   "source": [
    "## 2.1. Define Orchestration State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestrationState(BaseModel):\n",
    "    user_documents: dict = Field(default_factory=dict)\n",
    "    company_info: str = \"\"\n",
    "    scene: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cb520",
   "metadata": {},
   "source": [
    "Define the tools for the Orchestration stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03248af4",
   "metadata": {},
   "source": [
    "Tools (placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def FileUpload(description: str) -> dict:\n",
    "    \"\"\"\n",
    "    Simulate a file upload process.\n",
    "\n",
    "    Args:\n",
    "        description (str): A brief description of the file upload context.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing simulated content for 'CV' and 'Job Description'.\n",
    "    \"\"\"\n",
    "    return {\"CV\": \"Candidate's CV content...\", \"Job Description\": \"Job description content...\"}\n",
    "\n",
    "@tool\n",
    "def WebScraper(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulate web scraping to extract company information.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "\n",
    "    Returns:\n",
    "        str: Simulated company information extracted from the website.\n",
    "    \"\"\"\n",
    "    return \"Company information scraped from website...\"\n",
    "\n",
    "@tool\n",
    "def UserInput(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the user for input.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The message displayed to the user.\n",
    "\n",
    "    Returns:\n",
    "        str: The user's input as a string.\n",
    "    \"\"\"\n",
    "    return input(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71fb40",
   "metadata": {},
   "source": [
    "Define Nodes for the three sub-stages of the Orchestration stage:\n",
    "- Document upload\n",
    "- Scrape information about company\n",
    "- Create a \"Scene\" to outline the interview structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the Scene\n",
    "# placeholder\n",
    "\n",
    "GENERATE_SCENE_SYSTEM_PROMPT = \"\"\"\n",
    "    Based on the following:\n",
    "    - CV: {cv}\n",
    "    - Job Description: {job_description}\n",
    "    - Company Info: {company_info}\n",
    "\n",
    "    Generate a \"scene\" for conducting an interview.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Upload Node\n",
    "def upload_documents(state: OrchestrationState):\n",
    "    state.user_documents = FileUpload(\"Upload user documents (CV, job description).\")\n",
    "    return state\n",
    "\n",
    "# Web Scraper Node\n",
    "def scrape_company_info(state: OrchestrationState):\n",
    "    state.company_info = WebScraper(\"https://company.website\")\n",
    "    return state\n",
    "\n",
    "# Scene Generation Node\n",
    "# todo: update system prompt\n",
    "def generate_scene(state: OrchestrationState):\n",
    "    cv = state.user_documents.get(\"CV\", \"\")\n",
    "    job_description = state.user_documents.get(\"Job Description\", \"\")\n",
    "    company_info = state.company_info\n",
    "\n",
    "    system_prompt = GENERATE_SCENE_SYSTEM_PROMPT\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.scene = response.content.strip()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7d63a",
   "metadata": {},
   "source": [
    "Create the graph for the orchestration stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph for orchestration\n",
    "orchestration_graph = StateGraph(OrchestrationState)\n",
    "\n",
    "# Add the nodes for each of the \"sub-stages\"\n",
    "orchestration_graph.add_node('upload_documents', upload_documents)\n",
    "orchestration_graph.add_node('scrape_company_info', scrape_company_info)\n",
    "orchestration_graph.add_node('generate_scene', generate_scene)\n",
    "\n",
    "# Define the flow of tasks\n",
    "orchestration_graph.add_edge(START, 'upload_documents')\n",
    "orchestration_graph.add_edge('upload_documents', 'scrape_company_info')\n",
    "orchestration_graph.add_edge('scrape_company_info', 'generate_scene')\n",
    "orchestration_graph.add_edge('generate_scene', END)\n",
    "\n",
    "# Compile the orchestration graph\n",
    "orchestration_app = orchestration_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf338c1",
   "metadata": {},
   "source": [
    "Visualize orchestration graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(orchestration_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7997e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_orchestration_state = OrchestrationState()\n",
    "result_orchestration_state = orchestration_app.invoke(initial_orchestration_state)\n",
    "result_orchestration_state = OrchestrationState(**result_orchestration_state)\n",
    "print(type(result_orchestration_state))\n",
    "print(result_orchestration_state.scene)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a92335",
   "metadata": {},
   "source": [
    "## 2.2. Interview Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(BaseModel):\n",
    "    scene: str = \"\"\n",
    "    user_documents: dict = {}\n",
    "    company_info: str = \"\"\n",
    "    questions: list = []\n",
    "    user_responses: dict = {}\n",
    "    evaluation: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7669439",
   "metadata": {},
   "source": [
    "### 2.2.1. Transfer data from previous stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e97ee",
   "metadata": {},
   "source": [
    "To ensure seamless data transfer from the Orchestration graph to the Interview sub-graphs, we need to:\n",
    "1. Extract relevant data from the OrchestrationState (e.g., `User Documents`, `Company Info`, `Scene`).\n",
    "2. Transform it into an appropriate format for the HRState.\n",
    "3. Use a transformer function in the parent graph to bridge the data between the two sub-graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e16ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_interview(orchestration_state: OrchestrationState) -> InterviewState:\n",
    "    return InterviewState(\n",
    "        cv=orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "        job_description=orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "        company_info=orchestration_state.company_info,\n",
    "        scene=orchestration_state.scene\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08bc01",
   "metadata": {},
   "source": [
    "### 2.2.1. Define tools for interview stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def UserInput(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulates user input for demonstration purposes.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The message displayed to the user prompting them for input.\n",
    "\n",
    "    Returns:\n",
    "        str: The user's input as a string.\n",
    "    \n",
    "    Note:\n",
    "        This function uses Python's `input()` function to capture input. In a real-world scenario, \n",
    "        it would be used to obtain actual user input interactively.\n",
    "    \"\"\"\n",
    "    return input(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d8713",
   "metadata": {},
   "source": [
    "There will be three sub-stages within the Interview stage: \n",
    "- The HR interview\n",
    "- The Manager interview\n",
    "- The technical interview\n",
    "\n",
    "These are quite similar, so they share the base State (InterviewState) and the tools. \n",
    "\n",
    "For each of the sub-stages, there are going to be the following tasks, which we will define as nodes for our graph: \n",
    "1. Introduction: Introduces the candidate and sets the stage for the interview.\n",
    "2. Generate Questions Node: Uses the `Scene`, `User Documents`, and `Company Information` to generate a list of interview questions.\n",
    "3. Ask Questions Node: Iterates through the generated questions, asks the user, and records responses.\n",
    "4. Write Evaluation Node: Summarizes the candidate’s responses and writes an evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6eff65",
   "metadata": {},
   "source": [
    "### 2.2.2. HR Interview sub-stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d1d48",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3182273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "def introduction(state: InterviewState):\n",
    "    print(f\"Welcome to the interview! Here's an overview: {state.scene}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating questions\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an HR assistant conducting an interview.\n",
    "    Use the following information to generate between 1 and 3 tailored questions:\n",
    "\n",
    "    Scene: {state.scene}\n",
    "    CV: {state.cv}\n",
    "    Job Description: {state.job_description}\n",
    "    Company Information: {state.company_info}\n",
    "\"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.questions = response.content.strip().split('\\n')[:3]  # list slicing to cut off at max 3 questions\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions, using the UserInput tool\n",
    "\n",
    "def ask_questions(state: InterviewState):\n",
    "    while state.questions:\n",
    "        question = state.questions.pop(0)\n",
    "        response = UserInput(question)\n",
    "        state.user_responses[question] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52181c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final evaluation\n",
    "\n",
    "def write_evaluation(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    Based on the following user responses, write a brief evaluation of the candidate:\n",
    "\n",
    "    Responses: {state.user_responses}\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.evaluation = response.content.strip()\n",
    "    print(\"Evaluation:\", state.evaluation)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c15b14",
   "metadata": {},
   "source": [
    "#### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b44edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HR sub-graph\n",
    "hr_graph = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes\n",
    "hr_graph.add_node('introduction', introduction)\n",
    "hr_graph.add_node('generate_questions', generate_questions)\n",
    "hr_graph.add_node('ask_questions', ask_questions)\n",
    "hr_graph.add_node('write_evaluation', write_evaluation)\n",
    "\n",
    "# Define edges\n",
    "hr_graph.add_edge(START, 'introduction')\n",
    "hr_graph.add_edge('introduction', 'generate_questions')\n",
    "hr_graph.add_edge('generate_questions', 'ask_questions')\n",
    "hr_graph.add_edge('ask_questions', 'write_evaluation')\n",
    "hr_graph.add_edge('write_evaluation', END)\n",
    "\n",
    "# Compile the HR graph\n",
    "hr_app = hr_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(hr_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5340813",
   "metadata": {},
   "source": [
    "### 2.2.3. Manager Interview Sub-Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e037628",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "def introduction(state: InterviewState):\n",
    "    print(f\"Welcome to the interview! Here's an overview: {state.scene}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb467fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating questions\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an HR assistant conducting an interview.\n",
    "    Use the following information to generate between 1 and 3 tailored questions:\n",
    "\n",
    "    Scene: {state.scene}\n",
    "    CV: {state.cv}\n",
    "    Job Description: {state.job_description}\n",
    "    Company Information: {state.company_info}\n",
    "\"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.questions = response.content.strip().split('\\n')[:3]  # list slicing to cut off at max 3 questions\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98596479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions, using the UserInput tool\n",
    "\n",
    "def ask_questions(state: InterviewState):\n",
    "    while state.questions:\n",
    "        question = state.questions.pop(0)\n",
    "        response = UserInput(question)\n",
    "        state.user_responses[question] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72809b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final evaluation\n",
    "\n",
    "def write_evaluation(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    Based on the following user responses, write a brief evaluation of the candidate:\n",
    "\n",
    "    Responses: {state.user_responses}\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.evaluation = response.content.strip()\n",
    "    print(\"Evaluation:\", state.evaluation)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6f72c",
   "metadata": {},
   "source": [
    "#### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manager sub-graph\n",
    "manager_graph = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes\n",
    "manager_graph.add_node('introduction', introduction)\n",
    "manager_graph.add_node('generate_questions', generate_questions)\n",
    "manager_graph.add_node('ask_questions', ask_questions)\n",
    "manager_graph.add_node('write_evaluation', write_evaluation)\n",
    "\n",
    "# Define edges\n",
    "manager_graph.add_edge(START, 'introduction')\n",
    "manager_graph.add_edge('introduction', 'generate_questions')\n",
    "manager_graph.add_edge('generate_questions', 'ask_questions')\n",
    "manager_graph.add_edge('ask_questions', 'write_evaluation')\n",
    "manager_graph.add_edge('write_evaluation', END)\n",
    "\n",
    "# Compile the Manager graph\n",
    "manager_app = manager_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbca73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(manager_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fa4c8cd964f92",
   "metadata": {},
   "source": [
    "-----\n",
    "# Stretch goal: TTS\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08d9683f86af87",
   "metadata": {},
   "source": [
    "Define model and TTS pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210545fca2c5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the TTS model\n",
    "tts_pipeline = pipeline(\"text-to-speech\", model=\"espnet/kan-bayashi_ljspeech_vits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c26c3dd6ea189",
   "metadata": {},
   "source": [
    "Generate and Play Text with TTS in Real-Time\n",
    "\n",
    "Create a loop where the language model generates text in small chunks. Each chunk will be converted to speech and played immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81c891045e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "def generate_and_play_text(prompt, max_chunks=5, chunk_size=50):\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    # Generate text in chunks\n",
    "    for _ in range(max_chunks):\n",
    "        # Generate a chunk of text\n",
    "        output = text_generator(prompt + generated_text, max_new_tokens=chunk_size, do_sample=True)\n",
    "        new_text = output[0][\"generated_text\"][len(prompt + generated_text):]\n",
    "        \n",
    "        # Append the new text to the generated text\n",
    "        generated_text += new_text\n",
    "        print(new_text)  # Print the generated text chunk\n",
    "\n",
    "        # Generate TTS for the current chunk\n",
    "        audio = tts_pipeline(new_text)\n",
    "\n",
    "        # Autoplay the audio chunk in the notebook\n",
    "        ipd.display(ipd.Audio(audio[\"wav\"], autoplay=True))\n",
    "        \n",
    "        # Add a short delay to simulate real-time generation if needed\n",
    "        # time.sleep(1)  # Uncomment if you want to control the timing\n",
    "\n",
    "# Example usage\n",
    "generate_and_play_text(\"Once upon a time,\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_agents",
   "language": "python",
   "name": "genai_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
