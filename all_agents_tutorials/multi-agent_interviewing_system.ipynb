{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8376f449a4cd31",
   "metadata": {},
   "source": [
    "**TITLE:** MULTI-AGENT INTERVIEWING SYSTEM\n",
    "\n",
    "**DEVELOPERS:**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b669f9e2849047b",
   "metadata": {},
   "source": [
    "# Setup Instructions for Jupyter Notebook\n",
    "\n",
    "This notebook installs essential packages for working with LangChain, OpenAI, and other data handling tools. \n",
    "\n",
    "### Important Notes:\n",
    "- **Google Colab Users**: If you are using Google Colab, ensure to install `google-colab` specific packages. \n",
    "- **GPU Configuration**: If using Google Colab, you can enable GPU for faster performance by going to:\n",
    "  - **Runtime** > **Change runtime type** > **Hardware accelerator** and selecting **GPU**.\n",
    "  \n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb72c47",
   "metadata": {},
   "source": [
    "### Install General Utilities and Google Colab Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install general utilities and widgets\n",
    "%pip install pandas opendatasets nest_asyncio ipywebrtc ipywidgets IPython \n",
    "\n",
    "%pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64043e93ef176632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if using google-colab, else skip it\n",
    "%pip install google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde274d17b4b8f27",
   "metadata": {},
   "source": [
    "### Install OpenAI, LangChain, and Related Tools\n",
    "These packages are necessary for using OpenAI’s language models and LangChain's toolkit for search, document processing, and data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ffe9a8e044bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI and related LangChain tools\n",
    "%pip install openai langchain_openai\n",
    "\n",
    "# LangChain Community Tools for search and document handling\n",
    "%pip install langchain_community\n",
    "\n",
    "# Typing extensions and Pydantic\n",
    "%pip install typing_extensions pydantic\n",
    "\n",
    "# LangGraph and experimental LangChain tools\n",
    "%pip install langgraph langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For agent tools\n",
    "%pip install pypdf wikipedia duckduckgo-search playwright\n",
    "\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bbc30c8307592",
   "metadata": {},
   "source": [
    "### Database Utilities, SQLAlchemy, and FAISS for Vector Storage\n",
    "\n",
    "- **Database Utilities**: Install SQLAlchemy for database interactions.\n",
    "- **FAISS**: Choose `faiss-cpu` for CPU environments or `faiss-gpu` if you've enabled GPU support on Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5888251935ea6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database utilities and SQLAlchemy\n",
    "%pip install SQLAlchemy\n",
    "\n",
    "# FAISS for vector storage and retrieval\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa27fcb72b8e53e",
   "metadata": {},
   "source": [
    "## General Imports\n",
    "This cell includes the essential imports needed to use LangChain, OpenAI, and other data handling tools in any Jupyter Notebook or Python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d741d4caeeda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for data handling, display, and LangChain functionality\n",
    "import os\n",
    "import opendatasets as od\n",
    "import nest_asyncio\n",
    "\n",
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from IPython.display import Audio, display, clear_output, Image\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChain and related tools\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# LangChain Agents and supporting libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, trim_messages\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Literal, Sequence, List\n",
    "import functools\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6e32631d8416c",
   "metadata": {},
   "source": [
    "### Google Colab Specific Imports\n",
    "This cell should be run only if you're using Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33c03e419f01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab specific imports\n",
    "try:\n",
    "    from google.colab import output\n",
    "    from google.colab import userdata\n",
    "    from google.colab import files\n",
    "    # for colab\n",
    "    output.enable_custom_widget_manager()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70f69fdf69c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41518f1fbffb425c",
   "metadata": {},
   "source": [
    "## Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c49167463ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    # Retrieve API key from Google Colab userdata (if stored there)\n",
    "    open_ai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    # Not running on Google Colab; prompt for API key input or retrieve from environment variables\n",
    "    open_ai_api_key = os.getenv('OPENAI_API_KEY') or input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Set the API key as an environment variable for universal access within the notebook\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_api_key\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "\n",
    "# Confirm setup\n",
    "if open_ai_api_key:\n",
    "    print(f\"API key successfully set: {open_ai_api_key}\")\n",
    "else:\n",
    "    print(\"API key not set. Please check your setup.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a360a1929e585",
   "metadata": {},
   "source": [
    "# 1. Create Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241141af1e856401",
   "metadata": {},
   "source": [
    "## 1.1. Speech-to-text\n",
    "\n",
    "This tool allows the user to record speech and converts it to a text using OpenAI Whisper model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e18d7b5333b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a614597653bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_audio_recorder():\n",
    "    camera = CameraStream(constraints={'audio': True, 'video': False})\n",
    "    recorder = AudioRecorder(stream=camera)\n",
    "    display(recorder)\n",
    "    return recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b682f5e34981ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recording(recorder):\n",
    "    audio_data = recorder.audio.value\n",
    "    if audio_data:\n",
    "        with open(\"recording.webm\", \"wb\") as f:\n",
    "            f.write(audio_data)\n",
    "        return \"recording.webm\"\n",
    "    else:\n",
    "        print(\"No audio data was captured. Please try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8005ca2b61e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(input_filename, output_filename=\"my_recording.wav\"):\n",
    "    if input_filename and os.path.exists(input_filename):\n",
    "        os.system(f\"ffmpeg -i {input_filename} -ac 1 -f wav {output_filename} -y -hide_banner -loglevel panic\")\n",
    "        if os.path.exists(output_filename):\n",
    "            return output_filename\n",
    "        else:\n",
    "            print(\"Conversion failed.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Input file does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d27fc546946d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(filename):\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file\n",
    "        )\n",
    "    print(\"\")\n",
    "    print(\"Transcription:\", transcription.text)\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0a312a15ebfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_transcribe_candidate_answer():\n",
    "    \"\"\"Record and transcribe a candidate's answer on interviewers' questions.\"\"\"\n",
    "    # Set up the recorder\n",
    "    recorder = setup_audio_recorder()\n",
    "\n",
    "    # Create a save button\n",
    "    print(\"\")\n",
    "    save_button = widgets.Button(description=\"Save Recording\")\n",
    "\n",
    "    # This dictionary will store the transcribed text\n",
    "    transcription_result = {}\n",
    "\n",
    "    # Define the callback function for the save button\n",
    "    def on_save_clicked(button):\n",
    "        # Save the recording\n",
    "        webm_file = save_recording(recorder)\n",
    "        if webm_file:\n",
    "            # Convert to wav format\n",
    "            wav_file = convert_to_wav(webm_file)\n",
    "            if wav_file:\n",
    "                # Transcribe the audio and store the result\n",
    "                transcription_result['text'] = transcribe_audio(wav_file)\n",
    "\n",
    "    save_button.on_click(on_save_clicked)\n",
    "    display(save_button)\n",
    "\n",
    "    # Return the transcription result dictionary\n",
    "    return transcription_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca99386ecd3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "# Try to do live transcription, rather than recording a file. \n",
    "# Take a look at https://gist.github.com/Vaibhavs10/a48d141534cc8d877937d421bb828d8e\n",
    "# and https://github.com/VRSEN/langchain-agents-tutorial/blob/main/main.py\n",
    "\n",
    "# FOSS alternative pipeline, that doesn't rely on OpenAI models\n",
    "# Using HF free API instead \n",
    "# Something like https://github.com/nyrahealth/CrisperWhisper?tab=readme-ov-file#31-usage-with--transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d2e5e97f917bb",
   "metadata": {},
   "source": [
    "## 1.2. Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f961638c01d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_text_input():\n",
    "    text_input = widgets.Textarea(\n",
    "        placeholder=\"Type your answer here...\",\n",
    "        description=\"Answer:\",\n",
    "        layout=widgets.Layout(width='500px', height='100px')\n",
    "    )\n",
    "    display(text_input)\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bc16915c6f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_text_input(text_widget):\n",
    "    user_text = text_widget.value\n",
    "    if user_text.strip():\n",
    "        print(\"\\nInput:\\n\", user_text)\n",
    "        return user_text\n",
    "    else:\n",
    "        print(\"No input was provided. Please type your answer and try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091dca5fa15aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_submit_text():\n",
    "    \"\"\"Record a candidate's text answer on interviewers' questions which require written output like code.\"\"\"\n",
    "    # Set up the text input widget\n",
    "    text_widget = setup_text_input()\n",
    "\n",
    "    # Create a submit button\n",
    "    print(\"\")\n",
    "    submit_button = widgets.Button(description=\"Save Answer\")\n",
    "\n",
    "    # This variable will store the submitted text\n",
    "    submission_result = {}\n",
    "\n",
    "    # Define the callback function for the submit button\n",
    "    def on_submit_clicked(button):\n",
    "        # Capture the user's text input and store it in the dictionary\n",
    "        submission_result['text'] = submit_text_input(text_widget)\n",
    "\n",
    "    submit_button.on_click(on_submit_clicked)\n",
    "    display(submit_button)\n",
    "\n",
    "    # Wait for user input to be submitted\n",
    "    return submission_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f0fd76213961e",
   "metadata": {},
   "source": [
    "## 1.3. File Reader\n",
    "\n",
    "File Reader for PDF and DOCX files. It can extract all the text from the files, allowing us to use the files with our LLM agents.\n",
    "\n",
    "You can upload any files, but we have set a CV and job description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cb132",
   "metadata": {},
   "source": [
    "Set the file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a536813081d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_file_path = r\"all_agents_tutorials\\data\\CV.pdf\"\n",
    "job_description_file_path = r\"all_agents_tutorials\\data\\job_decription.docx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d2739",
   "metadata": {},
   "source": [
    "The following function, extract_ccv_text, is what we will use to extract text from files in .pdf or .docx format. It uses specialized loaders (PyPDFLoader for PDFs and Docx2txtLoader for Word documents) to handle text extraction efficiently. Below is a breakdown of the code and suggestions for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ccv_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text content from a given file in .pdf or .docx format.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input file (must end with .pdf or .docx).\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text content from the file.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the file type is unsupported (not .pdf or .docx).\n",
    "        Exception: For any issues encountered during text extraction.\n",
    "    \n",
    "    Example:\n",
    "        text = extract_ccv_text(\"example.pdf\")\n",
    "        print(text)\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "\n",
    "    try:\n",
    "        if file_path.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_path.split('.')[-1]}\")\n",
    "\n",
    "        # Process each page and concatenate its content\n",
    "        for page in loader.load():\n",
    "            text += page.page_content\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred while extracting text: {e}\")\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(extract_ccv_text(cv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b580af1e74d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(extract_ccv_text(job_description_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18422b0153a965fd",
   "metadata": {},
   "source": [
    "## 1.4. Hiring Company Info Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411db1436669cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(query):\n",
    "    \"\"\"Fetches content from Wikipedia based on a query.\"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    wikipedia_content = wikipedia.run(query)\n",
    "    return wikipedia_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c94cc9e174acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_websites_links(query):\n",
    "    \"\"\"Fetches a list of website links based on a search query using DuckDuckGo.\"\"\"\n",
    "    search = DuckDuckGoSearchResults(output_format=\"list\")\n",
    "    search_results = search.invoke(query)\n",
    "    return [result[\"link\"] for result in search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138048ef56616c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def load_websites_content(websites):\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a list of websites synchronously using the `requests` library.\n",
    "\n",
    "    Args:\n",
    "        websites (list): A list of website URLs to fetch content from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of HTML content from the provided websites.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If fetching any website fails.\n",
    "    \"\"\"\n",
    "    content_list = []\n",
    "    for website in websites:\n",
    "        try:\n",
    "            response = requests.get(website)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "            content_list.append(response.text)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch {website}: {e}\")\n",
    "            content_list.append(None)  # Optionally add `None` for failed requests\n",
    "    return content_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61832d87cfa91b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_html_content(html_content_list, tags=[\"span\", \"p\", \"b\", \"h3\", \"h4\"]):\n",
    "    \"\"\"\n",
    "    Transforms HTML content to extract specific tags using BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        html_content_list (list): A list of raw HTML strings.\n",
    "        tags (list): HTML tags to extract content from.\n",
    "\n",
    "    Returns:\n",
    "        list: Extracted and cleaned content from the HTML strings.\n",
    "    \"\"\"\n",
    "    transformed_content = []\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "\n",
    "    # Wrap HTML strings in Document objects\n",
    "    documents = [Document(page_content=html) for html in html_content_list]\n",
    "\n",
    "    # Transform the documents using BeautifulSoupTransformer\n",
    "    docs_transformed = bs_transformer.transform_documents(documents, tags_to_extract=tags)\n",
    "\n",
    "    # Extract and store the transformed content\n",
    "    for doc in docs_transformed:\n",
    "        transformed_content.append(doc.page_content)\n",
    "\n",
    "    return transformed_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e031bd1b42305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_content(query):\n",
    "    \"\"\"\n",
    "    Main function to gather content from Wikipedia and websites based on a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query for fetching content.\n",
    "\n",
    "    Returns:\n",
    "        list: Combined content from Wikipedia and scraped websites.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "\n",
    "    # Fetch content from Wikipedia\n",
    "    wikipedia_content = get_wikipedia_content(query)\n",
    "    if wikipedia_content:\n",
    "        content.append(wikipedia_content)\n",
    "\n",
    "    # Fetch website links based on the query\n",
    "    website_links = get_websites_links(f\"What is {query}?\")\n",
    "\n",
    "    # Load raw HTML content from the fetched links\n",
    "    html_content_list = load_websites_content(website_links)\n",
    "\n",
    "    # Transform and clean the HTML content\n",
    "    transformed_content = transform_html_content(html_content_list)\n",
    "\n",
    "    # Combine Wikipedia and transformed website content\n",
    "    content.extend(transformed_content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32851a57d96f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = \"Google\"\n",
    "websites_content = get_web_content(query=company_name)\n",
    "websites_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27dca13",
   "metadata": {},
   "source": [
    "This is too long, so we'll call our LLM and ask it to summarize for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Ensure websites_content is a list of strings\n",
    "# Join the list into a single string with proper separator (e.g., \"\\n\\n\" for readability)\n",
    "system_prompt = f\"\"\"\n",
    "Summarize the text from this list:\n",
    "\n",
    "{''.join(websites_content)}\n",
    "\n",
    "To make an overview of the contents:\n",
    "\"\"\"\n",
    "\n",
    "response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "company_info = response.content.strip()\n",
    "\n",
    "print(company_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d716952ac5020",
   "metadata": {},
   "source": [
    "## 1.5. Dataset Querying\n",
    "\n",
    "This is an optional tool for enhancing the process of hard skills review.\n",
    "\n",
    "The dataset can be changed depending on the needs of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada19680d80eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"https://www.kaggle.com/datasets/syedmharis/software-engineering-interview-questions-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c587555228f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaggle_ds(dataset_url):\n",
    "    od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59829e5ba10feda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "\n",
    "# Set the file path to the downloaded data and the encoding of the file\n",
    "file_path = r\"C:\\Users\\DMA\\Downloads\\Software Questions.csv\"\n",
    "encoding = \"ISO-8859-1\"  # default English encoding\n",
    "\n",
    "loader = CSVLoader(file_path=file_path, encoding=encoding)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba42e05d275965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text splitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f67d516dc51406",
   "metadata": {},
   "source": [
    "### 5.1a Using OpenAI Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fec19fe82b517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_questions_dataset_retriever(texts, k):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c013344d9c5a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_questions_dataset_retriever(texts=texts, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d590160f25aa77e",
   "metadata": {},
   "source": [
    "### 5.1b Using HuggingFace Embeddings \n",
    "\n",
    "To represent each chunk as a high-dimensional vector, we’ll use Hugging Face's pre-trained model sentence-transformers/all-MiniLM-L6-v2. This model is efficient and well-suited for generating text embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84977845029fc567",
   "metadata": {},
   "source": [
    "We’ll define a simple helper class to handle embedding generation using the Hugging Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8dca7934bf6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbeddings:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        # Load the model and tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def embed_texts(self, texts):\n",
    "        # Generate embeddings for each text\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ce16f9cfb72df",
   "metadata": {},
   "source": [
    "Now, let’s generate embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897a69ec2ff2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Generate embeddings for each chunk of text\n",
    "embeddings = embeddings_model.embed_texts([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eff59bdd058320",
   "metadata": {},
   "source": [
    "After this step, `embeddings` will contain a vector representation of each document chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663811a7f4974e2",
   "metadata": {},
   "source": [
    "To make our embeddings searchable, we’ll use FAISS to create an index. This allows us to find the most similar embeddings to any query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62481f0456eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Initialize the FAISS index\n",
    "embedding_dim = embeddings.shape[1]  # Dimension of embeddings\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6a9124bc25b62",
   "metadata": {},
   "source": [
    "Finally, we’ll define a `retriever` function that, given a query, will embed it and retrieve the most similar document chunks from the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d84f1e759c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query, texts, embeddings_model, faiss_index, k=5):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embeddings_model.embed_texts([query])[0]\n",
    "    \n",
    "    # Search FAISS index for the top-k similar chunks\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    # Retrieve the corresponding text chunks\n",
    "    results = [texts[i].page_content for i in indices[0]]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498063ba154aff57",
   "metadata": {},
   "source": [
    "For testing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d35189e601810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your query\n",
    "query = \"What is the topic of interest?\"\n",
    "\n",
    "# Call the retriever with the required arguments\n",
    "results = retriever(query, texts, embeddings_model, faiss_index, k=5)\n",
    "\n",
    "# Print the top results\n",
    "print(\"Top similar chunks:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9fc77f1caaec8",
   "metadata": {},
   "source": [
    "### 5.2 Define the tool for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae193c12d94107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: update this tool so its usable by agents\n",
    "\n",
    "# @tool\n",
    "questions_database_tool = create_retriever_tool(\n",
    "    create_questions_dataset_retriever,\n",
    "    \"search_subject_matter_questions\",\n",
    "    \"Searches and returns subject matter questions for checking hard skills.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ecc3bc",
   "metadata": {},
   "source": [
    "# 2. Creating the Graph(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293add6",
   "metadata": {},
   "source": [
    "## Workflow Structure\n",
    "\n",
    "- **Orchestration Stage:**\n",
    "  - **Information Gathering:**\n",
    "    - Obtain user documents (CV, Job description) using the `FileUpload` tool.\n",
    "    - Retrieve company information via the `WebScraper` tool.\n",
    "    - Create a \"Scene\" to outline the interview structure.\n",
    "\n",
    "- **Interview Stages:**\n",
    "  - **HR Stage:** Introduce the user, ask general and behavioral questions, and evaluate responses.\n",
    "  - **Manager Stage:** Introduce the team, ask role-specific questions, and evaluate responses.\n",
    "  - **Technical/Field Expert Stage:** Pose field-specific questions, assess technical knowledge, and evaluate responses.\n",
    "\n",
    "- **Feedback Stage:** Compile evaluations from all stages and generate final feedback for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260be015",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb2438",
   "metadata": {},
   "source": [
    "Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911709f",
   "metadata": {},
   "source": [
    "## 2.1. Define Orchestration State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_company_name = input(prompt=\"What is the name of the company you are applying to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestrationState(BaseModel):\n",
    "    company_name:str = user_company_name\n",
    "    user_documents: dict = Field(default_factory=dict)\n",
    "    company_info: str = \"\"\n",
    "    scene: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cb520",
   "metadata": {},
   "source": [
    "Define the tools for the Orchestration stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def FileUpload(description: str) -> dict:\n",
    "    \"\"\"\n",
    "    Simulate a file upload process.\n",
    "\n",
    "    Args:\n",
    "        description (str): A brief description of the file upload context.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing simulated content for 'CV' and 'Job Description'.\n",
    "    \"\"\"\n",
    "\n",
    "    ccv_text = extract_ccv_text(cv_file_path)\n",
    "    job_description = extract_ccv_text(job_description_file_path)\n",
    "    \n",
    "    return {\"CV\": ccv_text, \"Job Description\": job_description}\n",
    "\n",
    "@tool\n",
    "def WebScraper(company_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulate web scraping to extract company information.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of a company\n",
    "\n",
    "    Returns:\n",
    "        str: Simulated company information extracted from the website.\n",
    "    \"\"\"\n",
    "    web_content = get_web_content(company_name)\n",
    "    content = \"\\n\".join(web_content)\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71fb40",
   "metadata": {},
   "source": [
    "Define Nodes for the three sub-stages of the Orchestration stage:\n",
    "- Document upload\n",
    "- Scrape information about company\n",
    "- Create a \"Scene\" to outline the interview structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_SCENE_SYSTEM_PROMPT = \"\"\"\n",
    "    Based on the following inputs:\n",
    "    - CV: {cv}\n",
    "    - Job Description: {job_description}\n",
    "    - Company Info: {company_info}\n",
    "\n",
    "    Generate a detailed \"scene\" for conducting an interview process. \n",
    "    The interview will consist of three distinct parts:\n",
    "    1. **HR Screening**: Focus on cultural fit, general background, and the candidate's interest in the role and company.\n",
    "    2. **Managerial Round**: Evaluate leadership skills, team collaboration, and alignment with the role’s strategic objectives.\n",
    "    3. **Technical Assessment**: Deep dive into the candidate’s technical knowledge, problem-solving skills, and expertise relevant to the position.\n",
    "\n",
    "    The scene should encourage creativity while providing a clear structure for each round, offering general guidance on what topics or approaches the participants (HR, Manager, and Technical Interviewer) might adopt.\n",
    "\n",
    "    Your response should include:\n",
    "    - A brief narrative for each part of the interview.\n",
    "    - Example questions or discussion points for each section.\n",
    "    - Notes on the overall flow or tone of the interview.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Upload Node\n",
    "# Using the FileUpload tool we defined before\n",
    "def upload_documents(state: OrchestrationState):\n",
    "    state.user_documents = FileUpload(\"Upload user documents (CV, job description).\")\n",
    "    return state\n",
    "\n",
    "# Web Scraper Node\n",
    "# Using the WebScraper tool we defined before\n",
    "def scrape_company_info(state: OrchestrationState):\n",
    "    state.company_info = WebScraper(state.company_name)  # we can pass along the company name directly from the State\n",
    "    return state\n",
    "\n",
    "# Scene Generation Node\n",
    "# Grabbing all the necessary info directly from the State\n",
    "def generate_scene(state: OrchestrationState):\n",
    "    cv = state.user_documents.get(\"CV\", \"\")\n",
    "    job_description = state.user_documents.get(\"Job Description\", \"\")\n",
    "    company_info = state.company_info\n",
    "\n",
    "    system_prompt = GENERATE_SCENE_SYSTEM_PROMPT\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.scene = response.content.strip()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7d63a",
   "metadata": {},
   "source": [
    "Create the graph for the orchestration stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph for orchestration\n",
    "orchestration_graph = StateGraph(OrchestrationState)\n",
    "\n",
    "# Add the nodes for each of the \"sub-stages\"\n",
    "orchestration_graph.add_node('upload_documents', upload_documents)\n",
    "orchestration_graph.add_node('scrape_company_info', scrape_company_info)\n",
    "orchestration_graph.add_node('generate_scene', generate_scene)\n",
    "\n",
    "# Define the flow of tasks\n",
    "orchestration_graph.add_edge(START, 'upload_documents')\n",
    "orchestration_graph.add_edge('upload_documents', 'scrape_company_info')\n",
    "orchestration_graph.add_edge('scrape_company_info', 'generate_scene')\n",
    "orchestration_graph.add_edge('generate_scene', END)\n",
    "\n",
    "# Compile the orchestration graph\n",
    "orchestration_app = orchestration_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf338c1",
   "metadata": {},
   "source": [
    "Visualize orchestration graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(orchestration_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7997e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_orchestration_state = OrchestrationState()\n",
    "result_orchestration_state = orchestration_app.invoke(initial_orchestration_state)\n",
    "result_orchestration_state = OrchestrationState(**result_orchestration_state)\n",
    "print(type(result_orchestration_state))\n",
    "print(result_orchestration_state.scene)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a92335",
   "metadata": {},
   "source": [
    "## 2.2. Interview Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(BaseModel):\n",
    "    scene: str = \"\"\n",
    "    user_documents: dict = {}\n",
    "    company_info: str = \"\"\n",
    "    questions: list = []\n",
    "    user_responses: dict = {}\n",
    "    evaluation: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1011e3",
   "metadata": {},
   "source": [
    "Define the tools for the Interview stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def UserInput(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the user for input.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The message displayed to the user.\n",
    "\n",
    "    Returns:\n",
    "        str: The user's input as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    print(prompt)\n",
    "    voice, text_input = display_input_form_with_return()\n",
    "    answer = f\"Answer: {voice.get('text', '') if voice else ''}\\n\\n{text_input.get('text', '') if text_input else ''}\"\n",
    "\n",
    "\n",
    "    return input(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d8713",
   "metadata": {},
   "source": [
    "There will be three sub-stages within the Interview stage: \n",
    "- The HR interview\n",
    "- The Manager interview\n",
    "- The technical interview\n",
    "\n",
    "These are quite similar, so they share the base State (InterviewState) and the tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6eff65",
   "metadata": {},
   "source": [
    "### 2.2.2. HR Interview sub-stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386e12d",
   "metadata": {},
   "source": [
    "For the HR Interview sub-stage, there are going to be the following tasks, which we will define as nodes for our graph: \n",
    "1. Introduction: Introduces the candidate and sets the stage for the interview.\n",
    "2. Generate Questions Node: Uses the `Scene`, `User Documents`, and `Company Information` to generate a list of interview questions.\n",
    "3. Ask Questions Node: Iterates through the generated questions, asks the user, and records responses.\n",
    "4. Write Evaluation Node: Summarizes the candidate’s responses and writes an evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d1d48",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3182273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "def introduction(state: InterviewState):\n",
    "    print(f\"Welcome to the interview! Here's an overview: {state.scene}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating questions\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an HR assistant conducting an interview.\n",
    "    Use the following information to generate between 1 and 3 tailored questions:\n",
    "\n",
    "    Scene: {state.scene}\n",
    "    CV: {state.cv}\n",
    "    Job Description: {state.job_description}\n",
    "    Company Information: {state.company_info}\n",
    "\"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.questions = response.content.strip().split('\\n')[:3]  # list slicing to cut off at max 3 questions\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions, using the UserInput tool\n",
    "\n",
    "def ask_questions(state: InterviewState):\n",
    "    while state.questions:\n",
    "        question = state.questions.pop(0)\n",
    "        response = UserInput(question)\n",
    "        state.user_responses[question] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52181c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final evaluation\n",
    "\n",
    "def write_evaluation(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    Based on the following user responses, write a brief evaluation of the candidate:\n",
    "\n",
    "    Responses: {state.user_responses}\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.evaluation = response.content.strip()\n",
    "    print(\"Evaluation:\", state.evaluation)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c15b14",
   "metadata": {},
   "source": [
    "#### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b44edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HR sub-graph\n",
    "hr_graph = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes\n",
    "hr_graph.add_node('introduction', introduction)\n",
    "hr_graph.add_node('generate_questions', generate_questions)\n",
    "hr_graph.add_node('ask_questions', ask_questions)\n",
    "hr_graph.add_node('write_evaluation', write_evaluation)\n",
    "\n",
    "# Define edges\n",
    "hr_graph.add_edge(START, 'introduction')\n",
    "hr_graph.add_edge('introduction', 'generate_questions')\n",
    "hr_graph.add_edge('generate_questions', 'ask_questions')\n",
    "hr_graph.add_edge('ask_questions', 'write_evaluation')\n",
    "hr_graph.add_edge('write_evaluation', END)\n",
    "\n",
    "# Compile the HR graph\n",
    "hr_app = hr_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(hr_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5340813",
   "metadata": {},
   "source": [
    "### 2.2.3. Manager Interview Sub-Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99fe6a",
   "metadata": {},
   "source": [
    "The Manager interview sub-stage is going to be very similar to the HR interview in its structure, but it will differ in the System Prompts we give to the Manager agent, in order to gear it more towards organizational and technical questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e037628",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "def introduction(state: InterviewState):\n",
    "    print(f\"Welcome to the interview! Here's an overview: {state.scene}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb467fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating questions\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an HR assistant conducting an interview.\n",
    "    Use the following information to generate between 1 and 3 tailored questions:\n",
    "\n",
    "    Scene: {state.scene}\n",
    "    CV: {state.cv}\n",
    "    Job Description: {state.job_description}\n",
    "    Company Information: {state.company_info}\n",
    "\"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.questions = response.content.strip().split('\\n')[:3]  # list slicing to cut off at max 3 questions\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98596479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions, using the UserInput tool\n",
    "\n",
    "def ask_questions(state: InterviewState):\n",
    "    while state.questions:\n",
    "        question = state.questions.pop(0)\n",
    "        response = UserInput(question)\n",
    "        state.user_responses[question] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72809b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final evaluation\n",
    "\n",
    "def write_evaluation(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    Based on the following user responses, write a brief evaluation of the candidate:\n",
    "\n",
    "    Responses: {state.user_responses}\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.evaluation = response.content.strip()\n",
    "    print(\"Evaluation:\", state.evaluation)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6f72c",
   "metadata": {},
   "source": [
    "#### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manager sub-graph\n",
    "manager_graph = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes\n",
    "manager_graph.add_node('introduction', introduction)\n",
    "manager_graph.add_node('generate_questions', generate_questions)\n",
    "manager_graph.add_node('ask_questions', ask_questions)\n",
    "manager_graph.add_node('write_evaluation', write_evaluation)\n",
    "\n",
    "# Define edges\n",
    "manager_graph.add_edge(START, 'introduction')\n",
    "manager_graph.add_edge('introduction', 'generate_questions')\n",
    "manager_graph.add_edge('generate_questions', 'ask_questions')\n",
    "manager_graph.add_edge('ask_questions', 'write_evaluation')\n",
    "manager_graph.add_edge('write_evaluation', END)\n",
    "\n",
    "# Compile the Manager graph\n",
    "manager_app = manager_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbca73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(manager_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7d290",
   "metadata": {},
   "source": [
    "### 2.2.4. Field-Specific Interview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa7bf9",
   "metadata": {},
   "source": [
    "The field-specific, or technical interiew, is geared towards asking questions about specifics in the field. For our example, the Field Expert agent can ask programming questions related to Python, Data Science libraries mentioned in the Job Description, etc. \n",
    "\n",
    "Due to this, this interview will differ slightly in its structure, in that there will be no introductory phase, and the questions can be pulled from a database of questions, if one has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ExpertInterviewState(BaseModel):\n",
    "    scene: str = \"\"\n",
    "    user_documents: dict = {}\n",
    "    company_info: str = \"\"\n",
    "    generated_query: str = \"\"\n",
    "    retrieved_context: str = \"\"\n",
    "    questions: list = []\n",
    "    user_responses: dict = {}\n",
    "    evaluation: str = \"\"\n",
    "    db_is_available: bool = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d8605",
   "metadata": {},
   "source": [
    "#### Tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba59b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def query_questions_database(query: str) -> str: # TODO on pause do not use for now\n",
    "    \"\"\"\n",
    "    Query the FAISS database for the most relevant questions or context.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved documents as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve relevant documents\n",
    "        results = retriever.get_relevant_documents(query)\n",
    "\n",
    "        # Format results as a string for the tool's output\n",
    "        formatted_results = \"\\n\".join([doc.page_content for doc in results])\n",
    "        return formatted_results\n",
    "    except Exception as e:\n",
    "        return f\"Error querying the database: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02390626",
   "metadata": {},
   "source": [
    "#### Nodes\n",
    "\n",
    "Placeholders, prompts need to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(state: InterviewState):\n",
    "    state.generated_query = f\"\"\"\n",
    "    Generate questions for the following:\n",
    "    - Scene: {state.scene}\n",
    "    - CV: {state.user_documents.get('CV', '')}\n",
    "    - Job Description: {state.user_documents.get('Job Description', '')}\n",
    "    - Company Information: {state.company_info}\n",
    "\n",
    "    Focus on retrieving relevant topics for the interview.\n",
    "    \"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def retrieve_context(state: InterviewState):\n",
    "    if \"DB_AVAILABLE\" in globals():  # Check if the database is available\n",
    "        # Use the generated query to search the database\n",
    "        results = query_questions_database(state.generated_query)\n",
    "        state.retrieved_context = \"\\n\".join(results.split('\\n'))  # Combine matches into context\n",
    "    else:\n",
    "        state.retrieved_context = \"\"  # No context retrieved if DB is unavailable\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    # Generate questions using retrieved context (if available) and other information\n",
    "    system_prompt = f\"\"\"\n",
    "    Using the retrieved context below, generate three new interview questions:\n",
    "    \n",
    "    Retrieved Context:\n",
    "    {state.retrieved_context}\n",
    "    \n",
    "    Additional Information:\n",
    "    - Scene: {state.scene}\n",
    "    - CV: {state.user_documents.get('CV', '')}\n",
    "    - Job Description: {state.user_documents.get('Job Description', '')}\n",
    "    - Company Information: {state.company_info}\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.questions = response.content.strip().split('\\n')\n",
    "    return state\n",
    "\n",
    "def ask_questions(state: InterviewState):\n",
    "    while state.questions:\n",
    "        question = state.questions.pop(0)\n",
    "        response = UserInput(question)  # Capture user input via tool\n",
    "        state.user_responses[question] = response\n",
    "    return state\n",
    "\n",
    "def evaluate_answers(state: InterviewState):\n",
    "    system_prompt = f\"\"\"\n",
    "    Evaluate the following user responses based on the context:\n",
    "    - Scene: {state.scene}\n",
    "    - CV: {state.user_documents.get('CV', '')}\n",
    "    - Job Description: {state.user_documents.get('Job Description', '')}\n",
    "    - Company Information: {state.company_info}\n",
    "    - User Responses: {state.user_responses}\n",
    "    \n",
    "    Provide a detailed evaluation.\n",
    "    \"\"\"\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.evaluation = response.content.strip()\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46142c81",
   "metadata": {},
   "source": [
    "#### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Interview sub-graph\n",
    "expert_interview_graph = StateGraph(ExpertInterviewState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "expert_interview_graph.add_node('generate_query', generate_query)\n",
    "expert_interview_graph.add_node('retrieve_context', retrieve_context)\n",
    "expert_interview_graph.add_node('generate_questions', generate_questions)\n",
    "expert_interview_graph.add_node('ask_questions', ask_questions)\n",
    "expert_interview_graph.add_node('evaluate_answers', evaluate_answers)\n",
    "\n",
    "# Define the condition function for the start node\n",
    "def start_condition(state: ExpertInterviewState):\n",
    "    if state.db_is_available:\n",
    "        return 'generate_query'\n",
    "    else:\n",
    "        return 'generate_questions'\n",
    "\n",
    "# Add edges\n",
    "expert_interview_graph.add_conditional_edges(START, start_condition, {'generate_query': 'generate_query', 'generate_questions': 'generate_questions'})\n",
    "expert_interview_graph.add_edge('generate_query', 'retrieve_context')\n",
    "expert_interview_graph.add_edge('retrieve_context', 'generate_questions')\n",
    "expert_interview_graph.add_edge('generate_questions', 'ask_questions')\n",
    "expert_interview_graph.add_edge('ask_questions', 'evaluate_answers')\n",
    "expert_interview_graph.add_edge('evaluate_answers', END)\n",
    "\n",
    "# Compile the graph\n",
    "expert_interview_app = expert_interview_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(expert_interview_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12916755",
   "metadata": {},
   "source": [
    "### 2.2.5. Connecting the interview subgraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd79de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_graph = StateGraph(BaseModel)\n",
    "\n",
    "# Add HR stage as a subgraph\n",
    "interview_graph.add_node('hr', hr_app)\n",
    "\n",
    "# Add Manager stage as a subgraph\n",
    "interview_graph.add_node('manager', manager_app)\n",
    "\n",
    "# Add Field expert stage as a subgraph\n",
    "interview_graph.add_node('expert', expert_interview_app)\n",
    "\n",
    "\n",
    "# add edges\n",
    "interview_graph.add_edge(START, \"hr\")\n",
    "interview_graph.add_edge(\"hr\", \"manager\")\n",
    "interview_graph.add_edge(\"manager\", \"expert\")\n",
    "interview_graph.add_edge(\"expert\", END)\n",
    "parent_graph = interview_graph.compile()\n",
    "\n",
    "# Compile the graph\n",
    "interview_app = interview_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "mermaid_diagram = interview_app.get_graph(xray=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba27d1",
   "metadata": {},
   "source": [
    "## 2.3. Feedback subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af8b85",
   "metadata": {},
   "source": [
    "This will be the simplest stage, where we pass along the context from the Orchestration stage, the feedback of each of the interviewers, and generate a comprehensive feedback document as the final summary. The final output will be saved as a PDF, using a markdown to PDF tool we will define. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fef748",
   "metadata": {},
   "source": [
    "#### PDF writing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install markdown reportlab beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1443c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdown import markdown\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.pagesizes import letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ea629",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def markdown_to_pdf(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a Markdown string to a PDF file.\n",
    "\n",
    "    Args:\n",
    "    - markdown_text (str): The Markdown-formatted string.\n",
    "\n",
    "    Returns:\n",
    "    - str: Confirmation message upon successful creation of the PDF.\n",
    "    \"\"\"\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown(markdown_text)\n",
    "    \n",
    "    # Create a PDF document\n",
    "    doc = SimpleDocTemplate(\"feedback.pdf\", pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = []\n",
    "\n",
    "    # Split HTML into paragraphs\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    for element in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\"]):\n",
    "        style = styles[\"BodyText\"]\n",
    "        if element.name.startswith(\"h\"):\n",
    "            style = styles[\"Heading{}\".format(min(int(element.name[1]), 4))]\n",
    "        story.append(Paragraph(element.text, style))\n",
    "\n",
    "    # Build the PDF\n",
    "    doc.build(story)\n",
    "    return \"PDF generated: 'feedback.pdf'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9727a",
   "metadata": {},
   "source": [
    "#### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackState(BaseModel):\n",
    "    hr_feedback: str = \"\"\n",
    "    manager_feedback: str = \"\"\n",
    "    field_specialist_feedback: str = \"\"\n",
    "    consolidated_feedback: str = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df9cf0",
   "metadata": {},
   "source": [
    "Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_feedback(state: FeedbackState, hr_state: InterviewState, manager_state: InterviewState, expert_state: ExpertInterviewState):\n",
    "    state.hr_feedback = hr_state.evaluation\n",
    "    state.manager_feedback = manager_state.evaluation\n",
    "    state.field_specialist_feedback = expert_state.evaluation\n",
    "    return state\n",
    "\n",
    "def write_feedback(state: FeedbackState):\n",
    "    # Construct the system prompt for the LLM\n",
    "    system_prompt = \"\"\"\n",
    "    You are an experienced interviewer assistant. Based on the feedback provided by the HR team, the Manager, and the Field Specialist, generate a professional and concise final feedback for the candidate.\n",
    "    Ensure the feedback is holistic, constructive, and highlights key points.\n",
    "\n",
    "    Here is the feedback from each stage:\n",
    "    \"\"\"\n",
    "\n",
    "    # Append individual feedback to the system prompt\n",
    "    system_prompt += f\"\"\"\n",
    "    HR Feedback:\n",
    "    {state.hr_feedback}\n",
    "\n",
    "    Manager Feedback:\n",
    "    {state.manager_feedback}\n",
    "\n",
    "    Field Specialist Feedback:\n",
    "    {state.field_specialist_feedback}\n",
    "\n",
    "    Provide a summary that combines and synthesizes all the points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the LLM to generate the final feedback\n",
    "    response = llm([{\"role\": \"system\", \"content\": system_prompt}])\n",
    "    state.consolidated_feedback = response.content.strip()\n",
    "    return state\n",
    "\n",
    "def generate_pdf(state: FeedbackState):\n",
    "    confirmation_message = markdown_to_pdf(state.consolidated_feedback)\n",
    "    print(confirmation_message)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c3e74",
   "metadata": {},
   "source": [
    "Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1002053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Define the feedback graph\n",
    "feedback_graph = StateGraph(FeedbackState)\n",
    "\n",
    "# Add nodes\n",
    "feedback_graph.add_node('gather_feedback', gather_feedback)\n",
    "feedback_graph.add_node('write_feedback', write_feedback)\n",
    "feedback_graph.add_node('generate_pdf', generate_pdf)\n",
    "\n",
    "# Define edges\n",
    "feedback_graph.add_edge(START, 'gather_feedback')\n",
    "feedback_graph.add_edge('gather_feedback', 'write_feedback')\n",
    "feedback_graph.add_edge('write_feedback', 'generate_pdf')\n",
    "feedback_graph.add_edge('generate_pdf', END)\n",
    "\n",
    "# Compile the feedback graph\n",
    "feedback_app = feedback_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(feedback_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968575c0",
   "metadata": {},
   "source": [
    "# 3. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb52864",
   "metadata": {},
   "source": [
    "## 3.1. Connecting the stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8479d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_graph = StateGraph(BaseModel)\n",
    "\n",
    "# Add Orchestration stage as a subgraph\n",
    "parent_graph.add_node('orchestration', orchestration_app)\n",
    "\n",
    "# Add Interview stage as a subgraph\n",
    "parent_graph.add_node('interview', interview_app)\n",
    "\n",
    "# Add Feedback stage as a subgraph\n",
    "parent_graph.add_node('feedback', feedback_app)\n",
    "\n",
    "\n",
    "# add edges\n",
    "parent_graph.add_edge(START, \"orchestration\")\n",
    "parent_graph.add_edge(\"orchestration\", \"interview\")\n",
    "parent_graph.add_edge(\"interview\", \"feedback\")\n",
    "parent_graph.add_edge(\"feedback\", END)\n",
    "\n",
    "# Compile the graph\n",
    "parent_app = parent_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize graph\n",
    "\n",
    "display(Image(parent_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05ecbc",
   "metadata": {},
   "source": [
    "## 3.2. Transferring data between stage `State`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea607056",
   "metadata": {},
   "source": [
    "To ensure seamless data transfer from the Orchestration graph to the Interview sub-graphs, we need to:\n",
    "1. Extract relevant data from the OrchestrationState (e.g., `User Documents`, `Company Info`, `Scene`).\n",
    "2. Transform it into an appropriate format for the HRState.\n",
    "3. Use a transformer function in the parent graph to bridge the data between the two sub-graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_interview(orchestration_state: OrchestrationState) -> InterviewState:\n",
    "    return InterviewState(\n",
    "        cv=orchestration_state.user_documents.get(\"CV\", \"\"),\n",
    "        job_description=orchestration_state.user_documents.get(\"Job Description\", \"\"),\n",
    "        company_info=orchestration_state.company_info,\n",
    "        scene=orchestration_state.scene\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_agents",
   "language": "python",
   "name": "genai_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
